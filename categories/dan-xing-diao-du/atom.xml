<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 弹性调度 | loop in codes]]></title>
  <link href="http://codemacro.com/categories/dan-xing-diao-du/atom.xml" rel="self"/>
  <link href="http://codemacro.com/"/>
  <updated>2018-03-08T21:57:53+08:00</updated>
  <id>http://codemacro.com/</id>
  <author>
    <name><![CDATA[Kevin Lynx]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[使用Kubeadm在CentOS部署Kubernets 1.8.7]]></title>
    <link href="http://codemacro.com/2018/03/08/deploy-kubernetes/"/>
    <updated>2018-03-08T00:00:00+08:00</updated>
    <id>http://codemacro.com/2018/03/08/deploy-kubernetes</id>
    <content type="html"><![CDATA[<p>主要参考：</p>

<ul>
<li><a href="https://v1-8.docs.kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">官方文档</a></li>
<li><a href="https://my.oschina.net/xdatk/blog/895645">如何在国内愉快的安装 Kubernetes</a></li>
<li><a href="https://my.oschina.net/andylo25/blog/1618342">kubernetes 1.8.7 国内安装(kubeadm)</a></li>
</ul>


<p>建议都大致浏览下。这里我也是简单地记录，估计每个人遇到的细节问题不一样。</p>

<h2>环境准备</h2>

<p>我拿到手的环境docker已经ready：</p>

<ul>
<li>docker (alidocker-1.12.6.22)</li>
<li>CentOS 7</li>
</ul>


<p>上面博客提到的一些系统设置可以先做掉：</p>

<pre><code>cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl -p /etc/sysctl.d/k8s.conf
</code></pre>

<p>其他一些设置：</p>

<ul>
<li>防火墙最好关闭</li>
<li>swap最好关闭</li>
<li><code>setenforce 0</code></li>
</ul>


<!-- more -->


<h2>软件包及镜像</h2>

<p>众所周知的局域网问题，官方的很多软件包和镜像无法获取。解决这个问题主要靠阿里云：</p>

<ul>
<li>配置yum源，由于阿里云yum源相对官方有滞后，并且各个软件包版本匹配我没有找到，所以就按照上面博客提到的，安装1.8.7，避免版本问题：</li>
</ul>


<pre><code>cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
        https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF
</code></pre>

<p>可以把包下到本地以备不时之需：</p>

<pre><code>yum install -y --downloadonly --downloaddir=./ kubelet-1.8.7 kubeadm-1.8.7 kubectl-1.8.7 kubernetes-cni-0.5.1
yum install -y *.rpm
</code></pre>

<ul>
<li>docker镜像问题后来我发现可以从阿里云直接拉，拉下来后通过打tag避免后续部署从google官方拉</li>
</ul>


<p>以下脚本内容未验证，主要注意镜像得提前拉全(上面链接的博客有写漏了)</p>

<pre><code>#!/bin/sh

images=(kube-scheduler-amd64:v1.8.7 \
kube-proxy-amd64:v1.8.7 \
kube-apiserver-amd64:v1.8.7 \
etcd-amd64:3.0.17 \
pause-amd64:3.0 \
k8s-dns-sidecar-amd64:1.14.5 \
k8s-dns-kube-dns-amd64:1.14.5 \
k8s-dns-dnsmasq-nanny-amd64:1.14.5 \
kubernetes-dashboard-amd64:v1.8.1)

for imageName in ${images[@]} ; do
  docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
  docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName gcr.io/google_containers/$imageName
  docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
done

# 注意这个镜像最好提前拉，看flannel的yaml描述里会用到
docker pull quay.io/coreos/flannel:v0.9.1-amd64
</code></pre>

<p>贴个我部署好后的镜像列表(自行忽略多余的)，方便对比：</p>

<pre><code>$sudo docker images
REPOSITORY                                                                          TAG                 IMAGE ID            CREATED             SIZE
registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64      v1.8.3              0c60bcf89900        3 weeks ago         102.3 MB
gcr.io/google_containers/kubernetes-dashboard-amd64                                 v1.8.1              63c78846e37b        4 weeks ago         120.7 MB
gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64                                1.14.5              6d7cc6e484b3        4 weeks ago         41.42 MB
gcr.io/google_containers/k8s-dns-kube-dns-amd64                                     1.14.5              33072cb8c892        4 weeks ago         49.38 MB
gcr.io/google_containers/k8s-dns-sidecar-amd64                                      1.14.5              e1414b167ca6        4 weeks ago         41.81 MB
gcr.io/google_containers/pause-amd64                                                3.0                 8ca66ae4813a        4 weeks ago         746.9 kB
gcr.io/google_containers/etcd-amd64                                                 3.0.17              10010bfa0a74        4 weeks ago         168.9 MB
gcr.io/google_containers/kube-scheduler-amd64                                       v1.8.7              906029e1500b        4 weeks ago         55.13 MB
gcr.io/google_containers/kube-apiserver-amd64                                       v1.8.7              c3bb648343de        4 weeks ago         194.7 MB
gcr.io/google_containers/kube-proxy-amd64                                           v1.8.7              125dec6bd8f2        7 weeks ago         93.36 MB
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy-amd64                v1.8.7              125dec6bd8f2        7 weeks ago         93.36 MB
gcr.io/google_containers/kube-controller-manager-amd64                              v1.8.7              d8df883aabf9        7 weeks ago         129.6 MB
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager-amd64   v1.8.7              d8df883aabf9        7 weeks ago         129.6 MB
quay.io/coreos/flannel                                                              v0.9.1-amd64        2b736d06ca4c        3 months ago        51.31 MB
</code></pre>

<h2>启动master</h2>

<pre><code>systemctl enable kubelet
systemctl start kubelet
</code></pre>

<p>注意，这里官方文档也提到了，需要确认docker的cgroup配置与kubelet的一致：</p>

<pre><code>docker info | grep -i cgroup
cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
</code></pre>

<p>不一致改成一致。例如：</p>

<pre><code>sed -i "s/cgroup-driver=systemd/cgroup-driver=cgroupfs/g" /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
</code></pre>

<p>配置有变更需要重启kubelet：</p>

<pre><code>systemctl daemon-reload
systemctl restart kubelet
</code></pre>

<p><code>systemctl status kubelet</code>查看日志会发现里面有warning/error级别的日志，所以出问题时很容易被误解。</p>

<p>按照官方文档，在这一步时，kubelet会不断重启，所以这个时候可以继续，准备初始化master：</p>

<pre><code>#!/bin/bash
kubeadm init --kubernetes-version=v1.8.7 --pod-network-cidr 10.244.0.0/16

mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config

kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml
</code></pre>

<p>注意上面flannel的版本与前面的链接不同，v0.9.1测试可用。当flannel启动后，kubelet就会发现这个CNI实现，从而趋于成功运行状态。</p>

<p>可以通过以下命令来确认master是否真的初始化成功：</p>

<pre><code>$sudo kubectl get node
NAME                 STATUS    ROLES     AGE       VERSION
v101083237zsqazzmf   Ready     master    12m       v1.8.7

$sudo kubectl get pod --all-namespaces
NAMESPACE     NAME                                         READY     STATUS    RESTARTS   AGE
kube-system   etcd-v101083237zsqazzmf                      1/1       Running   0          11m
kube-system   kube-apiserver-v101083237zsqazzmf            1/1       Running   0          11m
kube-system   kube-controller-manager-v101083237zsqazzmf   1/1       Running   0          11m
kube-system   kube-dns-545bc4bfd4-f7hsx                    3/3       Running   0          11m
kube-system   kube-flannel-ds-d6z78                        1/1       Running   5          6m
kube-system   kube-proxy-z88cd                             1/1       Running   0          11m
kube-system   kube-scheduler-v101083237zsqazzmf            1/1       Running   0          11m

$sudo kubectl get cs
NAME                 STATUS    MESSAGE              ERROR
controller-manager   Healthy   ok                   
scheduler            Healthy   ok                   
etcd-0               Healthy   {"health": "true"}  
</code></pre>

<p>master初始成功会返回加入node的token，例如：</p>

<pre><code>kubeadm join --token 795c0c.c5a1b252c2d23a0c 10.101.83.237:6443 --discovery-token-ca-cert-hash sha256:1c7760c9f02e2f058de3f6bc759e85316b65376cfcc83d975ea6c64ac2175ecc
</code></pre>

<p>注意token默认24小时过期。如果在这个过程中始终有问题，可以做reset回到干净状态：</p>

<pre><code>kubeadm reset
</code></pre>

<h2>部署node及加入</h2>

<ul>
<li>系统设置同上</li>
<li>前面提到的软件需要安装</li>
<li>镜像只需要部分</li>
<li>运行kubelet的cgroup设置参考前面</li>
</ul>


<pre><code>images=(kube-proxy-amd64:v1.8.7 \
pause-amd64:3.0 \
kubernetes-dashboard-amd64:1.8.1)
images=(kube-proxy-amd64:v1.8.7)
for imageName in ${images[@]} ; do
  docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
  docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName gcr.io/google_containers/$imageName
  docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
done
</code></pre>

<p>贴下镜像列表：</p>

<pre><code>$sudo docker images
REPOSITORY                                                                       TAG                 IMAGE ID            CREATED             SIZE
registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64   v1.8.3              0c60bcf89900        3 weeks ago         102.3 MB
gcr.io/google_containers/kube-proxy-amd64                                        v1.8.7              263b722c47c1        4 weeks ago         93.36 MB
gcr.io/google_containers/pause-amd64                                             3.0                 8ca66ae4813a        4 weeks ago         746.9 kB
gcr.io/google_containers/kubernetes-dashboard-amd64                              1.8.1               758ae6af38ca        5 weeks ago         120.7 MB
quay.io/coreos/flannel                                                           v0.9.1-amd64        2b736d06ca4c        3 months ago        51.31 MB
</code></pre>

<p>最后使用<code>kubeadm join xxxx</code> (部署master时会返回)加入网络。加入后在master端可以确认：</p>

<pre><code>$sudo kubectl get nodes
NAME                 STATUS    ROLES     AGE       VERSION
v101083225zsqazzmf   Ready     &lt;none&gt;    2h        v1.8.7
v101083237zsqazzmf   Ready     master    3h        v1.8.7
</code></pre>

<h2>部署dashboard</h2>

<p>高版本的dashboard出于安全考虑，访问方式发生了变更，需要处理鉴权相关的问题。验证token的方式测试不成功，使用HTTP协议的方式也不成功。最后成功的方式如下：</p>

<pre><code># commit 9159b005f65b21bd6b7156ccabd92f9e50c11333
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
</code></pre>

<p><strong>注意</strong> 替换里面的镜像地址，可以直接在阿里云找对应镜像。</p>

<p>参考<a href="https://stackoverflow.com/questions/46664104/how-to-sign-in-kubernetes-dashboard">stackoverflow</a> 最后一种方法：</p>

<pre><code>$ cat &lt;&lt;EOF | kubectl create -f -
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: kubernetes-dashboard
  labels:
    k8s-app: kubernetes-dashboard
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: kubernetes-dashboard
  namespace: kube-system
EOF
</code></pre>

<p>最后启动proxy，默认端口8001：</p>

<pre><code># 不加disable-filter浏览器端无法访问
kubectl proxy --address `hostname -i` --disable-filter=true
</code></pre>

<p>浏览器端即可访问：<code>http://10.101.83.237:8001/ui</code> 会重定向到新的URI。</p>

<h2>测试</h2>

<p>可以参考官方的例子<a href="https://v1-8.docs.kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/">stateless application sample</a> 进行测试。这一步很顺利没有什么问题。同样，注意配置中镜像地址的修改。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[基于Yarn的分布式应用调度器Slider]]></title>
    <link href="http://codemacro.com/2018/01/24/apache-slider/"/>
    <updated>2018-01-24T00:00:00+08:00</updated>
    <id>http://codemacro.com/2018/01/24/apache-slider</id>
    <content type="html"><![CDATA[<p>Apache Hadoop Map-Reduce
框架为了解决规模增长问题，发展出了yarn。而yarn不仅解决Map-Reduce调度问题，还成为了一个通用的分布式应用调度服务。yarn中的一个创新是把各种不同应用的调度逻辑拆分到了一个称为Application
Manager(以下简称AM)的角色中，从而让yarn自己变得更通用，同时解决调度性能问题。Apache
Slider就是这其中的一个AM具体实现。但Slider进一步做了通用化，可以用于调度长运行(long-running)的分布式应用。</p>

<p>为了更好地理解Slider/Yarn，需要思考这样一个问题：在不用Slider/Yarn这种自动部署并管理应用的软件时，我们如何在一个网络环境中部署一个分布式应用？</p>

<ul>
<li>可能需要在目标物理机上创建虚拟容器，指定容器所用的CPU核数、内存数</li>
<li>到容器中下载或复制应用运行所需的所有软件包</li>
<li>可能需要改写应用所需的各种配置</li>
<li>运行应用，输入可能很长的命令行参数</li>
</ul>


<p>注意这些操作需要在所有需要运行的容器中执行，当然现在也有很多自动部署的工具可以解决这些问题。但是，当应用首次部署运行起来后，继续思考以下问题：</p>

<ul>
<li>某台机器物理原因关机，对应的应用实例不可服务，如何自动发现故障并迁移该实例</li>
<li>应用有突发流量，需要基于当前运行中的版本做扩容</li>
<li>应用需要更新</li>
</ul>


<h2>架构</h2>

<p>看一下yarn的总体架构：</p>

<p><img src="/assets/res/yarn.png" alt="" /></p>

<!-- more -->


<p>yarn管理的每台机器上都会部署Node Manager (简称NM)，NM主要用于创建容器，用户的应用运行在这个容器中。一台机器可能会跑多个应用的实例(Instance)。Resource Manager
(简称RM)用于管理整个集群的资源，例如CPU、内存。App Master(Manager) (简称AM) 用于管理容器中用户的应用。AM本身也运行在容器中。</p>

<p>通过Client提交AM的请求到RM中，RM找到一个可用的NM并启动该AM。随后，AM与RM交互，为应用请求各种资源，并发出应用的部署请求。在运行期间，AM会监视应用每个实例的正确性，以在假设有机器挂掉后，申请新的资源来自动恢复该实例。</p>

<p>为了更具体地了解这个过程，可以先参考<a href="https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html">Writing YARN Applications</a>。</p>

<p>Slider是AM的一种实现，接下来从以下几个方面来了解Slider：</p>

<ul>
<li>Slider架构</li>
<li>如何描述一个应用</li>
<li>使用流程及主要接口</li>
<li>如何定制</li>
<li>其他细节</li>
</ul>


<h2>Slider架构</h2>

<p>借图(<a href="http://www.weixinnu.com/tag/article/2086961513">Slider设计理念与基本架构</a>)：</p>

<p><img src="/assets/res/slider.jpeg" alt="" /></p>

<p>作为一个Yarn AM其结构与之前描述的相差无几。Slider Client是一个命令行程序，它直接与RM交互，提交一个Slider AM包给RM。RM分配资源并配合NM启动Slider AM这个服务程序。Slider AM在启动目标应用时，通常会在目标容器中部署一个Slider
Agent。这个Agent实现了一套与不同应用之间的交互协议，例如：INSTALL/CONFIGURE/START/STOP/STATUS，应用一般通过Python脚本实现这些协议命令，就可以被Slider部署起来。</p>

<h2>如何描述一个应用</h2>

<p>完整地描述一个应用，就可以让第三方调度器，例如Slider，自动地部署应用、迁移应用。在Slider中描述一个应用，主要分为3部分内容：</p>

<ul>
<li>资源描述，resources.json，例如单实例所需要的CPU核数、内存数，总共需要多少实例</li>
<li>应用特定的配置描述，appConfig.json，例如Java应用JVM的内存配置</li>
<li>应用适配协议，这个不是配置文件，一般是Python脚本，实现一个应用实例如何安装、如何启动</li>
</ul>


<p>除了以上内容外，Slider认为一个完整的分布式应用可能包含多个组件(Component)，例如HBase包含Master、Worker。应用描述还应该包含各个组件的描述，例如每个组件可以有自己的资源需求。Component也被称为Role(可能不准确，但意思接近)。当有多个Component时，可以配置每个Component的优先级，高优先级的Component优先得到资源分配。</p>

<p>例如，一个<code>resource.json</code>例子，主要就是指定各个Component的资源：</p>

<pre><code>{
  "schema": "http://example.org/specification/v2.0.0",

  "metadata": {
    "description": "example of a resources file"
  },

  "global": {
    "yarn.vcores": "1",
    "yarn.memory": "512"
  },

  "components": {
    "master": {
      "instances": "1",
      "yarn.vcores": "1",
      "yarn.memory": "1024"
    },
    "worker": {
      "instances":"5",
      "yarn.vcores": "1",
      "yarn.memory": "512"
    }
  }
}
</code></pre>

<p><code>appConfig.json</code>主要就是应用相关的配置参数：</p>

<pre><code>{
  "schema": "http://example.org/specification/v2.0.0",

  "global": {

    "zookeeper.port": "2181",
    "zookeeper.path": "/yarnapps_small_cluster",
    "zookeeper.hosts": "zoo1,zoo2,zoo3",
  },
  "components": {
    "worker": {
      "jvm.heapsize": "512M"
    },
    "master": {
      "jvm.heapsize": "512M"
    }
  }
  "credentials" {
  }
}
</code></pre>

<p>在Slider中，应用需要将这些配置信息以及应用自己部署所需要的各种软件包，按照规范打成一个压缩包。然后使用slider工具提交，slider工具会将包上传至HDFS上。</p>

<p>可以通过<a href="http://slider.incubator.apache.org/docs/slider_specs/hello_world_slider_app.html">Hello World Slider App</a>获得直观的印象。</p>

<h2>使用流程及主要接口</h2>

<p>使用Slider主要就是使用其客户端工具<code>slider</code>。要通过Slider启动一个应用，主要步骤如下：</p>

<ul>
<li>准备好应用包，配置资源描述resources.json、应用配置appConfig.json、开发适配协议脚本。</li>
<li>打包并上传，通过<code>slider install-package</code>完成</li>
<li>提交并部署，通过<code>slider create</code>完成</li>
</ul>


<p>其中，应用的部署和AM自身的部署是一起提交的。在应用部署好后，后续的运维操作都可以通过<code>slider</code>工具完成，例如：</p>

<ul>
<li>对应用做扩缩容: <code>./slider flex cl1 --component worker 5</code></li>
<li>对应用做更新：<code>slider upgrade MyHBase_Facebook_Finance --components HBASE_MASTER HBASE_REGIONSERVER</code></li>
</ul>


<p>对应用做更新时，Slider文档中指出不可以同时做扩缩容。另外，如果更新过程中有部分容器坏掉自动替换，可能会自动更新为新版本。在更新过程中，Slider并不做更新过程的维护，即用户需要自己指定当前希望哪些容器得到更新（或全部更新），用户通过slider工具检查这些容器的版本是否达到预期，并继续更新下一批容器。过程大体如下：</p>

<pre><code># 上传更新包
slider package --install --name MyHBase_Facebook --version 2.0 --package ~/slider-hbase-app-package_v2.0.zip
# upgrades the internal state 
slider upgrade MyHBase_Facebook_Finance --template ~/myHBase_appConfig_v2.0.json --resources ~/myHBase_resources_v2.0.json
# 重复以下步骤，根据应用的需求更新各个容器
slider upgrade MyHBase_Facebook_Finance --containers id1 id2 .. idn
</code></pre>

<p>如果应用有多个Component，Component之间的更新一般是有顺序的，这里Slider交给用户自己去控制。用户也可以控制容器之间的更新间隔。</p>

<p>更新过程的细节参考<a href="http://slider.incubator.apache.org/docs/slider_specs/application_pkg_upgrade.html">Rolling Upgrade</a>。</p>

<h2>如何定制</h2>

<p>Slider中有一个概念叫<code>Provider</code>。provider可以理解为为了支持特定应用类型而开发的插件，以让Slider部署这些特殊的应用。默认的provider就是前面提到的slider agent。这个agent是一堆Python脚本，定义了与应用交互的各种协议。实现自己的provider，需要实现client端和server端。client端会被slider client (前面提到的<code>slider</code>工具)调用，可以用于添加应用所需要的特殊包，以提交为Slider AM。而server端主要指的是Slider AM端，可以定制具体部署应用时的行为，例如部署自己定制的agent。</p>

<p>具体provider例子可以参考源码中slider-providers子项目。</p>

<h2>其他细节</h2>

<p>以下细节仅供记录，未深入了解。</p>

<h3>服务发现</h3>

<p>服务发现用于解决分布式系统中上游服务如何发现下游服务实例，以发出RPC调用。Slider中依靠Yarn的服务发现机制，目前主要是通过zookeeper来自动对服务做注册。参考<a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/registry/yarn-registry.html">The Yarn Service Registry</a>。</p>

<h3>资源分配策略</h3>

<p>Slider在请求yarn分配容器时，可以配置不同的策略，这个称为<code>Placement Policy</code>。例如是否优先跑在打有特殊标签的机器上，或者跑在特定的机架(rack)上。这里的资源标签就是一些普通的文本，例如给一个机器打上<code>gpu</code>标签，而分配容器时，也只是简单的匹配，并没有看到互斥、多标签组合等功能。分配策略可以解决带数据应用的机器亲近性，当应用实例发生迁移时，优先在有历史数据的机器上部署，可以获得更快的启动速度。</p>

<p>具体参考<a href="https://slider.incubator.apache.org/docs/placement.html">Apache Slider Placement</a>。</p>

<h2>总结</h2>

<p>翻看了Slider的文档及部分源码，主要功能了解得七七八八。Slider虽然可以自动调度起一个应用，但是一个用于生产环境的调度器还要在很多细节上做得出色，例如：</p>

<ul>
<li>需要与服务发现深度结合。应用实例在服务发现中的状态能够参与到调度器的调度中，例如是否能做到对上游应用透明地更新</li>
<li>应用实例与agent交互时，STATUS需要表达应用确实可提供服务，并且在运行期间持续透出可服务状态</li>
<li>调度整个集群时，是否能对失败节点做容错，是否会自动recover这些失败节点，也就是我们说的基于目标式的调度实现(level-triggered)</li>
<li>应用的更新是日常运维的常态，更新语义应该基于百分比，而不是基于容器；更新既然是常态，调度器就该处理好在这期间集群里可能发生的任何事情，例如有容器被自动替换</li>
</ul>

]]></content>
  </entry>
  
</feed>
