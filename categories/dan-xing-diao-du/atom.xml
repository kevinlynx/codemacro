<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: 弹性调度 | loop in codes]]></title>
  <link href="http://codemacro.com/categories/dan-xing-diao-du/atom.xml" rel="self"/>
  <link href="http://codemacro.com/"/>
  <updated>2018-05-08T12:22:02+00:00</updated>
  <id>http://codemacro.com/</id>
  <author>
    <name><![CDATA[Kevin Lynx]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[kubernetes网络相关总结]]></title>
    <link href="http://codemacro.com/2018/04/01/kube-network/"/>
    <updated>2018-04-01T00:00:00+00:00</updated>
    <id>http://codemacro.com/2018/04/01/kube-network</id>
    <content type="html"><![CDATA[<p>要理解kubernetes的网络模型涉及到的技术点比较多，网络上各个知识点讲得细的有很多，这里我就大概梳理下整个架构，方便顺着这个脉络深入。本文主要假设kubernetes使用docker+flannel实现。</p>

<p>整体上，了解kubernetes的网络模型，涉及到以下知识：</p>

<ul>
<li>linux网络及网络基础</li>
<li>docker网络模型</li>
<li>kubernetes网络需求，及flannel网络实现</li>
</ul>


<p>最后大家就可以结合实例对照着学习。</p>

<h2>Linux网络</h2>

<p>先看几个概念，引用自<a href="https://www.kubernetes.org.cn/2059.html">Kubernetes网络原理及方案</a>:</p>

<ul>
<li>网络命名空间</li>
</ul>


<blockquote><p>Linux在网络栈中引入网络命名空间，将独立的网络协议栈隔离到不同的命令空间中，彼此间无法通信；docker利用这一特性，实现不同容器间的网络隔离</p></blockquote>

<ul>
<li>网桥</li>
</ul>


<blockquote><p>网桥是一个二层网络设备,通过网桥可以将linux支持的不同的端口连接起来,并实现类似交换机那样的多对多的通信</p></blockquote>

<ul>
<li>Veth设备对</li>
</ul>


<blockquote><p>Veth设备对的引入是为了实现在不同网络命名空间的通信</p></blockquote>

<ul>
<li>路由</li>
</ul>


<blockquote><p>Linux系统包含一个完整的路由功能，当IP层在处理数据发送或转发的时候，会使用路由表来决定发往哪里</p></blockquote>

<p>借图以关联上面的概念：</p>

<p><img src="/assets/res/kubenet/dnet.png" alt="dnet.png" /></p>

<!-- more -->


<p>安装docker后，系统中就会有一个docker0网桥。通过<code>ifconfig</code> 或<code>ip link</code>可以查看(准确的说是查看网络设备？)：</p>

<pre><code>$ip link
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000
    link/ether 00:16:3e:00:03:9a brd ff:ff:ff:ff:ff:ff
3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1500 qdisc noqueue state DOWN mode DEFAULT
    link/ether 02:42:c0:a8:05:01 brd ff:ff:ff:ff:ff:ff
</code></pre>

<p>一个docker POD中，多个容器间是共享网络的。在POD中，有一个默认的网络设备，就像物理机上一样，名为<code>eth0</code>。POD中的<code>eth0</code>通过<code>Veth设备对</code>，借由docker0网桥与外部网络通信。veth设备对同样可以用<code>ip link</code>查看，系统中有多少POD就会有多少veth对：</p>

<pre><code>$ip link
...
219: veth367a306c@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue master cni0 state UP mode DEFAULT 
    link/ether 62:de:88:30:86:fa brd ff:ff:ff:ff:ff:ff link-netnsid 19
220: veth684956fd@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue master cni0 state UP mode DEFAULT 
    link/ether fe:b8:33:8c:25:b0 brd ff:ff:ff:ff:ff:ff link-netnsid 20
222: veth9e23eff7@if3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue master cni0 state UP mode DEFAULT 
    link/ether 7e:e9:2d:f2:28:e5 brd ff:ff:ff:ff:ff:ff link-netnsid 22
</code></pre>

<p>veth对只是不同网络命名空间通信的一种解决方案，还有其他方案，借图：</p>

<p><img src="/assets/res/kubenet/veth.png" alt="veth.png" /></p>

<p>最后，“路由”表可以通过<code>ip route</code>查看，这块内容我理解就是根据网络地址交给不同的设备做处理：</p>

<pre><code>$ip route
default via 10.101.95.247 dev eth0 
...
# 匹配前24bits的地址交由flannel.1设备处理
10.244.0.0/24 via 10.244.0.0 dev flannel.1 onlink 
</code></pre>

<h2>docker网络模型</h2>

<p>docker网络模型用于解决容器间及容器与宿主机间的网络通信，主要分为以下模型：</p>

<ul>
<li>Bridge，默认模式，也就是上面通过<code>docker0</code>做通信的模式</li>
<li>Host，容器内的网络配置同宿主机，没有网络隔离</li>
<li>None，容器内仅有loopback</li>
</ul>


<p>不同的模式通常在启动容器时，可以通过<code>--net=xx</code>参数指定。可以通过<code>docker network inspect [host|bridge]</code>查看本机某种网络模型下启动的容器列表，例如：</p>

<pre><code>$sudo docker network inspect host
[
    {
        "Name": "host",
        "Id": "19958f2d93e0bf428c685c12b084fc5e6a7bd499627d90ae9ae1ca4b11a8f437",
        "Scope": "local",
        "Driver": "host",
        "EnableIPv6": false,
        "IPAM": {
            "Driver": "default",
            "Options": null,
            "Config": []
        },
        "Internal": false,
        "Containers": {
            "03d2b6e1faa3763fefd5528cce29ca454a13220dab693f242ff808051d7fd34b": {
                "Name": "k8s_POD_kube-proxy-69rl4_kube-system_b793f549-22a0-11e8-9ce1-00163e00039a_0",
                "EndpointID": "4380fab27d237e21f7fa6f84cf23b8a6d605f4e612968a3377f853abf987f6a4",
                "MacAddress": "",
                "IPv4Address": "",
                "IPv6Address": ""
            },
            "9140fdf7a3ea0b19fec3a922e998ba3c544d328c1bd372525dfef816c620a431": {
                "Name": "k8s_POD_kube-flannel-ds-m88p6_kube-system_b793f466-22a0-11e8-9ce1-00163e00039a_0",
                "EndpointID": "01024d8050167cd88c11220375f89217116b5c929575a45695fe5d759038f320",
                "MacAddress": "",
                "IPv4Address": "",
                "IPv6Address": ""
            }
        },
        "Options": {},
        "Labels": {}
    }
]
</code></pre>

<p>docker解决了单机容器网络隔离与网络通信，但没有解决多机间的通信。</p>

<h2>Kubernetes网络模型及Flannel实现</h2>

<p>kubernetes为了简单，它要求的网络模型里，整个集群中所有的POD都有独立唯一的IP。POD间互相看到的IP是稳定的，是直接可达的，是一个平铺网络。如何实现kubernetes的这个网络需求，CNI插件是一种解决方案，而Flannel项目则是CNI插件的一种实现。</p>

<p>Flannel的实现，本质上主要是两方面内容：</p>

<ul>
<li>一个proxy做流量转发，也就是flanneld进程，可以自行ps查看</li>
<li>通过中心化存储etcd来对整个集群做POD的IP分配</li>
</ul>


<p>借图，了解flannel网络原理：</p>

<p><img src="/assets/res/kubenet/flannel.png" alt="service.png" /></p>

<p>flannel网络上已经有很多资料，例如<a href="http://dockone.io/article/618">一篇文章带你了解Flannel</a>。flannel的网络通信可以配置不同的backend，例如，配置UDP为backend时，那么交由flanneld进程转发网络包时，就会以UDP包的形式包装起来做转发，到达对端的flanneld进程时再解包。当然，实际使用时是不会使用这种方式的，通常会使用由内核支持的vxLan。参考<a href="https://github.com/coreos/flannel/blob/master/Documentation/backends.md">flannel backend配置</a>。</p>

<p>在安装kubernetes时，基于kube-flannel.yml文件配置flannel就使用的vxLan：</p>

<pre><code>net-conf.json: |
{
  "Network": "10.244.0.0/16",
  "Backend": {
    "Type": "vxlan"
  }
}
</code></pre>

<p>docker在创建容器时，会给容器分配IP，flannel的安装里会hack docker daemon的启动方式，增加<code>--bip</code>参数，用于限定docker分配的IP范围，这也是网络上很多文章提到的。在我的环境里，docker容器全部以host方式设置网络，所以有没有设置bip也无所谓。</p>

<p>安装了flannel后，在kubernetes网络中就可以直接ping一个POD容器。到这里，可以小结一下，基于以上的技术，在一个集群中，每一个POD都可以有一个独立唯一的IP，其他宿主机的POD可以直接访问这个POD。但是，一个分布式服务通常处于性能和稳定性考虑会有多个实例，所以kubernetes还要解决负载均衡问题。</p>

<h2>Kubernetes中的负载均衡</h2>

<p>kubernetes中通过service概念来对应用做多POD间的负载均衡。service是一个虚拟概念，service都会被分配一个<code>clusterIP</code>，其实就是service对外暴露的地址。你可以为一个redis服务暴露一个service，然后将这个service的clusterIP传给一个PHP应用，以让PHP应用访问redis。那么，这个PHP应用具体是如何通过这个service clusterIP负载均衡到redis的多个容器呢？在kubernetees中，目前主要是通过iptable来实现。借图：</p>

<p><img src="/assets/res/kubenet/service.png" alt="service.png" /></p>

<p>要理解上图，首先大概要有一个iptable的知识，大概就是在整个包收发处理过程中可以配置很多链，当一个链的条件被满足时，就可以执行一个动作。这里，我们主要关注上图中的左边分支部分。本质上，拿到一个service的clusterIP，到发送到目的POD，整个过程主要涉及到内容：</p>

<ul>
<li>service clusterIP是虚拟的，是一个iptable规则，这个规则最终会映射到一个POD的IP</li>
<li>一个应用有多少POD，那么在集群中的每台机器上，就会有多少iptable链</li>
</ul>


<p>在kubernetes集群中，可以实际追踪看看。例如上面提到的redis service clusterIP为<code>10.99.136.250</code>：</p>

<pre><code>$sudo iptables-save | grep 10.99.136.250
# -d 10.99.136.250/32 地址完全匹配，然后执行 -j KUBE-SVC-AGR3D4D4FQNH4O33 规则
-A KUBE-SERVICES -d 10.99.136.250/32 -p tcp -m comment --comment "default/redis-slave: cluster IP" -m tcp --dport 6379 -j KUBE-SVC-AGR3D4D4FQNH4O33
# 这条规则对应到上图中`source!=podIP`，主要用于配合SNAT处理，简单理解为改写封包源IP
-A KUBE-SERVICES ! -s 10.244.0.0/16 -d 10.99.136.250/32 -p tcp -m comment --comment "default/redis-slave: cluster IP" -m tcp --dport 6379 -j KUBE-MARK-MASQ
</code></pre>

<p>追<code>KUBE-SVC-AGR3D4D4FQNH4O33</code>规则：</p>

<pre><code>$sudo iptables-save | grep KUBE-SVC-AGR3D4D4FQNH4O33
:KUBE-SVC-AGR3D4D4FQNH4O33 - [0:0]
-A KUBE-SVC-AGR3D4D4FQNH4O33 -m comment --comment "default/redis-slave:" -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-GDIX2RIKQIYS7RMI
-A KUBE-SVC-AGR3D4D4FQNH4O33 -m comment --comment "default/redis-slave:" -j KUBE-SEP-J5QZN63T7ON4OKV7
...
</code></pre>

<p>因为<code>redis-slave</code>有2个POD，所以上面通过<code>--probability 0.50</code>实现了50%的流量负载均衡。<code>KUBE-SEP-XX</code>就是各个POD的地址，可以继续追，例如：</p>

<pre><code>$sudo iptables-save | grep KUBE-SEP-GDIX2RIKQIYS7RMI
:KUBE-SEP-GDIX2RIKQIYS7RMI - [0:0]
-A KUBE-SEP-GDIX2RIKQIYS7RMI -s 10.244.1.56/32 -m comment --comment "default/redis-slave:" -j KUBE-MARK-MASQ
-A KUBE-SEP-GDIX2RIKQIYS7RMI -p tcp -m comment --comment "default/redis-slave:" -m tcp -j DNAT --to-destination 10.244.1.56:6379
</code></pre>

<p>最终，封包发送到POD之一<code>10.244.1.56</code>。上面的过程，对着前面图的左半部分很容易理解。</p>

<p>kubernetes中还提供了kube-dns，主要是为service clusterIP提供一个域名。每个service都可以按固定格式拼出一个域名，而kube-dns则负责解析这个域名，解析为service的clusterIP，实际网络数据传输流程还是同上。</p>

<h2>总结</h2>

<p>熟悉kubernetes，几大组件的工作原理其实不难理解。而kubernetes网络模型显得更难，尤其是要结合各种工具、命令去实践时，很容易与理论脱节。一旦梳理通彻整个链路，回头看时就会发现也就那么回事。当然，上面提到的各个技术点，深入下去了解还是有很多内容的。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[使用Kubeadm在CentOS部署Kubernets 1.8.7]]></title>
    <link href="http://codemacro.com/2018/03/08/deploy-kubernetes/"/>
    <updated>2018-03-08T00:00:00+00:00</updated>
    <id>http://codemacro.com/2018/03/08/deploy-kubernetes</id>
    <content type="html"><![CDATA[<p>主要参考：</p>

<ul>
<li><a href="https://v1-8.docs.kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">官方文档</a></li>
<li><a href="https://my.oschina.net/xdatk/blog/895645">如何在国内愉快的安装 Kubernetes</a></li>
<li><a href="https://my.oschina.net/andylo25/blog/1618342">kubernetes 1.8.7 国内安装(kubeadm)</a></li>
</ul>


<p>建议都大致浏览下。这里我也是简单地记录，估计每个人遇到的细节问题不一样。</p>

<h2>环境准备</h2>

<p>我拿到手的环境docker已经ready：</p>

<ul>
<li>docker (alidocker-1.12.6.22)</li>
<li>CentOS 7</li>
</ul>


<p>上面博客提到的一些系统设置可以先做掉：</p>

<pre><code>cat &lt;&lt;EOF &gt;  /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
EOF
sysctl -p /etc/sysctl.d/k8s.conf
</code></pre>

<p>其他一些设置：</p>

<ul>
<li>防火墙最好关闭</li>
<li>swap最好关闭</li>
<li><code>setenforce 0</code></li>
</ul>


<!-- more -->


<h2>软件包及镜像</h2>

<p>众所周知的局域网问题，官方的很多软件包和镜像无法获取。解决这个问题主要靠阿里云：</p>

<ul>
<li>配置yum源，由于阿里云yum源相对官方有滞后，并且各个软件包版本匹配我没有找到，所以就按照上面博客提到的，安装1.8.7，避免版本问题：</li>
</ul>


<pre><code>cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg
        https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
EOF
</code></pre>

<p>可以把包下到本地以备不时之需：</p>

<pre><code>yum install -y --downloadonly --downloaddir=./ kubelet-1.8.7 kubeadm-1.8.7 kubectl-1.8.7 kubernetes-cni-0.5.1
yum install -y *.rpm
</code></pre>

<ul>
<li>docker镜像问题后来我发现可以从阿里云直接拉，拉下来后通过打tag避免后续部署从google官方拉</li>
</ul>


<p>以下脚本内容未验证，主要注意镜像得提前拉全(上面链接的博客有写漏了)</p>

<pre><code>#!/bin/sh

images=(kube-scheduler-amd64:v1.8.7 \
kube-proxy-amd64:v1.8.7 \
kube-apiserver-amd64:v1.8.7 \
etcd-amd64:3.0.17 \
pause-amd64:3.0 \
k8s-dns-sidecar-amd64:1.14.5 \
k8s-dns-kube-dns-amd64:1.14.5 \
k8s-dns-dnsmasq-nanny-amd64:1.14.5 \
kubernetes-dashboard-amd64:v1.8.1)

for imageName in ${images[@]} ; do
  docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
  docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName gcr.io/google_containers/$imageName
  docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
done

# 注意这个镜像最好提前拉，看flannel的yaml描述里会用到
docker pull quay.io/coreos/flannel:v0.9.1-amd64
</code></pre>

<p>贴个我部署好后的镜像列表(自行忽略多余的)，方便对比：</p>

<pre><code>$sudo docker images
REPOSITORY                                                                          TAG                 IMAGE ID            CREATED             SIZE
registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64      v1.8.3              0c60bcf89900        3 weeks ago         102.3 MB
gcr.io/google_containers/kubernetes-dashboard-amd64                                 v1.8.1              63c78846e37b        4 weeks ago         120.7 MB
gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64                                1.14.5              6d7cc6e484b3        4 weeks ago         41.42 MB
gcr.io/google_containers/k8s-dns-kube-dns-amd64                                     1.14.5              33072cb8c892        4 weeks ago         49.38 MB
gcr.io/google_containers/k8s-dns-sidecar-amd64                                      1.14.5              e1414b167ca6        4 weeks ago         41.81 MB
gcr.io/google_containers/pause-amd64                                                3.0                 8ca66ae4813a        4 weeks ago         746.9 kB
gcr.io/google_containers/etcd-amd64                                                 3.0.17              10010bfa0a74        4 weeks ago         168.9 MB
gcr.io/google_containers/kube-scheduler-amd64                                       v1.8.7              906029e1500b        4 weeks ago         55.13 MB
gcr.io/google_containers/kube-apiserver-amd64                                       v1.8.7              c3bb648343de        4 weeks ago         194.7 MB
gcr.io/google_containers/kube-proxy-amd64                                           v1.8.7              125dec6bd8f2        7 weeks ago         93.36 MB
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-proxy-amd64                v1.8.7              125dec6bd8f2        7 weeks ago         93.36 MB
gcr.io/google_containers/kube-controller-manager-amd64                              v1.8.7              d8df883aabf9        7 weeks ago         129.6 MB
registry.cn-hangzhou.aliyuncs.com/google_containers/kube-controller-manager-amd64   v1.8.7              d8df883aabf9        7 weeks ago         129.6 MB
quay.io/coreos/flannel                                                              v0.9.1-amd64        2b736d06ca4c        3 months ago        51.31 MB
</code></pre>

<h2>启动master</h2>

<pre><code>systemctl enable kubelet
systemctl start kubelet
</code></pre>

<p>注意，这里官方文档也提到了，需要确认docker的cgroup配置与kubelet的一致：</p>

<pre><code>docker info | grep -i cgroup
cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
</code></pre>

<p>不一致改成一致。例如：</p>

<pre><code>sed -i "s/cgroup-driver=systemd/cgroup-driver=cgroupfs/g" /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
</code></pre>

<p>配置有变更需要重启kubelet：</p>

<pre><code>systemctl daemon-reload
systemctl restart kubelet
</code></pre>

<p><code>systemctl status kubelet</code>查看日志会发现里面有warning/error级别的日志，所以出问题时很容易被误解。</p>

<p>按照官方文档，在这一步时，kubelet会不断重启，所以这个时候可以继续，准备初始化master：</p>

<pre><code>#!/bin/bash
kubeadm init --kubernetes-version=v1.8.7 --pod-network-cidr 10.244.0.0/16

mkdir -p $HOME/.kube
cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
chown $(id -u):$(id -g) $HOME/.kube/config

kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/v0.9.1/Documentation/kube-flannel.yml
</code></pre>

<p>注意上面flannel的版本与前面的链接不同，v0.9.1测试可用。当flannel启动后，kubelet就会发现这个CNI实现，从而趋于成功运行状态。</p>

<p>可以通过以下命令来确认master是否真的初始化成功：</p>

<pre><code>$sudo kubectl get node
NAME                 STATUS    ROLES     AGE       VERSION
v101083237zsqazzmf   Ready     master    12m       v1.8.7

$sudo kubectl get pod --all-namespaces
NAMESPACE     NAME                                         READY     STATUS    RESTARTS   AGE
kube-system   etcd-v101083237zsqazzmf                      1/1       Running   0          11m
kube-system   kube-apiserver-v101083237zsqazzmf            1/1       Running   0          11m
kube-system   kube-controller-manager-v101083237zsqazzmf   1/1       Running   0          11m
kube-system   kube-dns-545bc4bfd4-f7hsx                    3/3       Running   0          11m
kube-system   kube-flannel-ds-d6z78                        1/1       Running   5          6m
kube-system   kube-proxy-z88cd                             1/1       Running   0          11m
kube-system   kube-scheduler-v101083237zsqazzmf            1/1       Running   0          11m

$sudo kubectl get cs
NAME                 STATUS    MESSAGE              ERROR
controller-manager   Healthy   ok                   
scheduler            Healthy   ok                   
etcd-0               Healthy   {"health": "true"}  
</code></pre>

<p>master初始成功会返回加入node的token，例如：</p>

<pre><code>kubeadm join --token 795c0c.c5a1b252c2d23a0c 10.101.83.237:6443 --discovery-token-ca-cert-hash sha256:1c7760c9f02e2f058de3f6bc759e85316b65376cfcc83d975ea6c64ac2175ecc
</code></pre>

<p>注意token默认24小时过期。如果在这个过程中始终有问题，可以做reset回到干净状态：</p>

<pre><code>kubeadm reset
</code></pre>

<h2>部署node及加入</h2>

<ul>
<li>系统设置同上</li>
<li>前面提到的软件需要安装</li>
<li>镜像只需要部分</li>
<li>运行kubelet的cgroup设置参考前面</li>
</ul>


<pre><code>images=(kube-proxy-amd64:v1.8.7 \
pause-amd64:3.0 \
kubernetes-dashboard-amd64:1.8.1)
images=(kube-proxy-amd64:v1.8.7)
for imageName in ${images[@]} ; do
  docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
  docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName gcr.io/google_containers/$imageName
  docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName
done
</code></pre>

<p>贴下镜像列表：</p>

<pre><code>$sudo docker images
REPOSITORY                                                                       TAG                 IMAGE ID            CREATED             SIZE
registry.cn-hangzhou.aliyuncs.com/google_containers/kubernetes-dashboard-amd64   v1.8.3              0c60bcf89900        3 weeks ago         102.3 MB
gcr.io/google_containers/kube-proxy-amd64                                        v1.8.7              263b722c47c1        4 weeks ago         93.36 MB
gcr.io/google_containers/pause-amd64                                             3.0                 8ca66ae4813a        4 weeks ago         746.9 kB
gcr.io/google_containers/kubernetes-dashboard-amd64                              1.8.1               758ae6af38ca        5 weeks ago         120.7 MB
quay.io/coreos/flannel                                                           v0.9.1-amd64        2b736d06ca4c        3 months ago        51.31 MB
</code></pre>

<p>最后使用<code>kubeadm join xxxx</code> (部署master时会返回)加入网络。加入后在master端可以确认：</p>

<pre><code>$sudo kubectl get nodes
NAME                 STATUS    ROLES     AGE       VERSION
v101083225zsqazzmf   Ready     &lt;none&gt;    2h        v1.8.7
v101083237zsqazzmf   Ready     master    3h        v1.8.7
</code></pre>

<h2>部署dashboard</h2>

<p>高版本的dashboard出于安全考虑，访问方式发生了变更，需要处理鉴权相关的问题。验证token的方式测试不成功，使用HTTP协议的方式也不成功。最后成功的方式如下：</p>

<pre><code># commit 9159b005f65b21bd6b7156ccabd92f9e50c11333
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml
</code></pre>

<p><strong>注意</strong> 替换里面的镜像地址，可以直接在阿里云找对应镜像。</p>

<p>参考<a href="https://stackoverflow.com/questions/46664104/how-to-sign-in-kubernetes-dashboard">stackoverflow</a> 最后一种方法：</p>

<pre><code>$ cat &lt;&lt;EOF | kubectl create -f -
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: kubernetes-dashboard
  labels:
    k8s-app: kubernetes-dashboard
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-admin
subjects:
- kind: ServiceAccount
  name: kubernetes-dashboard
  namespace: kube-system
EOF
</code></pre>

<p>最后启动proxy，默认端口8001：</p>

<pre><code># 不加disable-filter浏览器端无法访问
kubectl proxy --address `hostname -i` --disable-filter=true
</code></pre>

<p>浏览器端即可访问：<code>http://10.101.83.237:8001/ui</code> 会重定向到新的URI。</p>

<h2>测试</h2>

<p>可以参考官方的例子<a href="https://v1-8.docs.kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/">stateless application sample</a> 进行测试。这一步很顺利没有什么问题。同样，注意配置中镜像地址的修改。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[基于Yarn的分布式应用调度器Slider]]></title>
    <link href="http://codemacro.com/2018/01/24/apache-slider/"/>
    <updated>2018-01-24T00:00:00+00:00</updated>
    <id>http://codemacro.com/2018/01/24/apache-slider</id>
    <content type="html"><![CDATA[<p>Apache Hadoop Map-Reduce
框架为了解决规模增长问题，发展出了yarn。而yarn不仅解决Map-Reduce调度问题，还成为了一个通用的分布式应用调度服务。yarn中的一个创新是把各种不同应用的调度逻辑拆分到了一个称为Application
Manager(以下简称AM)的角色中，从而让yarn自己变得更通用，同时解决调度性能问题。Apache
Slider就是这其中的一个AM具体实现。但Slider进一步做了通用化，可以用于调度长运行(long-running)的分布式应用。</p>

<p>为了更好地理解Slider/Yarn，需要思考这样一个问题：在不用Slider/Yarn这种自动部署并管理应用的软件时，我们如何在一个网络环境中部署一个分布式应用？</p>

<ul>
<li>可能需要在目标物理机上创建虚拟容器，指定容器所用的CPU核数、内存数</li>
<li>到容器中下载或复制应用运行所需的所有软件包</li>
<li>可能需要改写应用所需的各种配置</li>
<li>运行应用，输入可能很长的命令行参数</li>
</ul>


<p>注意这些操作需要在所有需要运行的容器中执行，当然现在也有很多自动部署的工具可以解决这些问题。但是，当应用首次部署运行起来后，继续思考以下问题：</p>

<ul>
<li>某台机器物理原因关机，对应的应用实例不可服务，如何自动发现故障并迁移该实例</li>
<li>应用有突发流量，需要基于当前运行中的版本做扩容</li>
<li>应用需要更新</li>
</ul>


<h2>架构</h2>

<p>看一下yarn的总体架构：</p>

<p><img src="/assets/res/yarn.png" alt="" /></p>

<!-- more -->


<p>yarn管理的每台机器上都会部署Node Manager (简称NM)，NM主要用于创建容器，用户的应用运行在这个容器中。一台机器可能会跑多个应用的实例(Instance)。Resource Manager
(简称RM)用于管理整个集群的资源，例如CPU、内存。App Master(Manager) (简称AM) 用于管理容器中用户的应用。AM本身也运行在容器中。</p>

<p>通过Client提交AM的请求到RM中，RM找到一个可用的NM并启动该AM。随后，AM与RM交互，为应用请求各种资源，并发出应用的部署请求。在运行期间，AM会监视应用每个实例的正确性，以在假设有机器挂掉后，申请新的资源来自动恢复该实例。</p>

<p>为了更具体地了解这个过程，可以先参考<a href="https://hadoop.apache.org/docs/stable/hadoop-yarn/hadoop-yarn-site/WritingYarnApplications.html">Writing YARN Applications</a>。</p>

<p>Slider是AM的一种实现，接下来从以下几个方面来了解Slider：</p>

<ul>
<li>Slider架构</li>
<li>如何描述一个应用</li>
<li>使用流程及主要接口</li>
<li>如何定制</li>
<li>其他细节</li>
</ul>


<h2>Slider架构</h2>

<p>借图(<a href="http://www.weixinnu.com/tag/article/2086961513">Slider设计理念与基本架构</a>)：</p>

<p><img src="/assets/res/slider.jpeg" alt="" /></p>

<p>作为一个Yarn AM其结构与之前描述的相差无几。Slider Client是一个命令行程序，它直接与RM交互，提交一个Slider AM包给RM。RM分配资源并配合NM启动Slider AM这个服务程序。Slider AM在启动目标应用时，通常会在目标容器中部署一个Slider
Agent。这个Agent实现了一套与不同应用之间的交互协议，例如：INSTALL/CONFIGURE/START/STOP/STATUS，应用一般通过Python脚本实现这些协议命令，就可以被Slider部署起来。</p>

<h2>如何描述一个应用</h2>

<p>完整地描述一个应用，就可以让第三方调度器，例如Slider，自动地部署应用、迁移应用。在Slider中描述一个应用，主要分为3部分内容：</p>

<ul>
<li>资源描述，resources.json，例如单实例所需要的CPU核数、内存数，总共需要多少实例</li>
<li>应用特定的配置描述，appConfig.json，例如Java应用JVM的内存配置</li>
<li>应用适配协议，这个不是配置文件，一般是Python脚本，实现一个应用实例如何安装、如何启动</li>
</ul>


<p>除了以上内容外，Slider认为一个完整的分布式应用可能包含多个组件(Component)，例如HBase包含Master、Worker。应用描述还应该包含各个组件的描述，例如每个组件可以有自己的资源需求。Component也被称为Role(可能不准确，但意思接近)。当有多个Component时，可以配置每个Component的优先级，高优先级的Component优先得到资源分配。</p>

<p>例如，一个<code>resource.json</code>例子，主要就是指定各个Component的资源：</p>

<pre><code>{
  "schema": "http://example.org/specification/v2.0.0",

  "metadata": {
    "description": "example of a resources file"
  },

  "global": {
    "yarn.vcores": "1",
    "yarn.memory": "512"
  },

  "components": {
    "master": {
      "instances": "1",
      "yarn.vcores": "1",
      "yarn.memory": "1024"
    },
    "worker": {
      "instances":"5",
      "yarn.vcores": "1",
      "yarn.memory": "512"
    }
  }
}
</code></pre>

<p><code>appConfig.json</code>主要就是应用相关的配置参数：</p>

<pre><code>{
  "schema": "http://example.org/specification/v2.0.0",

  "global": {

    "zookeeper.port": "2181",
    "zookeeper.path": "/yarnapps_small_cluster",
    "zookeeper.hosts": "zoo1,zoo2,zoo3",
  },
  "components": {
    "worker": {
      "jvm.heapsize": "512M"
    },
    "master": {
      "jvm.heapsize": "512M"
    }
  }
  "credentials" {
  }
}
</code></pre>

<p>在Slider中，应用需要将这些配置信息以及应用自己部署所需要的各种软件包，按照规范打成一个压缩包。然后使用slider工具提交，slider工具会将包上传至HDFS上。</p>

<p>可以通过<a href="http://slider.incubator.apache.org/docs/slider_specs/hello_world_slider_app.html">Hello World Slider App</a>获得直观的印象。</p>

<h2>使用流程及主要接口</h2>

<p>使用Slider主要就是使用其客户端工具<code>slider</code>。要通过Slider启动一个应用，主要步骤如下：</p>

<ul>
<li>准备好应用包，配置资源描述resources.json、应用配置appConfig.json、开发适配协议脚本。</li>
<li>打包并上传，通过<code>slider install-package</code>完成</li>
<li>提交并部署，通过<code>slider create</code>完成</li>
</ul>


<p>其中，应用的部署和AM自身的部署是一起提交的。在应用部署好后，后续的运维操作都可以通过<code>slider</code>工具完成，例如：</p>

<ul>
<li>对应用做扩缩容: <code>./slider flex cl1 --component worker 5</code></li>
<li>对应用做更新：<code>slider upgrade MyHBase_Facebook_Finance --components HBASE_MASTER HBASE_REGIONSERVER</code></li>
</ul>


<p>对应用做更新时，Slider文档中指出不可以同时做扩缩容。另外，如果更新过程中有部分容器坏掉自动替换，可能会自动更新为新版本。在更新过程中，Slider并不做更新过程的维护，即用户需要自己指定当前希望哪些容器得到更新（或全部更新），用户通过slider工具检查这些容器的版本是否达到预期，并继续更新下一批容器。过程大体如下：</p>

<pre><code># 上传更新包
slider package --install --name MyHBase_Facebook --version 2.0 --package ~/slider-hbase-app-package_v2.0.zip
# upgrades the internal state 
slider upgrade MyHBase_Facebook_Finance --template ~/myHBase_appConfig_v2.0.json --resources ~/myHBase_resources_v2.0.json
# 重复以下步骤，根据应用的需求更新各个容器
slider upgrade MyHBase_Facebook_Finance --containers id1 id2 .. idn
</code></pre>

<p>如果应用有多个Component，Component之间的更新一般是有顺序的，这里Slider交给用户自己去控制。用户也可以控制容器之间的更新间隔。</p>

<p>更新过程的细节参考<a href="http://slider.incubator.apache.org/docs/slider_specs/application_pkg_upgrade.html">Rolling Upgrade</a>。</p>

<h2>如何定制</h2>

<p>Slider中有一个概念叫<code>Provider</code>。provider可以理解为为了支持特定应用类型而开发的插件，以让Slider部署这些特殊的应用。默认的provider就是前面提到的slider agent。这个agent是一堆Python脚本，定义了与应用交互的各种协议。实现自己的provider，需要实现client端和server端。client端会被slider client (前面提到的<code>slider</code>工具)调用，可以用于添加应用所需要的特殊包，以提交为Slider AM。而server端主要指的是Slider AM端，可以定制具体部署应用时的行为，例如部署自己定制的agent。</p>

<p>具体provider例子可以参考源码中slider-providers子项目。</p>

<h2>其他细节</h2>

<p>以下细节仅供记录，未深入了解。</p>

<h3>服务发现</h3>

<p>服务发现用于解决分布式系统中上游服务如何发现下游服务实例，以发出RPC调用。Slider中依靠Yarn的服务发现机制，目前主要是通过zookeeper来自动对服务做注册。参考<a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/registry/yarn-registry.html">The Yarn Service Registry</a>。</p>

<h3>资源分配策略</h3>

<p>Slider在请求yarn分配容器时，可以配置不同的策略，这个称为<code>Placement Policy</code>。例如是否优先跑在打有特殊标签的机器上，或者跑在特定的机架(rack)上。这里的资源标签就是一些普通的文本，例如给一个机器打上<code>gpu</code>标签，而分配容器时，也只是简单的匹配，并没有看到互斥、多标签组合等功能。分配策略可以解决带数据应用的机器亲近性，当应用实例发生迁移时，优先在有历史数据的机器上部署，可以获得更快的启动速度。</p>

<p>具体参考<a href="https://slider.incubator.apache.org/docs/placement.html">Apache Slider Placement</a>。</p>

<h2>总结</h2>

<p>翻看了Slider的文档及部分源码，主要功能了解得七七八八。Slider虽然可以自动调度起一个应用，但是一个用于生产环境的调度器还要在很多细节上做得出色，例如：</p>

<ul>
<li>需要与服务发现深度结合。应用实例在服务发现中的状态能够参与到调度器的调度中，例如是否能做到对上游应用透明地更新</li>
<li>应用实例与agent交互时，STATUS需要表达应用确实可提供服务，并且在运行期间持续透出可服务状态</li>
<li>调度整个集群时，是否能对失败节点做容错，是否会自动recover这些失败节点，也就是我们说的基于目标式的调度实现(level-triggered)</li>
<li>应用的更新是日常运维的常态，更新语义应该基于百分比，而不是基于容器；更新既然是常态，调度器就该处理好在这期间集群里可能发生的任何事情，例如有容器被自动替换</li>
</ul>

]]></content>
  </entry>
  
</feed>
