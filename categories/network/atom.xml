<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: network | loop in codes]]></title>
  <link href="http://codemacro.com/categories/network/atom.xml" rel="self"/>
  <link href="http://codemacro.com/"/>
  <updated>2017-02-05T20:26:17+08:00</updated>
  <id>http://codemacro.com/</id>
  <author>
    <name><![CDATA[Kevin Lynx]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[图解zookeeper FastLeader选举算法]]></title>
    <link href="http://codemacro.com/2014/10/19/zk-fastleaderelection/"/>
    <updated>2014-10-19T00:00:00+08:00</updated>
    <id>http://codemacro.com/2014/10/19/zk-fastleaderelection</id>
    <content type="html"><![CDATA[<p>zookeeper配置为集群模式时，在启动或异常情况时会选举出一个实例作为Leader。其默认选举算法为<code>FastLeaderElection</code>。</p>

<p>不知道zookeeper的可以考虑这样一个问题：某个服务可以配置为多个实例共同构成一个集群对外提供服务。其每一个实例本地都存有冗余数据，每一个实例都可以直接对外提供读写服务。在这个集群中为了保证数据的一致性，需要有一个Leader来协调一些事务。那么问题来了：如何确定哪一个实例是Leader呢？</p>

<p>问题的难点在于：</p>

<ul>
<li>没有一个仲裁者来选定Leader</li>
<li>每一个实例本地可能已经存在数据，不确定哪个实例上的数据是最新的</li>
</ul>


<p>分布式选举算法正是用来解决这个问题的。</p>

<p>本文基于zookeeper 3.4.6 的源码进行分析。FastLeaderElection算法的源码全部位于<code>FastLeaderElection.java</code>文件中，其对外接口为<code>FastLeaderElection.lookForLeader</code>，该接口是一个同步接口，直到选举结束才会返回。同样由于网上已有类似文章，所以我就从图示的角度来阐述。阅读一些其他文章有利于获得初步印象：</p>

<ul>
<li><a href="http://iwinit.iteye.com/blog/1773531">深入浅出Zookeeper之五 Leader选举</a>，代码导读</li>
<li><a href="http://blog.csdn.net/xhh198781/article/details/6619203">zookeeper3.3.3源码分析(二)FastLeader选举算法</a>，文字描述较细</li>
</ul>


<h2>主要流程</h2>

<p>阅读代码和以上推荐文章可以把整个流程梳理清楚。实现上，包括了一个消息处理主循环，也是选举的主要逻辑，以及一个消息发送队列处理线程和消息解码线程。主要流程可概括为下图：</p>

<!-- more -->


<p><img src="/assets/res/zk/fle-flow.png" alt="fle-flow.png" /></p>

<p>推荐对照着推荐的文章及代码理解，不赘述。</p>

<p>我们从感性上来理解这个算法。</p>

<p>每一个节点，相当于一个选民，他们都有自己的推荐人，最开始他们都推荐自己。谁更适合成为Leader有一个简单的规则，例如sid够大（配置）、持有的数据够新(zxid够大)。每个选民都告诉其他选民自己目前的推荐人是谁，类似于出去搞宣传拉拢其他选民。每一个选民发现有比自己更适合的人时就转而推荐这个更适合的人。最后，大部分人意见一致时，就可以结束选举。</p>

<p>就这么简单。总体上有一种不断演化逼近结果的感觉。</p>

<p>当然，会有些特殊情况的处理。例如总共3个选民，1和2已经确定3是Leader，但3还不知情，此时就走入<code>LEADING/FOLLOWING</code>的分支，选民3只是接收结果。</p>

<p>代码中不是所有逻辑都在这个大流程中完成的。在接收消息线程中，还可能单独地回应某个节点(<code>WorkerReceiver.run</code>)：</p>

<p><img src="/assets/res/zk/recv.png" alt="recv.png" /></p>

<p>从这里可以看出，当某个节点已经确定选举结果不再处于<code>LOOKING</code>状态时，其收到<code>LOOKING</code>消息时都会直接回应选举的最终结果。结合上面那个比方，相当于某次选举结束了，这个时候来了选民4又发起一次新的选举，那么其他选民就直接告诉它当前的Leader情况。相当于，在这个集群主从已经就绪的情况下，又开启了一个实例，这个实例就会直接使用当前的选举结果。</p>

<h2>状态转换</h2>

<p>每个节点上有一些关键的数据结构：</p>

<ul>
<li>当前推荐人，初始推荐自己，每次收到其他更好的推荐人时就更新</li>
<li>其他人的投票集合，用于确定何时选举结束</li>
</ul>


<p>每次推荐人更新时就会进行广播，正是这个不断地广播驱动整个算法趋向于结果。假设有3个节点A/B/C，其都还没有数据，按照sid关系为C>B>A，那么按照规则，C更可能成为Leader，其各个节点的状态转换为：</p>

<p><img src="/assets/res/zk/state.png" alt="state.png" /></p>

<p>图中，v(A)表示当前推荐人为A；r[]表示收到的投票集合。需要注意一个细节，初始投票集合里包含了自己的投票，代码中自己会将推荐人推荐给自己，网络模块(<code>QuorumCnxManager</code>)直接将该消息放入接收队列。</p>

<p>可以看看当其他节点已经确定投票结果时，即不再是<code>LOOKING</code>时的状态：</p>

<p><img src="/assets/res/zk/state-ret.png" alt="state-ret.png" /></p>

<p>代码中有一个特殊的投票集合<code>outofelection</code>，我理解为选举已结束的那些投票，这些投票仅用于表征选举结果。</p>

<p>当一个新启动的节点加入集群时，它对集群内其他节点发出投票请求，而其他节点已不处于<code>LOOKING</code>状态，此时其他节点回应选举结果，该节点收集这些结果到<code>outofelection</code>中，最终在收到合法LEADER消息且这些选票也构成选举结束条件时，该节点就结束自己的选举行为。<em>注意到代码中会<code>logicalclock = n.electionEpoch;</code>更新选举轮数</em></p>

<p><em>完</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[图解分布式一致性协议Paxos]]></title>
    <link href="http://codemacro.com/2014/10/15/explain-poxos/"/>
    <updated>2014-10-15T00:00:00+08:00</updated>
    <id>http://codemacro.com/2014/10/15/explain-poxos</id>
    <content type="html"><![CDATA[<p>Paxos协议/算法是分布式系统中比较重要的协议，它有多重要呢？</p>

<p><a href="http://coolshell.cn/articles/10910.html">&lt;分布式系统的事务处理></a>：</p>

<blockquote><p>Google Chubby的作者Mike Burrows说过这个世界上只有一种一致性算法，那就是Paxos，其它的算法都是残次品。</p></blockquote>

<p><a href="http://book.douban.com/subject/25723658/">&lt;大规模分布式存储系统></a>：</p>

<blockquote><p>理解了这两个分布式协议之后(Paxos/2PC)，学习其他分布式协议会变得相当容易。</p></blockquote>

<p>学习Paxos算法有两部分：a) 算法的原理/证明；b) 算法的理解/运作。</p>

<p>理解这个算法的运作过程其实基本就可以用于工程实践。而且理解这个过程相对来说也容易得多。</p>

<p>网上我觉得讲Paxos讲的好的属于这篇：<a href="http://coderxy.com/archives/121">paxos图解</a>及<a href="http://coderxy.com/archives/136">Paxos算法详解</a>，我这里就结合<a href="http://zh.wikipedia.org/zh-cn/Paxos%E7%AE%97%E6%B3%95#.E5.AE.9E.E4.BE.8B">wiki上的实例</a>进一步阐述。一些paxos基础通过这里提到的两篇文章，以及wiki上的内容基本可以理解。</p>

<!-- more -->


<h2>算法内容</h2>

<p>Paxos在原作者的《Paxos Made Simple》中内容是比较精简的：</p>

<blockquote><p>Phase  1</p>

<p>  (a) A proposer selects a proposal number n  and sends a prepare request with number  n to a majority of acceptors.</p>

<p>  (b)  If  an  acceptor  receives  a prepare  request  with  number  n  greater than  that  of  any  prepare  request  to  which  it  has  already  responded, then it responds to the request with a promise not to accept any more proposals numbered less than  n  and with the highest-numbered pro-posal (if any) that it has accepted.</p>

<p>  Phase  2</p>

<p>  (a)  If  the  proposer  receives  a  response  to  its  prepare requests (numbered  n)  from  a  majority  of  acceptors,  then  it  sends  an  accept request to each of those acceptors for a proposal numbered  n  with a value v , where v is the value of the highest-numbered proposal among the responses, or is any value if the responses reported no proposals.</p>

<p>  (b) If an acceptor receives an accept request for a proposal numbered n, it accepts the proposal unless it has already responded to a prepare request having a number greater than  n.</p></blockquote>

<p>借用<a href="http://coderxy.com/archives/121">paxos图解</a>文中的流程图可概括为：</p>

<p><img src="/assets/res/paxos/paxos-flow.png" alt="" /></p>

<h2>实例及详解</h2>

<p>Paxos中有三类角色<code>Proposer</code>、<code>Acceptor</code>及<code>Learner</code>，主要交互过程在<code>Proposer</code>和<code>Acceptor</code>之间。</p>

<p><code>Proposer</code>与<code>Acceptor</code>之间的交互主要有4类消息通信，如下图：</p>

<p><img src="/assets/res/paxos/paxos-messages.png" alt="" /></p>

<p>这4类消息对应于paxos算法的两个阶段4个过程：</p>

<ul>
<li>phase 1

<ul>
<li>a) proposer向网络内超过半数的acceptor发送prepare消息</li>
<li>b) acceptor正常情况下回复promise消息</li>
</ul>
</li>
<li>phase 2

<ul>
<li>a) 在有足够多acceptor回复promise消息时，proposer发送accept消息</li>
<li>b) 正常情况下acceptor回复accepted消息</li>
</ul>
</li>
</ul>


<p>因为在整个过程中可能有其他proposer针对同一件事情发出以上请求，所以在每个过程中都会有些特殊情况处理，这也是为了达成一致性所做的事情。如果在整个过程中没有其他proposer来竞争，那么这个操作的结果就是确定无异议的。但是如果有其他proposer的话，情况就不一样了。</p>

<p>以<a href="http://zh.wikipedia.org/zh-cn/Paxos%E7%AE%97%E6%B3%95#.E5.AE.9E.E4.BE.8B">paxos中文wiki上的例子</a>为例。简单来说该例子以若干个议员提议税收，确定最终通过的法案税收比例。</p>

<p>以下图中基本只画出proposer与一个acceptor的交互。时间标志T2总是在T1后面。propose number简称N。</p>

<p>情况之一如下图：</p>

<p><img src="/assets/res/paxos/paxos-e1.png" alt="" /></p>

<p>A3在T1发出accepted给A1，然后在T2收到A5的prepare，在T3的时候A1才通知A5最终结果(税率10%)。这里会有两种情况：</p>

<ul>
<li>A5发来的N5小于A1发出去的N1，那么A3直接拒绝(reject)A5</li>
<li>A5发来的N5大于A1发出去的N1，那么A3回复promise，但带上A1的(N1, 10%)</li>
</ul>


<p>这里可以与paxos流程图对应起来，更好理解。<strong>acceptor会记录(MaxN, AcceptN, AcceptV)</strong>。</p>

<p>A5在收到promise后，后续的流程可以顺利进行。但是发出accept时，因为收到了(AcceptN, AcceptV)，所以会取最大的AcceptN对应的AcceptV，例子中也就是A1的10%作为AcceptV。如果在收到promise时没有发现有其他已记录的AcceptV，则其值可以由自己决定。</p>

<p>针对以上A1和A5冲突的情况，最终A1和A5都会广播接受的值为10%。</p>

<p>其实4个过程中对于acceptor而言，在回复promise和accepted时由于都可能因为其他proposer的介入而导致特殊处理。所以基本上看在这两个时间点收到其他proposer的请求时就可以了解整个算法了。例如在回复promise时则可能因为proposer发来的N不够大而reject：</p>

<p><img src="/assets/res/paxos/paxos-e2.png" alt="" /></p>

<p>如果在发accepted消息时，对其他更大N的proposer发出过promise，那么也会reject该proposer发出的accept，如图：</p>

<p><img src="/assets/res/paxos/paxos-e3.png" alt="" /></p>

<p>这个对应于Phase 2 b)：</p>

<blockquote><p>it accepts the proposal unless it has already responded to a prepare request having a number greater than  n.</p></blockquote>

<h2>总结</h2>

<p>Leslie Lamport没有用数学描述Paxos，但是他用英文阐述得很清晰。将Paxos的两个Phase的内容理解清楚，整个算法过程还是不复杂的。</p>

<p>至于Paxos中一直提到的一个全局唯一且递增的proposer number，其如何实现，引用如下：</p>

<blockquote><p>如何产生唯一的编号呢？在《Paxos made simple》中提到的是让所有的Proposer都从不相交的数据集合中进行选择，例如系统有5个Proposer，则可为每一个Proposer分配一个标识j(0~4)，则每一个proposer每次提出决议的编号可以为5*i + j(i可以用来表示提出议案的次数)</p></blockquote>

<h2>参考文档</h2>

<ul>
<li>paxos图解, <a href="http://coderxy.com/archives/121">http://coderxy.com/archives/121</a></li>
<li>Paxos算法详解, <a href="http://coderxy.com/archives/136">http://coderxy.com/archives/136</a></li>
<li>Paxos算法 wiki, <a href="http://zh.wikipedia.org/zh-cn/Paxos%E7%AE%97%E6%B3%95#.E5.AE.9E.E4.BE.8B">http://zh.wikipedia.org/zh-cn/Paxos%E7%AE%97%E6%B3%95#.E5.AE.9E.E4.BE.8B</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[淘宝分布式配置管理服务Diamond]]></title>
    <link href="http://codemacro.com/2014/10/12/diamond/"/>
    <updated>2014-10-12T00:00:00+08:00</updated>
    <id>http://codemacro.com/2014/10/12/diamond</id>
    <content type="html"><![CDATA[<p>在一个分布式环境中，同类型的服务往往会部署很多实例。这些实例使用了一些配置，为了更好地维护这些配置就产生了配置管理服务。通过这个服务可以轻松地管理这些应用服务的配置问题。应用场景可概括为：</p>

<p><img src="/assets/res/diamond/disconf.PNG" alt="" /></p>

<p>zookeeper的一种应用就是分布式配置管理(<a href="http://wenku.baidu.com/view/ee86ca90daef5ef7ba0d3c7d.html">基于ZooKeeper的配置信息存储方案的设计与实现</a>)。百度也有类似的实现：<a href="https://github.com/knightliao/disconf">disconf</a>。</p>

<p><a href="http://code.taobao.org/p/diamond/src/">Diamond</a>则是淘宝开源的一种分布式配置管理服务的实现。Diamond本质上是一个Java写的Web应用，其对外提供接口都是基于HTTP协议的，在阅读代码时可以从实现各个接口的controller入手。</p>

<h2>分布式配置管理</h2>

<p>分布式配置管理的本质基本上就是一种<strong><a href="http://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern">推送-订阅</a></strong>模式的运用。配置的应用方是订阅者，配置管理服务则是推送方。概括为下图：</p>

<p><img src="/assets/res/diamond/pubsub.PNG" alt="" /></p>

<p>其中，客户端包括管理人员publish数据到配置管理服务，可以理解为添加/更新数据；配置管理服务notify数据到订阅者，可以理解为推送。</p>

<!-- more -->


<p>配置管理服务往往会封装一个客户端库，应用方则是基于该库与配置管理服务进行交互。在实际实现时，客户端库可能是主动拉取(pull)数据，但对于应用方而言，一般是一种事件通知方式。</p>

<p>Diamond中的数据是简单的key-value结构。应用方订阅数据则是基于key来订阅，未订阅的数据当然不会被推送。数据从类型上又划分为聚合和非聚合。因为数据推送者可能很多，在整个分布式环境中，可能有多个推送者在推送相同key的数据，这些数据如果是聚合的，那么所有这些推送者推送的数据会被合并在一起；反之如果是非聚合的，则会出现覆盖。</p>

<p>数据的来源可能是人工通过管理端录入，也可能是其他服务通过配置管理服务的推送接口自动录入。</p>

<h2>架构及实现</h2>

<p>Diamond服务是一个集群，是一个去除了单点的协作集群。如图：</p>

<p><img src="/assets/res/diamond/arch.PNG" alt="" /></p>

<p>图中可分为以下部分讲解：</p>

<h3>服务之间同步</h3>

<p>Diamond服务集群每一个实例都可以对外完整地提供服务，那么意味着每个实例上都有整个集群维护的数据。Diamond有两种方式保证这一点：</p>

<ul>
<li>任何一个实例都有其他实例的地址；任何一个实例上的数据变更时，都会将改变的数据同步到mysql上，然后通知其他所有实例从mysql上进行一次数据拉取(<code>DumpService::dump</code>)，这个过程只拉取改变了的数据</li>
<li>任何一个实例启动后都会以较长的时间间隔（几小时），从mysql进行一次全量的数据拉取(<code>DumpAllProcessor</code>)</li>
</ul>


<p>实现上为了一致性，通知其他实例实际上也包含自己。以服务器收到添加聚合数据为例，处理过程大致为：</p>

<pre><code>DatumController::addDatum // /datum.do?method=addDatum
    PersistService::addAggrConfigInfo 
    MergeDatumService::addMergeTask // 添加一个MergeDataTask，异步处理

MergeTaskProcessor::process
    PersistService::insertOrUpdate
        EventDispatcher.fireEvent(new ConfigDataChangeEvent // 派发一个ConfigDataChangeEvent事件

NotifyService::onEvent // 接收事件并处理
    TaskManager::addTask(..., new NotifyTask // 由此，当数据发生变动，则最终创建了一个NoticyTask

// NotifyTask同样异步处理
NotifyTaskProcessor::process
    foreach server in serverList // 包含自己
        notifyToDump // 调用 /notify.do?method=notifyConfigInfo 从mysql更新变动的数据
</code></pre>

<p>虽然Diamond去除了单点问题，不过问题都下降到了mysql上。但由于其作为配置管理的定位，其数据量就mysql的应用而言算小的了，所以可以一定程度上保证整个服务的可用性。</p>

<h3>数据一致性</h3>

<p>由于Diamond服务器没有master，任何一个实例都可以读写数据，那么针对同一个key的数据则可能面临冲突。这里应该是通过mysql来保证数据的一致性。每一次客户端请求写数据时，Diamond都将写请求投递给mysql，然后通知集群内所有Diamond实例（包括自己）从mysql拉取数据。当然，拉取数据则可能不是每一次写入都能拉出来，也就是最终一致性。</p>

<p>Diamond中没有把数据放入内存，但会放到本地文件。对于客户端的读操作而言，则是直接返回本地文件里的数据。</p>

<h3>服务实例列表</h3>

<p>Diamond服务实例列表是一份静态数据，直接将每个实例的地址存放在一个web server上。无论是Diamond服务还是客户端都从该web server上取出实例列表。</p>

<p>对于客户端而言，当其取出了该列表后，则是随机选择一个节点(<code>ServerListManager.java</code>)，以后的请求都会发往该节点。</p>

<h3>数据同步</h3>

<p>客户端库中以固定时间间隔从服务器拉取数据(<code>ClientWorker::ClientWorker</code>，<code>ClientWorker::checkServerConfigInfo</code>)。只有应用方关心的数据才可能被拉取。另外，为了数据推送的及时，Diamond还使用了一种long polling的技术，其实也是为了突破HTTP协议的局限性。<em>如果整个服务是基于TCP的自定义协议，客户端与服务器保持长连接则没有这些问题</em>。</p>

<h3>数据的变更</h3>

<p>Diamond中很多操作都会检查数据是否发生了变化。标识数据变化则是基于数据对应的MD5值来实现的。</p>

<h2>容灾</h2>

<p>在整个Diamond系统中，几个角色为了提高容灾性，都有自己的缓存，概括为下图：</p>

<p><img src="/assets/res/diamond/failover.PNG" alt="" /></p>

<p>每一个角色出问题时，都可以尽量保证客户端对应用层提供服务。</p>

<h2>参考文档</h2>

<ul>
<li><a href="http://code.taobao.org/p/diamond/src">diamond project</a></li>
<li><a href="http://jm-blog.aliapp.com/?p=1588">diamond专题</a></li>
<li><a href="http://jm-blog.aliapp.com/?p=3450">中间件技术及双十一实践·软负载篇</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[zookeeper节点数与watch的性能测试]]></title>
    <link href="http://codemacro.com/2014/09/21/zk-watch-benchmark/"/>
    <updated>2014-09-21T00:00:00+08:00</updated>
    <id>http://codemacro.com/2014/09/21/zk-watch-benchmark</id>
    <content type="html"><![CDATA[<p>zookeeper中节点数量理论上仅受限于内存，但一个节点下的子节点数量<a href="http://zookeeper-user.578899.n2.nabble.com/ZooKeeper-Limitation-td6675643.html">受限于request/response 1M数据</a> (<a href="http://web.archiveorange.com/archive/v/AQXskdBodZB7kWpjpjHw">size of data / number of znodes</a>)</p>

<p>zookeeper的watch机制用于数据变更时zookeeper的主动通知。watch可以被附加到每一个节点上，那么如果一个应用有10W个节点，那zookeeper中就可能有10W个watch（甚至更多）。每一次在zookeeper完成改写节点的操作时就会检测是否有对应的watch，有的话则会通知到watch。<a href="http://shift-alt-ctrl.iteye.com/blog/1847320">Zookeeper-Watcher机制与异步调用原理</a></p>

<p>本文将关注以下内容：</p>

<ul>
<li>zookeeper的性能是否会受节点数量的影响</li>
<li>zookeeper的性能是否会受watch数量的影响</li>
</ul>


<h2>测试方法</h2>

<p>在3台机器上分别部署一个zookeeper，版本为<code>3.4.3</code>，机器配置：</p>

<pre><code>Intel(R) Xeon(R) CPU E5-2430 0 @ 2.20GHz

16G

java version "1.6.0_32"
Java(TM) SE Runtime Environment (build 1.6.0_32-b05)
OpenJDK (Taobao) 64-Bit Server VM (build 20.0-b12-internal, mixed mode)
</code></pre>

<p>大部分实验JVM堆大小使用默认，也就是<code>1/4 RAM</code>：</p>

<pre><code>java -XX:+PrintFlagsFinal -version | grep HeapSize
</code></pre>

<p>测试客户端使用<a href="https://github.com/phunt/zk-smoketest">zk-smoketest</a>，针对watch的测试则是我自己写的。基于zk-smoketest我写了些脚本可以自动跑数据并提取结果，相关脚本可以在这里找到：<a href="https://github.com/kevinlynx/zk-benchmark">https://github.com/kevinlynx/zk-benchmark</a></p>

<!-- more -->


<h2>测试结果</h2>

<h3>节点数对读写性能的影响</h3>

<p>测试最大10W个节点，度量1秒内操作数(ops)：</p>

<p><img src="/assets/res/zk_benchmark/node_count.png" alt="" /></p>

<p>可见节点数的增加并不会对zookeeper读写性能造成影响。</p>

<h3>节点数据大小对读写性能的影响</h3>

<p>这个网上其实已经有公认的结论。本身单个节点数据越大，对网络方面的吞吐就会造成影响，所以其数据越大读写性能越低也在预料之中。</p>

<p><img src="/assets/res/zk_benchmark/node_size.png" alt="" /></p>

<p>写数据会在zookeeper集群内进行同步，所以其速度整体会比读数据更慢。该实验需要把超时时间进行一定上调，同时我也把JVM最大堆大小调整到8G。</p>

<p>测试结果很明显，节点数据大小会严重影响zookeeper效率。</p>

<h2>watch对读写性能的影响</h2>

<p>zk-smoketest自带的latency测试有个参数<code>--watch_multiple</code>用来指定watch的数量，但其实仅是指定客户端的数量，在server端通过<code>echo whcp | nc 127.0.0.1 4181</code>会发现实际每个节点还是只有一个watch。</p>

<p>在我写的测试中，则是通过创建多个客户端来模拟单个节点上的多个watch。这也更符合实际应用。同时对节点的写也是在另一个独立的客户端中，这样可以避免zookeeper client的实现对测试带来的干扰。</p>

<p>每一次完整的测试，首先是对每个节点添加节点数据的watch，然后在另一个客户端中对这些节点进行数据改写，收集这些改写操作的耗时，以确定添加的watch对这些写操作带来了多大的影响。</p>

<p><img src="/assets/res/zk_benchmark/watch.png" alt="" /></p>

<p>图中，<code>0 watch</code>表示没有对节点添加watch；<code>1 watch</code>表示有一个客户端对每个节点进行了watch；<code>3 watch</code>表示有其他3个客户端对每个节点进行了watch；依次类推。</p>

<p>可见，watch对写操作还是有较大影响的，毕竟需要进行网络传输。同样，这里也显示出整个zookeeper的watch数量同节点数量一样对整体性能没有影响。</p>

<h2>总体结论</h2>

<ul>
<li>对单个节点的操作并不会因为zookeeper中节点的总数而受到影响</li>
<li>数据大小对zookeeper的性能有较大影响，性能和内存都会</li>
<li>单个节点上独立session的watch数对性能有一定影响</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[分布式环境中的负载均衡策略]]></title>
    <link href="http://codemacro.com/2014/08/25/lb-policy/"/>
    <updated>2014-08-25T00:00:00+08:00</updated>
    <id>http://codemacro.com/2014/08/25/lb-policy</id>
    <content type="html"><![CDATA[<p>在分布式系统中相同的服务常常会部署很多台，每一台被称为一个服务节点（实例）。通过一些负载均衡策略将服务请求均匀地分布到各个节点，以实现整个系统支撑海量请求的需求。本文描述一些简单的负载均衡策略。</p>

<h2>Round-robin</h2>

<p>简单地轮询。记录一个选择位置，每次请求来时调整该位置到下一个节点：</p>

<pre><code>curId = ++curId % nodeCnt
</code></pre>

<h2>随机选择</h2>

<p>随机地在所有节点中选择：</p>

<pre><code>id = random(nodeCnt);
</code></pre>

<h2>本机优先</h2>

<p>访问后台服务的访问者可能本身是一个整合服务，或者是一个proxy，如果后台服务节点恰好有节点部署在本机的，则可以优先使用。在未找到本机节点时则可以继续走Round-robin策略：</p>

<pre><code>if (node-&gt;ip() == local_ip) {
    return node;
} else {
    return roundRobin();
}
</code></pre>

<!-- more -->


<p>一旦遍历到本机节点，则后面的请求会一直落到本机节点。所以这里可以加上一些权重机制，仅是保证本机节点会被优先选择，但不会被一直选择。例如：</p>

<pre><code>// initial
cur_weight = 100;
...
// select node
cur_weight -= 5;
if (cur_weight &lt;= 0)
    cur_weight = 100;
if (cur_weight &gt; 50 &amp;&amp; node-&gt;ip() == local_ip) {
    return node;
} else {
    return roundRobin();
}
</code></pre>

<h2>本机房优先</h2>

<p>服务节点可能会被部署到多个机房，有时候确实是需要考虑跨机房服务。同<code>本机优先</code>策略类似，本机房优先则是优先考虑位于相同机房内的服务节点。该请求是从哪个机房中的前端服务发送过来的，则需要前端在请求参数中携带上机房ID。</p>

<p>在服务节点对应的数据结构中，也最好按照机房来组织。</p>

<p>本机房优先策略实际上会作为节点选择的第一道工序，它可以把非本机房的节点先过滤掉，然后再传入后面的各种节点选择策略。这里还可以考虑节点数参数，如果本机房的节点过少，则可以不使用该策略，避免流量严重不均。</p>

<h2>Weighted Round-Robin</h2>

<p>加权轮询。相对于普通轮询而言，该策略中每一个节点都有自己的权重，优先选择权重更大的节点。权重可以根据机器性能预先配置。摘抄一下网上的算法：</p>

<pre><code>假设有一组服务器S = {S0, S1, …, Sn-1}，W(Si)表示服务器Si的权值，一个
指示变量i表示上一次选择的服务器，指示变量cw表示当前调度的权值，max(S)
表示集合S中所有服务器的最大权值，gcd(S)表示集合S中所有服务器权值的最大
公约数。变量i初始化为-1，cw初始化为零。

while (true) {
  i = (i + 1) mod n;
  if (i == 0) {
     cw = cw - gcd(S); 
     if (cw &lt;= 0) {
       cw = max(S);
       if (cw == 0)
         return NULL;
     }
  } 
  if (W(Si) &gt;= cw) 
    return Si;
}
</code></pre>

<p>遍历完所有节点后权重衰减，衰减到0后重新开始。这样可以让权重更大的节点被选择得更多。</p>

<h2>Consistent Hash</h2>

<p>一致性哈希。一致性哈希用于在分布式环境中，分布在各个节点上的请求，不会因为新增节点（扩容）或减少节点（节点宕机）而变化。如果每个服务节点上都有自己的缓存，其保存了该节点响应请求时的回应。正常情况下，这些缓存都可以很好地被运用，也即cache命中率较高。</p>

<p>如果某个节点不可用了，我们的选择策略又是基于所有节点的公平选择，那么原来一直分配在节点A上请求就很可能被分配到节点B上，从而导致节点A上的缓存较难被命中。这个时候就可以运用一致性哈希来解决。</p>

<p>其基本思想是，在节点选择区间内，在找节点时以顺时针方向找到不小于该请求对应的哈希值的节点。在这个区间里增加很多虚拟节点，每一个虚拟节点相当于一个物理节点的引用，这样相当于把物理节点变成了一个哈希值区间。这个哈希值区间不会因为增加节点和减少节点而变化，那么对某个请求而言，它就会始终落到这个区间里，也就会始终被分配到原来的节点。</p>

<p>至于这个不可用的节点，其上的请求也会被均匀地分配到其他节点中。</p>

<p>摘抄网上的一段代码：</p>

<pre><code>// 添加一个物理节点时，会随之增加很多虚拟节点
template &lt;class Node, class Data, class Hash&gt;
size_t HashRing&lt;Node, Data, Hash&gt;::AddNode(const Node&amp; node)
{
    size_t hash;
    std::string nodestr = Stringify(node);
    for (unsigned int r = 0; r &lt; replicas_; r++) {
        hash = hash_((nodestr + Stringify(r)).c_str());
        ring_[hash] = node;  // 物理节点和虚拟节点都保存在一个std::map中
    }
    return hash;
}

// 选择data对应的节点，data可以是请求
template &lt;class Node, class Data, class Hash&gt;
const Node&amp; HashRing&lt;Node, Data, Hash&gt;::GetNode(const Data&amp; data) const
{
    if (ring_.empty()) {
        throw EmptyRingException();
    }
    size_t hash = hash_(Stringify(data).c_str()); // 对请求进行哈希
    typename NodeMap::const_iterator it;
    // Look for the first node &gt;= hash
    it = ring_.lower_bound(hash); // 找到第一个不小于请求哈希的节点
    if (it == ring_.end()) {
        // Wrapped around; get the first node
        it = ring_.begin();
    }
    return it-&gt;second;
}
</code></pre>

<p>参考<a href="http://blog.csdn.net/sparkliang/article/details/5279393">一致性 hash 算法(consistent hashing)</a>，<a href="http://www.martinbroadhurst.com/Consistent-Hash-Ring.html">Consistent Hash Ring</a></p>
]]></content>
  </entry>
  
</feed>
