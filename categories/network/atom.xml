<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: network | loop in codes]]></title>
  <link href="http://codemacro.com/categories/network/atom.xml" rel="self"/>
  <link href="http://codemacro.com/"/>
  <updated>2017-04-23T21:07:29+08:00</updated>
  <id>http://codemacro.com/</id>
  <author>
    <name><![CDATA[Kevin Lynx]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[XNIO源码阅读]]></title>
    <link href="http://codemacro.com/2017/04/09/xnio-source/"/>
    <updated>2017-04-09T00:00:00+08:00</updated>
    <id>http://codemacro.com/2017/04/09/xnio-source</id>
    <content type="html"><![CDATA[<p><a href="http://xnio.jboss.org/">XNIO</a>是JBoss的一个IO框架。最开始我想找个lightweight servlet container库，于是看到了<a href="http://undertow.io/">undertow</a>，发现其网络部分使用的就是XNIO。所以干脆就先把XNIO的源码读下。</p>

<p>XNIO文档非常匮乏，能找到都是<a href="https://docs.jboss.org/author/display/XNIO/About+XNIO">3.0的版本</a>，而且描述也不完全。Git上已经出到3.5.0。我读的是3.3.6.Final。</p>

<h2>使用方式</h2>

<p>可以参考<a href="https://github.com/ecki/xnio-samples/blob/master/src/main/java/org/xnio/samples/SimpleEchoServer.java">SimpleEchoServer.java</a>，不过这个例子使用的API已经被deprecated，仅供参考。使用方式大致为：</p>

<ul>
<li>创建服务，提供acceptListener</li>
<li>在acceptListener中accept新的连接，并注册连接listener</li>
<li>在连接listener回调中完成IO读写</li>
</ul>


<!-- more -->


<h2>主要概念</h2>

<ul>
<li>Channel，基本上同Java NIO中的Channel一致，一个server socket是一个channel，accept出来的连接也是channel</li>
<li>ChannelListener，监听Channel上的IO事件，应用代码与XNIO交互的地方</li>
<li>XnioWorker，维护IO线程池及应用任务线程池</li>
</ul>


<h2>项目结构</h2>

<p>源码分为两个项目: xnio-api及nio-impl。xnio-api属于API层；nio-impl是基于NIO的实现。通过Java service provider动态地找到nio-impl这个实现。可见XNIO还可以用其他方式来实现。</p>

<p><code>org.xnio.channels</code>这个包里包含了大量的Channel接口定义，这个是非常恶心的一个地方，读代码的时候很容易被绕进去。这个包主要的实现后面提。<code>org.xnio.conduits</code>，我理解为比Channel更底层的传输通道，channel依赖于conduit实现，总之也是个恶心的概念。</p>

<h2>线程模型</h2>

<p>可以通过连接如何建立以及建立连接后如何管理连接来了解XNIO的线程模型。通过这个过程我简单画了下主要类关系以及连接建立过程：</p>

<p><img src="http://i.imgur.com/HoL99Wz.png" alt="" /></p>

<p><em>用的Dia绘图，UML图支持得不够好</em></p>

<p>XNIO的线程模型是一个典型的one loop per thread的Reactor模型。<code>WorkerThread</code>类就是这个线程，其有一个主循环，不断地检测其关心的IO设备是否有IO事件发生。当有事件发生时，就将事件通知给关心的listener。站在上层模块的角度，这个线程就是一个Reactor，事件产生器。整个系统有固定数量的<code>WorkerThread</code>，也就是IO线程数。这个模型基本上凡是基于epoll/select模型实现的网络库都会用，例如<a href="http://codemacro.com/2014/05/04/muduo-source/">muduo</a>。可以回看下这个模型：</p>

<p><img src="http://codemacro.com/assets/res/muduo-model.png" alt="" /></p>

<p>XNIO中接收到一个新连接时，会根据这个连接的地址(remote&amp;local address)算出一个哈希值，然后根据哈希值分配到某一个IO线程，然后该连接以后的IO事件都由该线程处理。<code>WorkerThread</code>会始终回调<code>NioHandle</code>。<code>QueuedNioTcpServerHandle</code>是一个accept socket，监听accept事件。而<code>NioSocketStreamConnection</code>则是一个建立好的连接，每次新连接进来就会创建，被哈希到某个<code>WorkerThread</code>处理。<code>NioSocketConduit</code>是一个连接具体关心IO事件的类，正是前面提到的，是一个Channel的底层实现。</p>

<p><code>NioXnioWorker</code>继承于<code>XnioWorker</code>，<code>XnioWorker</code>内部包含了一个应用任务的线程池。应用代码通过channel listener获取到IO事件通知，channel listener是在IO线程中回调的，所以不适合做耗时操作，否则会导致IO线程中其他IO设备饿死。所以对于这类任务就可以放到这个线程池中做。</p>

<h2>Channel架构</h2>

<p>前面提到的XNIO例子使用了一个deprecated的接口，那如何不使用这个接口呢？这就需要更具体地了解channel。XNIO中抽象的channel有很多类型，有些是只读的，有些是只写的，有些则是全双工的。channel还能被组合 (<code>AssembledChannel</code>)。可以看下3.1里channel包的大图：<a href="http://docs.jboss.org/xnio/3.1/api/org/xnio/channels/package-summary.png">channel package summary</a></p>

<p>这里我只关注基于TCP服务中的channel。如图：</p>

<p><img src="http://i.imgur.com/BjmU3BJ.png" alt="" /></p>

<p>重点关注 <code>QueuedNioTcpServer</code> 及 <code>NioSocketStreamConnection</code>。<code>QueuedNioTcpServer</code>实现<code>AcceptingChannel</code> 没什么好说的，就是表示一个可以接收连接的channel。<code>NioSocketStreamConnection</code>表示一个网络连接。<code>StreamConnection</code>是一个可读可写的channel，但是其内部是通过另外两个channel来实现的，分别是<code>ConduitStreamSourceChannel</code>及<code>ConduitStreamSinkChannel</code>，分别用读和写。这两个channel内部其实是分别通过两个conduit 来实现，分别为<code>ConduitStreamSourceChannel</code> 及 <code>ConduitStreamSinkChannel</code> 。</p>

<p><code>NioSocketStreamConnection</code> 内部包含<code>NioSocketConduit</code>，这个类实现了 <code>ConduitStreamSourceChannel</code> 及 <code>ConduitStreamSinkChannel</code> 。在TCP场景下，<code>StreamConnection</code>中的读写channel正是指向了<code>NioSocketConduit</code>。这个层次包装得有点绕，需要慢慢梳理。</p>

<p>在accept的时候，得到的可以是<code>StreamConnection</code>，其实也就是得到了一个可读可写的channel，设计得也没问题。可以基于这个channel设置读写listener。但是如果想在读listener里发起写操作，由于在读listener里看到的是一个只读的channel，所以就没办法写。所以才会有其他包装的channel。</p>

<p>理清了以上关系，就可以不用那个deprecated的API来实现一个echo server：</p>

<p><div class="highlight"><pre><code class="language-java" data-lang="java"><span class="kd">class</span> <span class="nc">ReadListener</span> <span class="kd">implements</span> <span class="n">ChannelListener</span><span class="o">&lt;</span><span class="n">StreamSourceChannel</span><span class="o">&gt;</span> <span class="o">{</span>
  <span class="c1">// 保存一个可写的channel，才能在读listener里做写操作</span>
  <span class="kd">private</span> <span class="n">StreamSinkChannel</span> <span class="n">sinkChannel</span><span class="o">;&lt;/</span><span class="n">p</span><span class="o">&gt;</span>

<span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>  <span class="kd">public</span> <span class="nf">ReadListener</span><span class="o">(</span><span class="n">StreamSinkChannel</span> <span class="n">sinkChannel</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">this</span><span class="o">.</span><span class="na">sinkChannel</span> <span class="o">=</span> <span class="n">sinkChannel</span><span class="o">;</span>
  <span class="o">}&lt;/</span><span class="n">p</span><span class="o">&gt;</span>

<span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span>  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">handleEvent</span><span class="o">(</span><span class="n">StreamSourceChannel</span> <span class="n">channel</span><span class="o">)</span> <span class="o">{</span>
    <span class="kd">final</span> <span class="n">ByteBuffer</span> <span class="n">buffer</span> <span class="o">=</span> <span class="n">ByteBuffer</span><span class="o">.</span><span class="na">allocate</span><span class="o">(</span><span class="mi">512</span><span class="o">);</span>
    <span class="kt">int</span> <span class="n">res</span><span class="o">;</span>
    <span class="k">try</span> <span class="o">{</span>
      <span class="k">while</span> <span class="o">((</span><span class="n">res</span> <span class="o">=</span> <span class="n">channel</span><span class="o">.</span><span class="na">read</span><span class="o">(</span><span class="n">buffer</span><span class="o">))</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">buffer</span><span class="o">.</span><span class="na">flip</span><span class="o">();</span>
        <span class="n">Channels</span><span class="o">.</span><span class="na">writeBlocking</span><span class="o">(</span><span class="n">sinkChannel</span><span class="o">,</span> <span class="n">buffer</span><span class="o">);</span>
      <span class="o">}</span>
      <span class="n">Channels</span><span class="o">.</span><span class="na">flushBlocking</span><span class="o">(</span><span class="n">sinkChannel</span><span class="o">);</span>
      <span class="k">if</span> <span class="o">(</span><span class="n">res</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">channel</span><span class="o">.</span><span class="na">close</span><span class="o">();</span>
      <span class="o">}</span> <span class="k">else</span> <span class="o">{</span>
        <span class="n">channel</span><span class="o">.</span><span class="na">resumeReads</span><span class="o">();</span>
      <span class="o">}</span>
    <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="n">IOException</span> <span class="n">e</span><span class="o">)</span> <span class="o">{</span>
      <span class="n">e</span><span class="o">.</span><span class="na">printStackTrace</span><span class="o">();</span>
      <span class="n">IoUtils</span><span class="o">.</span><span class="na">safeClose</span><span class="o">(</span><span class="n">channel</span><span class="o">);</span>
    <span class="o">}</span>
  <span class="o">}</span>
<span class="o">}</span>
<span class="kd">final</span> <span class="n">ChannelListener</span><span class="o">&amp;</span><span class="n">lt</span><span class="o">;</span><span class="n">AcceptingChannel</span><span class="o">&lt;</span><span class="n">StreamConnection</span><span class="o">&gt;&gt;</span> <span class="n">acceptListener</span> <span class="o">=</span> <span class="k">new</span> <span class="n">ChannelListener</span><span class="o">&amp;</span><span class="n">lt</span><span class="o">;</span><span class="n">AcceptingChannel</span><span class="o">&lt;</span><span class="n">StreamConnection</span><span class="o">&gt;&gt;()</span> <span class="o">{</span>
  <span class="kd">public</span> <span class="kt">void</span> <span class="nf">handleEvent</span><span class="o">(</span><span class="n">AcceptingChannel</span><span class="o">&lt;</span><span class="n">StreamConnection</span><span class="o">&gt;</span> <span class="n">channel</span><span class="o">)</span> <span class="o">{</span>
    <span class="k">try</span> <span class="o">{</span>
      <span class="n">StreamConnection</span> <span class="n">accepted</span><span class="o">;</span>
      <span class="c1">// channel is ready to accept zero or more connections</span>
      <span class="k">while</span> <span class="o">((</span><span class="n">accepted</span> <span class="o">=</span> <span class="n">channel</span><span class="o">.</span><span class="na">accept</span><span class="o">())</span> <span class="o">!=</span> <span class="kc">null</span><span class="o">)</span> <span class="o">{</span>
        <span class="n">System</span><span class="o">.</span><span class="na">out</span><span class="o">.</span><span class="na">println</span><span class="o">(&amp;</span><span class="n">ldquo</span><span class="o">;</span><span class="n">accepted</span> <span class="o">&amp;</span><span class="n">rdquo</span><span class="o">;</span>
            <span class="o">+</span> <span class="n">accepted</span><span class="o">.</span><span class="na">getPeerAddress</span><span class="o">());</span>
        <span class="c1">// stream channel has been accepted at this stage.</span>
        <span class="c1">// read listener is set; start it up</span>
        <span class="n">accepted</span><span class="o">.</span><span class="na">getSourceChannel</span><span class="o">().</span><span class="na">setReadListener</span><span class="o">(</span><span class="k">new</span> <span class="nf">ReadListener</span><span class="o">(</span><span class="n">accepted</span><span class="o">.</span><span class="na">getSinkChannel</span><span class="o">()));</span>
        <span class="n">accepted</span><span class="o">.</span><span class="na">getSourceChannel</span><span class="o">().</span><span class="na">resumeReads</span><span class="o">();</span>
      <span class="o">}</span>
    <span class="o">}</span> <span class="k">catch</span> <span class="o">(</span><span class="n">IOException</span> <span class="n">ignored</span><span class="o">)</span> <span class="o">{</span>
    <span class="o">}</span>
  <span class="o">}</span>
<span class="o">};</span>
<span class="kd">final</span> <span class="n">XnioWorker</span> <span class="n">worker</span> <span class="o">=</span> <span class="n">Xnio</span><span class="o">.</span><span class="na">getInstance</span><span class="o">().</span><span class="na">createWorker</span><span class="o">(</span>
    <span class="n">OptionMap</span><span class="o">.</span><span class="na">EMPTY</span><span class="o">);</span>
<span class="c1">// Create the server.</span>
<span class="n">AcceptingChannel</span><span class="o">&amp;</span><span class="n">lt</span><span class="o">;?</span> <span class="kd">extends</span> <span class="n">StreamConnection</span><span class="o">&gt;</span> <span class="n">server</span> <span class="o">=</span> <span class="n">worker</span>
    <span class="o">.</span><span class="na">createStreamConnectionServer</span><span class="o">(</span><span class="k">new</span> <span class="nf">InetSocketAddress</span><span class="o">(</span><span class="mi">12345</span><span class="o">),</span>
        <span class="n">acceptListener</span><span class="o">,</span> <span class="n">OptionMap</span><span class="o">.</span><span class="na">EMPTY</span><span class="o">);</span>
<span class="c1">// lets start accepting connections</span>
<span class="n">server</span><span class="o">.</span><span class="na">resumeAccepts</span><span class="o">();</span></code></pre></div></p>

<p>完。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[图解zookeeper FastLeader选举算法]]></title>
    <link href="http://codemacro.com/2014/10/19/zk-fastleaderelection/"/>
    <updated>2014-10-19T00:00:00+08:00</updated>
    <id>http://codemacro.com/2014/10/19/zk-fastleaderelection</id>
    <content type="html"><![CDATA[<p>zookeeper配置为集群模式时，在启动或异常情况时会选举出一个实例作为Leader。其默认选举算法为<code>FastLeaderElection</code>。</p>

<p>不知道zookeeper的可以考虑这样一个问题：某个服务可以配置为多个实例共同构成一个集群对外提供服务。其每一个实例本地都存有冗余数据，每一个实例都可以直接对外提供读写服务。在这个集群中为了保证数据的一致性，需要有一个Leader来协调一些事务。那么问题来了：如何确定哪一个实例是Leader呢？</p>

<p>问题的难点在于：</p>

<ul>
<li>没有一个仲裁者来选定Leader</li>
<li>每一个实例本地可能已经存在数据，不确定哪个实例上的数据是最新的</li>
</ul>


<p>分布式选举算法正是用来解决这个问题的。</p>

<p>本文基于zookeeper 3.4.6 的源码进行分析。FastLeaderElection算法的源码全部位于<code>FastLeaderElection.java</code>文件中，其对外接口为<code>FastLeaderElection.lookForLeader</code>，该接口是一个同步接口，直到选举结束才会返回。同样由于网上已有类似文章，所以我就从图示的角度来阐述。阅读一些其他文章有利于获得初步印象：</p>

<ul>
<li><a href="http://iwinit.iteye.com/blog/1773531">深入浅出Zookeeper之五 Leader选举</a>，代码导读</li>
<li><a href="http://blog.csdn.net/xhh198781/article/details/6619203">zookeeper3.3.3源码分析(二)FastLeader选举算法</a>，文字描述较细</li>
</ul>


<h2>主要流程</h2>

<p>阅读代码和以上推荐文章可以把整个流程梳理清楚。实现上，包括了一个消息处理主循环，也是选举的主要逻辑，以及一个消息发送队列处理线程和消息解码线程。主要流程可概括为下图：</p>

<!-- more -->


<p><img src="/assets/res/zk/fle-flow.png" alt="fle-flow.png" /></p>

<p>推荐对照着推荐的文章及代码理解，不赘述。</p>

<p>我们从感性上来理解这个算法。</p>

<p>每一个节点，相当于一个选民，他们都有自己的推荐人，最开始他们都推荐自己。谁更适合成为Leader有一个简单的规则，例如sid够大（配置）、持有的数据够新(zxid够大)。每个选民都告诉其他选民自己目前的推荐人是谁，类似于出去搞宣传拉拢其他选民。每一个选民发现有比自己更适合的人时就转而推荐这个更适合的人。最后，大部分人意见一致时，就可以结束选举。</p>

<p>就这么简单。总体上有一种不断演化逼近结果的感觉。</p>

<p>当然，会有些特殊情况的处理。例如总共3个选民，1和2已经确定3是Leader，但3还不知情，此时就走入<code>LEADING/FOLLOWING</code>的分支，选民3只是接收结果。</p>

<p>代码中不是所有逻辑都在这个大流程中完成的。在接收消息线程中，还可能单独地回应某个节点(<code>WorkerReceiver.run</code>)：</p>

<p><img src="/assets/res/zk/recv.png" alt="recv.png" /></p>

<p>从这里可以看出，当某个节点已经确定选举结果不再处于<code>LOOKING</code>状态时，其收到<code>LOOKING</code>消息时都会直接回应选举的最终结果。结合上面那个比方，相当于某次选举结束了，这个时候来了选民4又发起一次新的选举，那么其他选民就直接告诉它当前的Leader情况。相当于，在这个集群主从已经就绪的情况下，又开启了一个实例，这个实例就会直接使用当前的选举结果。</p>

<h2>状态转换</h2>

<p>每个节点上有一些关键的数据结构：</p>

<ul>
<li>当前推荐人，初始推荐自己，每次收到其他更好的推荐人时就更新</li>
<li>其他人的投票集合，用于确定何时选举结束</li>
</ul>


<p>每次推荐人更新时就会进行广播，正是这个不断地广播驱动整个算法趋向于结果。假设有3个节点A/B/C，其都还没有数据，按照sid关系为C>B>A，那么按照规则，C更可能成为Leader，其各个节点的状态转换为：</p>

<p><img src="/assets/res/zk/state.png" alt="state.png" /></p>

<p>图中，v(A)表示当前推荐人为A；r[]表示收到的投票集合。需要注意一个细节，初始投票集合里包含了自己的投票，代码中自己会将推荐人推荐给自己，网络模块(<code>QuorumCnxManager</code>)直接将该消息放入接收队列。</p>

<p>可以看看当其他节点已经确定投票结果时，即不再是<code>LOOKING</code>时的状态：</p>

<p><img src="/assets/res/zk/state-ret.png" alt="state-ret.png" /></p>

<p>代码中有一个特殊的投票集合<code>outofelection</code>，我理解为选举已结束的那些投票，这些投票仅用于表征选举结果。</p>

<p>当一个新启动的节点加入集群时，它对集群内其他节点发出投票请求，而其他节点已不处于<code>LOOKING</code>状态，此时其他节点回应选举结果，该节点收集这些结果到<code>outofelection</code>中，最终在收到合法LEADER消息且这些选票也构成选举结束条件时，该节点就结束自己的选举行为。<em>注意到代码中会<code>logicalclock = n.electionEpoch;</code>更新选举轮数</em></p>

<p><em>完</em></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[图解分布式一致性协议Paxos]]></title>
    <link href="http://codemacro.com/2014/10/15/explain-poxos/"/>
    <updated>2014-10-15T00:00:00+08:00</updated>
    <id>http://codemacro.com/2014/10/15/explain-poxos</id>
    <content type="html"><![CDATA[<p>Paxos协议/算法是分布式系统中比较重要的协议，它有多重要呢？</p>

<p><a href="http://coolshell.cn/articles/10910.html">&lt;分布式系统的事务处理></a>：</p>

<blockquote><p>Google Chubby的作者Mike Burrows说过这个世界上只有一种一致性算法，那就是Paxos，其它的算法都是残次品。</p></blockquote>

<p><a href="http://book.douban.com/subject/25723658/">&lt;大规模分布式存储系统></a>：</p>

<blockquote><p>理解了这两个分布式协议之后(Paxos/2PC)，学习其他分布式协议会变得相当容易。</p></blockquote>

<p>学习Paxos算法有两部分：a) 算法的原理/证明；b) 算法的理解/运作。</p>

<p>理解这个算法的运作过程其实基本就可以用于工程实践。而且理解这个过程相对来说也容易得多。</p>

<p>网上我觉得讲Paxos讲的好的属于这篇：<a href="http://coderxy.com/archives/121">paxos图解</a>及<a href="http://coderxy.com/archives/136">Paxos算法详解</a>，我这里就结合<a href="http://zh.wikipedia.org/zh-cn/Paxos%E7%AE%97%E6%B3%95#.E5.AE.9E.E4.BE.8B">wiki上的实例</a>进一步阐述。一些paxos基础通过这里提到的两篇文章，以及wiki上的内容基本可以理解。</p>

<!-- more -->


<h2>算法内容</h2>

<p>Paxos在原作者的《Paxos Made Simple》中内容是比较精简的：</p>

<blockquote><p>Phase  1</p>

<p>  (a) A proposer selects a proposal number n  and sends a prepare request with number  n to a majority of acceptors.</p>

<p>  (b)  If  an  acceptor  receives  a prepare  request  with  number  n  greater than  that  of  any  prepare  request  to  which  it  has  already  responded, then it responds to the request with a promise not to accept any more proposals numbered less than  n  and with the highest-numbered pro-posal (if any) that it has accepted.</p>

<p>  Phase  2</p>

<p>  (a)  If  the  proposer  receives  a  response  to  its  prepare requests (numbered  n)  from  a  majority  of  acceptors,  then  it  sends  an  accept request to each of those acceptors for a proposal numbered  n  with a value v , where v is the value of the highest-numbered proposal among the responses, or is any value if the responses reported no proposals.</p>

<p>  (b) If an acceptor receives an accept request for a proposal numbered n, it accepts the proposal unless it has already responded to a prepare request having a number greater than  n.</p></blockquote>

<p>借用<a href="http://coderxy.com/archives/121">paxos图解</a>文中的流程图可概括为：</p>

<p><img src="/assets/res/paxos/paxos-flow.png" alt="" /></p>

<h2>实例及详解</h2>

<p>Paxos中有三类角色<code>Proposer</code>、<code>Acceptor</code>及<code>Learner</code>，主要交互过程在<code>Proposer</code>和<code>Acceptor</code>之间。</p>

<p><code>Proposer</code>与<code>Acceptor</code>之间的交互主要有4类消息通信，如下图：</p>

<p><img src="/assets/res/paxos/paxos-messages.png" alt="" /></p>

<p>这4类消息对应于paxos算法的两个阶段4个过程：</p>

<ul>
<li>phase 1

<ul>
<li>a) proposer向网络内超过半数的acceptor发送prepare消息</li>
<li>b) acceptor正常情况下回复promise消息</li>
</ul>
</li>
<li>phase 2

<ul>
<li>a) 在有足够多acceptor回复promise消息时，proposer发送accept消息</li>
<li>b) 正常情况下acceptor回复accepted消息</li>
</ul>
</li>
</ul>


<p>因为在整个过程中可能有其他proposer针对同一件事情发出以上请求，所以在每个过程中都会有些特殊情况处理，这也是为了达成一致性所做的事情。如果在整个过程中没有其他proposer来竞争，那么这个操作的结果就是确定无异议的。但是如果有其他proposer的话，情况就不一样了。</p>

<p>以<a href="http://zh.wikipedia.org/zh-cn/Paxos%E7%AE%97%E6%B3%95#.E5.AE.9E.E4.BE.8B">paxos中文wiki上的例子</a>为例。简单来说该例子以若干个议员提议税收，确定最终通过的法案税收比例。</p>

<p>以下图中基本只画出proposer与一个acceptor的交互。时间标志T2总是在T1后面。propose number简称N。</p>

<p>情况之一如下图：</p>

<p><img src="/assets/res/paxos/paxos-e1.png" alt="" /></p>

<p>A3在T1发出accepted给A1，然后在T2收到A5的prepare，在T3的时候A1才通知A5最终结果(税率10%)。这里会有两种情况：</p>

<ul>
<li>A5发来的N5小于A1发出去的N1，那么A3直接拒绝(reject)A5</li>
<li>A5发来的N5大于A1发出去的N1，那么A3回复promise，但带上A1的(N1, 10%)</li>
</ul>


<p>这里可以与paxos流程图对应起来，更好理解。<strong>acceptor会记录(MaxN, AcceptN, AcceptV)</strong>。</p>

<p>A5在收到promise后，后续的流程可以顺利进行。但是发出accept时，因为收到了(AcceptN, AcceptV)，所以会取最大的AcceptN对应的AcceptV，例子中也就是A1的10%作为AcceptV。如果在收到promise时没有发现有其他已记录的AcceptV，则其值可以由自己决定。</p>

<p>针对以上A1和A5冲突的情况，最终A1和A5都会广播接受的值为10%。</p>

<p>其实4个过程中对于acceptor而言，在回复promise和accepted时由于都可能因为其他proposer的介入而导致特殊处理。所以基本上看在这两个时间点收到其他proposer的请求时就可以了解整个算法了。例如在回复promise时则可能因为proposer发来的N不够大而reject：</p>

<p><img src="/assets/res/paxos/paxos-e2.png" alt="" /></p>

<p>如果在发accepted消息时，对其他更大N的proposer发出过promise，那么也会reject该proposer发出的accept，如图：</p>

<p><img src="/assets/res/paxos/paxos-e3.png" alt="" /></p>

<p>这个对应于Phase 2 b)：</p>

<blockquote><p>it accepts the proposal unless it has already responded to a prepare request having a number greater than  n.</p></blockquote>

<h2>总结</h2>

<p>Leslie Lamport没有用数学描述Paxos，但是他用英文阐述得很清晰。将Paxos的两个Phase的内容理解清楚，整个算法过程还是不复杂的。</p>

<p>至于Paxos中一直提到的一个全局唯一且递增的proposer number，其如何实现，引用如下：</p>

<blockquote><p>如何产生唯一的编号呢？在《Paxos made simple》中提到的是让所有的Proposer都从不相交的数据集合中进行选择，例如系统有5个Proposer，则可为每一个Proposer分配一个标识j(0~4)，则每一个proposer每次提出决议的编号可以为5*i + j(i可以用来表示提出议案的次数)</p></blockquote>

<h2>参考文档</h2>

<ul>
<li>paxos图解, <a href="http://coderxy.com/archives/121">http://coderxy.com/archives/121</a></li>
<li>Paxos算法详解, <a href="http://coderxy.com/archives/136">http://coderxy.com/archives/136</a></li>
<li>Paxos算法 wiki, <a href="http://zh.wikipedia.org/zh-cn/Paxos%E7%AE%97%E6%B3%95#.E5.AE.9E.E4.BE.8B">http://zh.wikipedia.org/zh-cn/Paxos%E7%AE%97%E6%B3%95#.E5.AE.9E.E4.BE.8B</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[淘宝分布式配置管理服务Diamond]]></title>
    <link href="http://codemacro.com/2014/10/12/diamond/"/>
    <updated>2014-10-12T00:00:00+08:00</updated>
    <id>http://codemacro.com/2014/10/12/diamond</id>
    <content type="html"><![CDATA[<p>在一个分布式环境中，同类型的服务往往会部署很多实例。这些实例使用了一些配置，为了更好地维护这些配置就产生了配置管理服务。通过这个服务可以轻松地管理这些应用服务的配置问题。应用场景可概括为：</p>

<p><img src="/assets/res/diamond/disconf.PNG" alt="" /></p>

<p>zookeeper的一种应用就是分布式配置管理(<a href="http://wenku.baidu.com/view/ee86ca90daef5ef7ba0d3c7d.html">基于ZooKeeper的配置信息存储方案的设计与实现</a>)。百度也有类似的实现：<a href="https://github.com/knightliao/disconf">disconf</a>。</p>

<p><a href="http://code.taobao.org/p/diamond/src/">Diamond</a>则是淘宝开源的一种分布式配置管理服务的实现。Diamond本质上是一个Java写的Web应用，其对外提供接口都是基于HTTP协议的，在阅读代码时可以从实现各个接口的controller入手。</p>

<h2>分布式配置管理</h2>

<p>分布式配置管理的本质基本上就是一种<strong><a href="http://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern">推送-订阅</a></strong>模式的运用。配置的应用方是订阅者，配置管理服务则是推送方。概括为下图：</p>

<p><img src="/assets/res/diamond/pubsub.PNG" alt="" /></p>

<p>其中，客户端包括管理人员publish数据到配置管理服务，可以理解为添加/更新数据；配置管理服务notify数据到订阅者，可以理解为推送。</p>

<!-- more -->


<p>配置管理服务往往会封装一个客户端库，应用方则是基于该库与配置管理服务进行交互。在实际实现时，客户端库可能是主动拉取(pull)数据，但对于应用方而言，一般是一种事件通知方式。</p>

<p>Diamond中的数据是简单的key-value结构。应用方订阅数据则是基于key来订阅，未订阅的数据当然不会被推送。数据从类型上又划分为聚合和非聚合。因为数据推送者可能很多，在整个分布式环境中，可能有多个推送者在推送相同key的数据，这些数据如果是聚合的，那么所有这些推送者推送的数据会被合并在一起；反之如果是非聚合的，则会出现覆盖。</p>

<p>数据的来源可能是人工通过管理端录入，也可能是其他服务通过配置管理服务的推送接口自动录入。</p>

<h2>架构及实现</h2>

<p>Diamond服务是一个集群，是一个去除了单点的协作集群。如图：</p>

<p><img src="/assets/res/diamond/arch.PNG" alt="" /></p>

<p>图中可分为以下部分讲解：</p>

<h3>服务之间同步</h3>

<p>Diamond服务集群每一个实例都可以对外完整地提供服务，那么意味着每个实例上都有整个集群维护的数据。Diamond有两种方式保证这一点：</p>

<ul>
<li>任何一个实例都有其他实例的地址；任何一个实例上的数据变更时，都会将改变的数据同步到mysql上，然后通知其他所有实例从mysql上进行一次数据拉取(<code>DumpService::dump</code>)，这个过程只拉取改变了的数据</li>
<li>任何一个实例启动后都会以较长的时间间隔（几小时），从mysql进行一次全量的数据拉取(<code>DumpAllProcessor</code>)</li>
</ul>


<p>实现上为了一致性，通知其他实例实际上也包含自己。以服务器收到添加聚合数据为例，处理过程大致为：</p>

<pre><code>DatumController::addDatum // /datum.do?method=addDatum
    PersistService::addAggrConfigInfo 
    MergeDatumService::addMergeTask // 添加一个MergeDataTask，异步处理

MergeTaskProcessor::process
    PersistService::insertOrUpdate
        EventDispatcher.fireEvent(new ConfigDataChangeEvent // 派发一个ConfigDataChangeEvent事件

NotifyService::onEvent // 接收事件并处理
    TaskManager::addTask(..., new NotifyTask // 由此，当数据发生变动，则最终创建了一个NoticyTask

// NotifyTask同样异步处理
NotifyTaskProcessor::process
    foreach server in serverList // 包含自己
        notifyToDump // 调用 /notify.do?method=notifyConfigInfo 从mysql更新变动的数据
</code></pre>

<p>虽然Diamond去除了单点问题，不过问题都下降到了mysql上。但由于其作为配置管理的定位，其数据量就mysql的应用而言算小的了，所以可以一定程度上保证整个服务的可用性。</p>

<h3>数据一致性</h3>

<p>由于Diamond服务器没有master，任何一个实例都可以读写数据，那么针对同一个key的数据则可能面临冲突。这里应该是通过mysql来保证数据的一致性。每一次客户端请求写数据时，Diamond都将写请求投递给mysql，然后通知集群内所有Diamond实例（包括自己）从mysql拉取数据。当然，拉取数据则可能不是每一次写入都能拉出来，也就是最终一致性。</p>

<p>Diamond中没有把数据放入内存，但会放到本地文件。对于客户端的读操作而言，则是直接返回本地文件里的数据。</p>

<h3>服务实例列表</h3>

<p>Diamond服务实例列表是一份静态数据，直接将每个实例的地址存放在一个web server上。无论是Diamond服务还是客户端都从该web server上取出实例列表。</p>

<p>对于客户端而言，当其取出了该列表后，则是随机选择一个节点(<code>ServerListManager.java</code>)，以后的请求都会发往该节点。</p>

<h3>数据同步</h3>

<p>客户端库中以固定时间间隔从服务器拉取数据(<code>ClientWorker::ClientWorker</code>，<code>ClientWorker::checkServerConfigInfo</code>)。只有应用方关心的数据才可能被拉取。另外，为了数据推送的及时，Diamond还使用了一种long polling的技术，其实也是为了突破HTTP协议的局限性。<em>如果整个服务是基于TCP的自定义协议，客户端与服务器保持长连接则没有这些问题</em>。</p>

<h3>数据的变更</h3>

<p>Diamond中很多操作都会检查数据是否发生了变化。标识数据变化则是基于数据对应的MD5值来实现的。</p>

<h2>容灾</h2>

<p>在整个Diamond系统中，几个角色为了提高容灾性，都有自己的缓存，概括为下图：</p>

<p><img src="/assets/res/diamond/failover.PNG" alt="" /></p>

<p>每一个角色出问题时，都可以尽量保证客户端对应用层提供服务。</p>

<h2>参考文档</h2>

<ul>
<li><a href="http://code.taobao.org/p/diamond/src">diamond project</a></li>
<li><a href="http://jm-blog.aliapp.com/?p=1588">diamond专题</a></li>
<li><a href="http://jm-blog.aliapp.com/?p=3450">中间件技术及双十一实践·软负载篇</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[zookeeper节点数与watch的性能测试]]></title>
    <link href="http://codemacro.com/2014/09/21/zk-watch-benchmark/"/>
    <updated>2014-09-21T00:00:00+08:00</updated>
    <id>http://codemacro.com/2014/09/21/zk-watch-benchmark</id>
    <content type="html"><![CDATA[<p>zookeeper中节点数量理论上仅受限于内存，但一个节点下的子节点数量<a href="http://zookeeper-user.578899.n2.nabble.com/ZooKeeper-Limitation-td6675643.html">受限于request/response 1M数据</a> (<a href="http://web.archiveorange.com/archive/v/AQXskdBodZB7kWpjpjHw">size of data / number of znodes</a>)</p>

<p>zookeeper的watch机制用于数据变更时zookeeper的主动通知。watch可以被附加到每一个节点上，那么如果一个应用有10W个节点，那zookeeper中就可能有10W个watch（甚至更多）。每一次在zookeeper完成改写节点的操作时就会检测是否有对应的watch，有的话则会通知到watch。<a href="http://shift-alt-ctrl.iteye.com/blog/1847320">Zookeeper-Watcher机制与异步调用原理</a></p>

<p>本文将关注以下内容：</p>

<ul>
<li>zookeeper的性能是否会受节点数量的影响</li>
<li>zookeeper的性能是否会受watch数量的影响</li>
</ul>


<h2>测试方法</h2>

<p>在3台机器上分别部署一个zookeeper，版本为<code>3.4.3</code>，机器配置：</p>

<pre><code>Intel(R) Xeon(R) CPU E5-2430 0 @ 2.20GHz

16G

java version "1.6.0_32"
Java(TM) SE Runtime Environment (build 1.6.0_32-b05)
OpenJDK (Taobao) 64-Bit Server VM (build 20.0-b12-internal, mixed mode)
</code></pre>

<p>大部分实验JVM堆大小使用默认，也就是<code>1/4 RAM</code>：</p>

<pre><code>java -XX:+PrintFlagsFinal -version | grep HeapSize
</code></pre>

<p>测试客户端使用<a href="https://github.com/phunt/zk-smoketest">zk-smoketest</a>，针对watch的测试则是我自己写的。基于zk-smoketest我写了些脚本可以自动跑数据并提取结果，相关脚本可以在这里找到：<a href="https://github.com/kevinlynx/zk-benchmark">https://github.com/kevinlynx/zk-benchmark</a></p>

<!-- more -->


<h2>测试结果</h2>

<h3>节点数对读写性能的影响</h3>

<p>测试最大10W个节点，度量1秒内操作数(ops)：</p>

<p><img src="/assets/res/zk_benchmark/node_count.png" alt="" /></p>

<p>可见节点数的增加并不会对zookeeper读写性能造成影响。</p>

<h3>节点数据大小对读写性能的影响</h3>

<p>这个网上其实已经有公认的结论。本身单个节点数据越大，对网络方面的吞吐就会造成影响，所以其数据越大读写性能越低也在预料之中。</p>

<p><img src="/assets/res/zk_benchmark/node_size.png" alt="" /></p>

<p>写数据会在zookeeper集群内进行同步，所以其速度整体会比读数据更慢。该实验需要把超时时间进行一定上调，同时我也把JVM最大堆大小调整到8G。</p>

<p>测试结果很明显，节点数据大小会严重影响zookeeper效率。</p>

<h2>watch对读写性能的影响</h2>

<p>zk-smoketest自带的latency测试有个参数<code>--watch_multiple</code>用来指定watch的数量，但其实仅是指定客户端的数量，在server端通过<code>echo whcp | nc 127.0.0.1 4181</code>会发现实际每个节点还是只有一个watch。</p>

<p>在我写的测试中，则是通过创建多个客户端来模拟单个节点上的多个watch。这也更符合实际应用。同时对节点的写也是在另一个独立的客户端中，这样可以避免zookeeper client的实现对测试带来的干扰。</p>

<p>每一次完整的测试，首先是对每个节点添加节点数据的watch，然后在另一个客户端中对这些节点进行数据改写，收集这些改写操作的耗时，以确定添加的watch对这些写操作带来了多大的影响。</p>

<p><img src="/assets/res/zk_benchmark/watch.png" alt="" /></p>

<p>图中，<code>0 watch</code>表示没有对节点添加watch；<code>1 watch</code>表示有一个客户端对每个节点进行了watch；<code>3 watch</code>表示有其他3个客户端对每个节点进行了watch；依次类推。</p>

<p>可见，watch对写操作还是有较大影响的，毕竟需要进行网络传输。同样，这里也显示出整个zookeeper的watch数量同节点数量一样对整体性能没有影响。</p>

<h2>总体结论</h2>

<ul>
<li>对单个节点的操作并不会因为zookeeper中节点的总数而受到影响</li>
<li>数据大小对zookeeper的性能有较大影响，性能和内存都会</li>
<li>单个节点上独立session的watch数对性能有一定影响</li>
</ul>

]]></content>
  </entry>
  
</feed>
