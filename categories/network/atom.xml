<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: network | loop in codes]]></title>
  <link href="http://codemacro.com/categories/network/atom.xml" rel="self"/>
  <link href="http://codemacro.com/"/>
  <updated>2014-09-04T22:34:26+08:00</updated>
  <id>http://codemacro.com/</id>
  <author>
    <name><![CDATA[Kevin Lynx]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[分布式环境中的负载均衡策略]]></title>
    <link href="http://codemacro.com/2014/08/25/lb-policy/"/>
    <updated>2014-08-25T00:00:00+08:00</updated>
    <id>http://codemacro.com/2014/08/25/lb-policy</id>
    <content type="html"><![CDATA[<p>在分布式系统中相同的服务常常会部署很多台，每一台被称为一个服务节点（实例）。通过一些负载均衡策略将服务请求均匀地分布到各个节点，以实现整个系统支撑海量请求的需求。本文描述一些简单的负载均衡策略。</p>

<h2>Round-robin</h2>

<p>简单地轮询。记录一个选择位置，每次请求来时调整该位置到下一个节点：</p>

<pre><code>curId = ++curId % nodeCnt
</code></pre>

<h2>随机选择</h2>

<p>随机地在所有节点中选择：</p>

<pre><code>id = random(nodeCnt);
</code></pre>

<h2>本机优先</h2>

<p>访问后台服务的访问者可能本身是一个整合服务，或者是一个proxy，如果后台服务节点恰好有节点部署在本机的，则可以优先使用。在未找到本机节点时则可以继续走Round-robin策略：</p>

<pre><code>if (node-&gt;ip() == local_ip) {
    return node;
} else {
    return roundRobin();
}
</code></pre>

<!-- more -->


<p>一旦遍历到本机节点，则后面的请求会一直落到本机节点。所以这里可以加上一些权重机制，仅是保证本机节点会被优先选择，但不会被一直选择。例如：</p>

<pre><code>// initial
cur_weight = 100;
...
// select node
cur_weight -= 5;
if (cur_weight &lt;= 0)
    cur_weight = 100;
if (cur_weight &gt; 50 &amp;&amp; node-&gt;ip() == local_ip) {
    return node;
} else {
    return roundRobin();
}
</code></pre>

<h2>本机房优先</h2>

<p>服务节点可能会被部署到多个机房，有时候确实是需要考虑跨机房服务。同<code>本机优先</code>策略类似，本机房优先则是优先考虑位于相同机房内的服务节点。该请求是从哪个机房中的前端服务发送过来的，则需要前端在请求参数中携带上机房ID。</p>

<p>在服务节点对应的数据结构中，也最好按照机房来组织。</p>

<p>本机房优先策略实际上会作为节点选择的第一道工序，它可以把非本机房的节点先过滤掉，然后再传入后面的各种节点选择策略。这里还可以考虑节点数参数，如果本机房的节点过少，则可以不使用该策略，避免流量严重不均。</p>

<h2>Weighted Round-Robin</h2>

<p>加权轮询。相对于普通轮询而言，该策略中每一个节点都有自己的权重，优先选择权重更大的节点。权重可以根据机器性能预先配置。摘抄一下网上的算法：</p>

<pre><code>假设有一组服务器S = {S0, S1, …, Sn-1}，W(Si)表示服务器Si的权值，一个
指示变量i表示上一次选择的服务器，指示变量cw表示当前调度的权值，max(S)
表示集合S中所有服务器的最大权值，gcd(S)表示集合S中所有服务器权值的最大
公约数。变量i初始化为-1，cw初始化为零。

while (true) {
  i = (i + 1) mod n;
  if (i == 0) {
     cw = cw - gcd(S); 
     if (cw &lt;= 0) {
       cw = max(S);
       if (cw == 0)
         return NULL;
     }
  } 
  if (W(Si) &gt;= cw) 
    return Si;
}
</code></pre>

<p>遍历完所有节点后权重衰减，衰减到0后重新开始。这样可以让权重更大的节点被选择得更多。</p>

<h2>Consistent Hash</h2>

<p>一致性哈希。一致性哈希用于在分布式环境中，分布在各个节点上的请求，不会因为新增节点（扩容）或减少节点（节点宕机）而变化。如果每个服务节点上都有自己的缓存，其保存了该节点响应请求时的回应。正常情况下，这些缓存都可以很好地被运用，也即cache命中率较高。</p>

<p>如果某个节点不可用了，我们的选择策略又是基于所有节点的公平选择，那么原来一直分配在节点A上请求就很可能被分配到节点B上，从而导致节点A上的缓存较难被命中。这个时候就可以运用一致性哈希来解决。</p>

<p>其基本思想是，在节点选择区间内，在找节点时以顺时针方向找到不小于该请求对应的哈希值的节点。在这个区间里增加很多虚拟节点，每一个虚拟节点相当于一个物理节点的引用，这样相当于把物理节点变成了一个哈希值区间。这个哈希值区间不会因为增加节点和减少节点而变化，那么对某个请求而言，它就会始终落到这个区间里，也就会始终被分配到原来的节点。</p>

<p>至于这个不可用的节点，其上的请求也会被均匀地分配到其他节点中。</p>

<p>摘抄网上的一段代码：</p>

<pre><code>// 添加一个物理节点时，会随之增加很多虚拟节点
template &lt;class Node, class Data, class Hash&gt;
size_t HashRing&lt;Node, Data, Hash&gt;::AddNode(const Node&amp; node)
{
    size_t hash;
    std::string nodestr = Stringify(node);
    for (unsigned int r = 0; r &lt; replicas_; r++) {
        hash = hash_((nodestr + Stringify(r)).c_str());
        ring_[hash] = node;  // 物理节点和虚拟节点都保存在一个std::map中
    }
    return hash;
}

// 选择data对应的节点，data可以是请求
template &lt;class Node, class Data, class Hash&gt;
const Node&amp; HashRing&lt;Node, Data, Hash&gt;::GetNode(const Data&amp; data) const
{
    if (ring_.empty()) {
        throw EmptyRingException();
    }
    size_t hash = hash_(Stringify(data).c_str()); // 对请求进行哈希
    typename NodeMap::const_iterator it;
    // Look for the first node &gt;= hash
    it = ring_.lower_bound(hash); // 找到第一个不小于请求哈希的节点
    if (it == ring_.end()) {
        // Wrapped around; get the first node
        it = ring_.begin();
    }
    return it-&gt;second;
}
</code></pre>

<p>参考<a href="http://blog.csdn.net/sparkliang/article/details/5279393">一致性 hash 算法(consistent hashing)</a>，<a href="http://www.martinbroadhurst.com/Consistent-Hash-Ring.html">Consistent Hash Ring</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[select真的有限制吗]]></title>
    <link href="http://codemacro.com/2014/06/01/select-limit/"/>
    <updated>2014-06-01T00:00:00+08:00</updated>
    <id>http://codemacro.com/2014/06/01/select-limit</id>
    <content type="html"><![CDATA[<p>在刚开始学习网络编程时，似乎莫名其妙地就会被某人/某资料告诉<code>select</code>函数是有fd(file descriptor)数量限制的。在最近的一次记忆里还有个人笑说<code>select</code>只支持64个fd。我甚至还写过一篇不负责任甚至错误的博客(<a href="http://www.cppblog.com/kevinlynx/archive/2008/05/20/50500.html">突破select的FD_SETSIZE限制</a>)。有人说，直接重新定义<code>FD_SETSIZE</code>就可以突破这个<code>select</code>的限制，也有人说除了重定义这个宏之外还的重新编译内核。</p>

<p>事实具体是怎样的？实际上，造成这些混乱的原因恰好是不同平台对<code>select</code>的实现不一样。</p>

<h2>Windows的实现</h2>

<p><a href="http://msdn.microsoft.com/en-us/library/windows/desktop/ms740141(v=vs.85">MSDN</a>.aspx)上对<code>select</code>的说明：</p>

<pre><code>int select(
  _In_     int nfds,
  _Inout_  fd_set *readfds,
  _Inout_  fd_set *writefds,
  _Inout_  fd_set *exceptfds,
  _In_     const struct timeval *timeout
);

nfds [in] Ignored. The nfds parameter is included only for compatibility with Berkeley sockets.
</code></pre>

<p>第一个参数MSDN只说没有使用，其存在仅仅是为了保持与Berkeley Socket的兼容。</p>

<blockquote><p>The variable FD_SETSIZE determines the maximum number of descriptors in a set. (The default value of FD_SETSIZE is 64, which can be modified by defining FD_SETSIZE to another value before including Winsock2.h.) Internally, socket handles in an fd_set structure are not represented as bit flags as in Berkeley Unix.</p></blockquote>

<p>Windows上<code>select</code>的实现不同于Berkeley Unix，<strong>后者使用位标志来表示socket</strong>。</p>

<!-- more -->


<p>在MSDN的评论中有人提到：</p>

<blockquote><p>Unlike the Linux versions of these macros which use a single calculation to set/check the fd, the Winsock versions use a loop which goes through the entire set of fds each time you call FD_SET or FD_ISSET (check out winsock2.h and you&rsquo;ll see). So you might want to consider an alternative if you have thousands of sockets!</p></blockquote>

<p>不同于Linux下处理<code>fd_set</code>的那些宏(FD_CLR/FD_SET之类)，Windows上这些宏的实现都使用了一个循环，看看这些宏的大致实现(Winsock2.h)：</p>

<pre><code>#define FD_SET(fd, set) do { \
    u_int __i; \
    for (__i = 0; __i &lt; ((fd_set FAR *)(set))-&gt;fd_count; __i++) { \
        if (((fd_set FAR *)(set))-&gt;fd_array[__i] == (fd)) { \
            break; \
        } \
    } \
    if (__i == ((fd_set FAR *)(set))-&gt;fd_count) { \
        if (((fd_set FAR *)(set))-&gt;fd_count &lt; FD_SETSIZE) { \
            ((fd_set FAR *)(set))-&gt;fd_array[__i] = (fd); \
            ((fd_set FAR *)(set))-&gt;fd_count++; \
        } \
    } \
} while(0)
</code></pre>

<p>看下Winsock2.h中关于<code>fd_set</code>的定义：</p>

<pre><code>typedef struct fd_set {
    u_int fd_count;
    SOCKET fd_array[FD_SETSIZE];
} fd_set;
</code></pre>

<p>再看一篇更重要的MSDN <a href="http://msdn.microsoft.com/en-us/library/windows/desktop/ms739169(v=vs.85">Maximum Number of Sockets Supported</a>.aspx)：</p>

<blockquote><p>The Microsoft Winsock provider limits the maximum number of sockets supported only by available memory on the local computer.
The maximum number of sockets that a Windows Sockets application can use is not affected by the manifest constant FD_SETSIZE.
If an application is designed to be capable of working with more than 64 sockets using the select and WSAPoll functions, the implementor should define the manifest FD_SETSIZE in every source file before including the Winsock2.h header file.</p></blockquote>

<p>Windows上<code>select</code>支持的socket数量并不受宏<code>FD_SETSIZE</code>的影响，而仅仅受内存的影响。如果应用程序想使用超过<code>FD_SETSIZE</code>的socket，仅需要重新定义<code>FD_SETSIZE</code>即可。</p>

<p>实际上稍微想想就可以明白，既然<code>fd_set</code>里面已经有一个socket的数量计数，那么<code>select</code>的实现完全可以使用这个计数，而不是<code>FD_SETSIZE</code>这个宏。那么结论是，<strong><code>select</code>至少在Windows上并没有socket支持数量的限制。</strong>当然效率问题这里不谈。</p>

<p>这看起来推翻了我们一直以来没有深究的一个事实。</p>

<h2>Linux的实现</h2>

<p>在上面提到的MSDN中，其实已经提到了Windows与Berkeley Unix实现的不同。在<code>select</code>的API文档中也看到了第一个参数并没有说明其作用。看下Linux的<a href="http://linux.die.net/man/2/select">man</a>：</p>

<blockquote><p>nfds is the highest-numbered file descriptor in any of the three sets, plus 1.</p></blockquote>

<p>第一个参数简单来说就是最大描述符+1。</p>

<blockquote><p>An fd_set is a fixed size buffer. Executing FD_CLR() or FD_SET() with a value of fd that is negative or is equal to or larger than FD_SETSIZE will result in undefined behavior.</p></blockquote>

<p>明确说了，如果调用<code>FD_SET</code>之类的宏fd超过了<code>FD_SETSIZE</code>将导致<code>undefined behavior</code>。也有人专门做了测试：<a href="http://www.moythreads.com/wordpress/2009/12/22/select-system-call-limitation/">select system call limitation in Linux</a>。也有现实遇到的问题：<a href="http://serverfault.com/questions/497086/socket-file-descriptor-1063-is-larger-than-fd-setsize-1024-you-probably-nee">socket file descriptor (1063) is larger than FD_SETSIZE (1024), you probably need to rebuild Apache with a larger FD_SETSIZE</a></p>

<p>看起来在Linux上使用<code>select</code>确实有<code>FD_SETSIZE</code>的限制。有必要看下相关的实现 <a href="http://fxr.watson.org/fxr/source/sys/fd_set.h?v=NETBSD">fd_set.h</a>：</p>

<pre><code>typedef __uint32_t      __fd_mask;

/* 32 = 2 ^ 5 */
#define __NFDBITS       (32)
#define __NFDSHIFT      (5)
#define __NFDMASK       (__NFDBITS - 1)

/*
 * Select uses bit fields of file descriptors.  These macros manipulate
 * such bit fields.  Note: FD_SETSIZE may be defined by the user.
 */

#ifndef FD_SETSIZE
#define FD_SETSIZE      256
#endif

#define __NFD_SIZE      (((FD_SETSIZE) + (__NFDBITS - 1)) / __NFDBITS)

typedef struct fd_set {
    __fd_mask       fds_bits[__NFD_SIZE];
} fd_set;
</code></pre>

<p>在这份实现中不同于Windows实现，它使用了位来表示fd。看下<code>FD_SET</code>系列宏的大致实现：</p>

<pre><code>#define FD_SET(n, p)    \
   ((p)-&gt;fds_bits[(unsigned)(n) &gt;&gt; __NFDSHIFT] |= (1 &lt;&lt; ((n) &amp; __NFDMASK)))
</code></pre>

<p>添加一个fd到<code>fd_set</code>中也不是Windows的遍历，而是直接位运算。这里也有人对另一份类似实现做了剖析：<a href="http://my.oschina.net/u/870054/blog/212063">linux的I/O多路转接select的fd_set数据结构和相应FD_宏的实现分析</a>。在APUE中也提到<code>fd_set</code>：</p>

<blockquote><p>这种数据类型(fd_set)为每一可能的描述符保持了一位。</p></blockquote>

<p>既然<code>fd_set</code>中不包含其保存了多少个fd的计数，那么<code>select</code>的实现里要知道自己要处理多少个fd，那只能使用FD_SETSIZE宏去做判定，但Linux的实现选用了更好的方式，即通过第一个参数让应用层告诉<code>select</code>需要处理的最大fd（这里不是数量）。那么其实现大概为：</p>

<pre><code>for (int i = 0; i &lt; nfds; ++i) {
    if (FD_ISSET...
       ...
}
</code></pre>

<p>如此看来，<strong>Linux的<code>select</code>实现则是受限于<code>FD_SETSIZE</code>的大小</strong>。这里也看到，<code>fd_set</code>使用位数组来保存fd，那么fd本身作为一个int数，其值就不能超过<code>FD_SETSIZE</code>。<strong>这不仅仅是数量的限制，还是其取值的限制</strong>。实际上，Linux上fd的取值是保证了小于<code>FD_SETSIZE</code>的（但不是不变的）<a href="http://stackoverflow.com/questions/12583927/is-the-value-of-a-linux-file-descriptor-always-smaller-than-the-open-file-limits">Is the value of a Linux file descriptor always smaller than the open file limits?</a>：</p>

<blockquote><p>Each process is further limited via the setrlimit(2) RLIMIT_NOFILE per-process limit on the number of open files. 1024 is a common RLIMIT_NOFILE limit. (It&rsquo;s very easy to change this limit via /etc/security/limits.conf.)</p></blockquote>

<p>fd的取值会小于<code>RLIMIT_NOFILE</code>，有很多方法可以改变这个值。这个值默认情况下和<code>FD_SETSIZE</code>应该是一样的。这个信息告诉我们，<strong>Linux下fd的取值应该是从0开始递增的</strong>（理论上，实际上还有stdin/stdout/stderr之类的fd）。这才能保证<code>select</code>的那些宏可以工作。</p>

<h2>应用层使用</h2>

<p>标准的<code>select</code>用法应该大致如下：</p>

<pre><code>while (true) {
    ...
    select(...)
    for-each socket {
        if (FD_ISSET(fd, set))
            ...
    }

    ...
}
</code></pre>

<p>即遍历目前管理的fd，通过<code>FD_ISSET</code>去判定当前fd是否有IO事件。因为Windows的实现<code>FD_ISSET</code>都是一个循环，所以有了另一种不跨平台的用法：</p>

<pre><code>while (true) {
    ...
    select(. &amp;read_sockets, &amp;write_sockets..)
    for-each read_socket {
        use fd.fd_array[i)
    }
    ...
}
</code></pre>

<h2>总结</h2>

<ul>
<li>Windows上<code>select</code>没有fd数量的限制，但因为使用了循环来检查，所以效率相对较低</li>
<li>Linux上<code>select</code>有<code>FD_SETSIZE</code>的限制，但其相对效率较高</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[muduo源码阅读]]></title>
    <link href="http://codemacro.com/2014/05/04/muduo-source/"/>
    <updated>2014-05-04T00:00:00+08:00</updated>
    <id>http://codemacro.com/2014/05/04/muduo-source</id>
    <content type="html"><![CDATA[<p>最近简单读了下<a href="http://blog.csdn.net/solstice/article/details/5848547">muduo</a>的源码，本文对其主要实现/结构简单总结下。</p>

<p>muduo的主要源码位于net文件夹下，base文件夹是一些基础代码，不影响理解网络部分的实现。muduo主要类包括：</p>

<ul>
<li>EventLoop</li>
<li>Channel</li>
<li>Poller</li>
<li>TcpConnection</li>
<li>TcpClient</li>
<li>TcpServer</li>
<li>Connector</li>
<li>Acceptor</li>
<li>EventLoopThread</li>
<li>EventLoopThreadPool</li>
</ul>


<p>其中，Poller（及其实现类）包装了Poll/EPoll，封装了OS针对设备(fd)的操作；Channel是设备fd的包装，在muduo中主要包装socket；TcpConnection抽象一个TCP连接，无论是客户端还是服务器只要建立了网络连接就会使用TcpConnection；TcpClient/TcpServer分别抽象TCP客户端和服务器；Connector/Acceptor分别包装TCP客户端和服务器的建立连接/接受连接；EventLoop是一个主控类，是一个事件发生器，它驱动Poller产生/发现事件，然后将事件派发到Channel处理；EventLoopThread是一个带有EventLoop的线程；EventLoopThreadPool自然是一个EventLoopThread的资源池，维护一堆EventLoopThread。</p>

<p>阅读库源码时可以从库的接口层着手，看看关键功能是如何实现的。对于muduo而言，可以从TcpServer/TcpClient/EventLoop/TcpConnection这几个类着手。接下来看看主要功能的实现：</p>

<!-- more -->


<h2>建立连接</h2>

<p><div class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="n">TcpClient</span><span class="o">::</span><span class="n">connect</span>
        <span class="o">-&gt;</span> <span class="n">Connector</span><span class="o">::</span><span class="n">start</span>
            <span class="o">-&gt;</span> <span class="n">EventLoop</span><span class="o">::</span><span class="n">runInLoop</span><span class="p">(</span><span class="n">Connector</span><span class="o">::</span><span class="n">startInLoop</span><span class="o">&amp;</span><span class="n">hellip</span><span class="p">;</span>
            <span class="o">-&gt;</span> <span class="n">Connector</span><span class="o">::</span><span class="n">connect</span>           <span class="o">&lt;</span><span class="n">br</span><span class="o">/&gt;</span></code></pre></div></p>

<p>EventLoop::runInLoop接口用于在this所在的线程运行某个函数，这个后面看下EventLoop的实现就可以了解。 网络连接的最终建立是在Connector::connect中实现，建立连接之后会创建一个Channel来代表这个socket，并且绑定事件监听接口。最后最重要的是，调用<code>Channel::enableWriting</code>。<code>Channel</code>有一系列的enableXX接口，这些接口用于标识自己关心某IO事件。后面会看到他们的实现。</p>

<p>Connector监听的主要事件无非就是连接已建立，用它监听读数据/写数据事件也不符合设计。TcpConnection才是做这种事的。</p>

<h2>客户端收发数据</h2>

<p>当Connector发现连接真正建立好后，会回调到<code>TcpClient::newConnection</code>，在TcpClient构造函数中：</p>

<p><div class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="n">connector</span><span class="o">&lt;</span><span class="n">em</span><span class="o">&gt;-&gt;</span><span class="n">setNewConnectionCallback</span><span class="p">(</span>
      <span class="n">boost</span><span class="o">::</span><span class="n">bind</span><span class="p">(</span><span class="o">&amp;</span><span class="n">amp</span><span class="p">;</span><span class="n">TcpClient</span><span class="o">::</span><span class="n">newConnection</span><span class="p">,</span> <span class="k">this</span><span class="p">,</span> <span class="o">&lt;/</span><span class="n">em</span><span class="o">&gt;</span><span class="mi">1</span><span class="p">));</span></code></pre></div></p>

<p><code>TcpClient::newConnection</code>中创建一个TcpConnection来代表这个连接：</p>

<p><div class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="n">TcpConnectionPtr</span> <span class="nf">conn</span><span class="p">(</span><span class="k">new</span> <span class="n">TcpConnection</span><span class="p">(</span><span class="n">loop_</span><span class="p">,</span>
                                            <span class="n">connName</span><span class="p">,</span>
                                            <span class="n">sockfd</span><span class="p">,</span>
                                            <span class="n">localAddr</span><span class="p">,</span>
                                            <span class="n">peerAddr</span><span class="p">));</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>

<span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="n">conn</span><span class="o">-&amp;</span><span class="n">gt</span><span class="p">;</span><span class="n">setConnectionCallback</span><span class="p">(</span><span class="n">connectionCallback_</span><span class="p">);</span>
<span class="n">conn</span><span class="o">-&amp;</span><span class="n">gt</span><span class="p">;</span><span class="n">setMessageCallback</span><span class="p">(</span><span class="n">messageCallback_</span><span class="p">);</span>
<span class="n">conn</span><span class="o">-&amp;</span><span class="n">gt</span><span class="p">;</span><span class="n">setWriteCompleteCallback</span><span class="p">(</span><span class="n">writeCompleteCallback_</span><span class="p">);</span>
<span class="p">...</span>
<span class="n">conn</span><span class="o">-&amp;</span><span class="n">gt</span><span class="p">;</span><span class="n">connectEstablished</span><span class="p">();</span>
<span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>

<span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span></code></pre></div></p>

<p>并同时设置事件回调，以上设置的回调都是应用层（即库的使用者）的接口。每一个TcpConnection都有一个Channel，毕竟每一个网络连接都对应了一个socket fd。在TcpConnection构造函数中创建了一个Channel，并设置事件回调函数。</p>

<p><code>TcpConnection::connectEstablished</code>函数最主要的是通知Channel自己开始关心IO读取事件：</p>

<p><div class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">void</span> <span class="n">TcpConnection</span><span class="o">::</span><span class="n">connectEstablished</span><span class="p">()</span>
    <span class="p">{</span>
        <span class="o">&amp;</span><span class="n">hellip</span><span class="p">;</span>
        <span class="n">channel_</span><span class="o">-&gt;</span><span class="n">enableReading</span><span class="p">();</span></code></pre></div></p>

<p>这是自此我们看到的第二个<code>Channel::enableXXX</code>接口，这些接口是如何实现关心IO事件的呢？这个后面讲到。</p>

<p>muduo的数据发送都是通过<code>TcpConnection::send</code>完成，这个就是一般网络库中在不使用OS的异步IO情况下的实现：缓存应用层传递过来的数据，在IO设备可写的情况下尽量写入数据。这个主要实现在<code>TcpConnection::sendInLoop</code>中。</p>

<p><div class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="n">TcpConnection</span><span class="o">::</span><span class="n">sendInLoop</span><span class="p">(</span><span class="o">&amp;</span><span class="n">hellip</span><span class="p">;.)</span> <span class="p">{</span>
        <span class="o">&amp;</span><span class="n">hellip</span><span class="p">;</span>
        <span class="c1">// if no thing in output queue, try writing directly</span>
        <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">channel</span><span class="o">&lt;</span><span class="n">em</span><span class="o">&gt;-&gt;</span><span class="n">isWriting</span><span class="p">()</span> <span class="o">&amp;</span><span class="n">amp</span><span class="p">;</span><span class="o">&amp;</span><span class="n">amp</span><span class="p">;</span> <span class="n">outputBuffer</span><span class="o">&lt;/</span><span class="n">em</span><span class="o">&gt;</span><span class="p">.</span><span class="n">readableBytes</span><span class="p">()</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span>  <span class="c1">// 设备可写且没有缓存时立即写入</span>
        <span class="p">{</span>
            <span class="n">nwrote</span> <span class="o">=</span> <span class="n">sockets</span><span class="o">::</span><span class="n">write</span><span class="p">(</span><span class="n">channel</span><span class="o">&lt;</span><span class="n">em</span><span class="o">&gt;-&gt;</span><span class="n">fd</span><span class="p">(),</span> <span class="n">data</span><span class="p">,</span> <span class="n">len</span><span class="p">);</span>
        <span class="p">}</span>
        <span class="o">&amp;</span><span class="n">hellip</span><span class="p">;</span>
        <span class="c1">// 否则加入数据到缓存，等待IO可写时再写</span>
        <span class="n">outputBuffer</span><span class="o">&lt;/</span><span class="n">em</span><span class="o">&gt;</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="k">static_cast</span><span class="o">&lt;</span><span class="k">const</span> <span class="kt">char</span><span class="o">*&gt;</span><span class="p">(</span><span class="n">data</span><span class="p">)</span><span class="o">+</span><span class="n">nwrote</span><span class="p">,</span> <span class="n">remaining</span><span class="p">);</span>
        <span class="k">if</span> <span class="p">(</span><span class="o">!</span><span class="n">channel</span><span class="o">&lt;</span><span class="n">em</span><span class="o">&gt;-&gt;</span><span class="n">isWriting</span><span class="p">())</span>
        <span class="p">{</span>
            <span class="c1">// 注册关心IO写事件，Poller就会对写做检测</span>
            <span class="n">channel</span><span class="o">&lt;/</span><span class="n">em</span><span class="o">&gt;-&gt;</span><span class="n">enableWriting</span><span class="p">();</span>
        <span class="p">}</span>
        <span class="o">&amp;</span><span class="n">hellip</span><span class="p">;</span>   <span class="o">&lt;</span><span class="n">br</span><span class="o">/&gt;</span>
    <span class="p">}</span></code></pre></div></p>

<p>当IO可写时，Channel就会回调<code>TcpConnection::handleWrite</code>（构造函数中注册）</p>

<p><div class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">void</span> <span class="n">TcpConnection</span><span class="o">::</span><span class="n">handleWrite</span><span class="p">()</span>
    <span class="p">{</span>
        <span class="o">&amp;</span><span class="n">hellip</span><span class="p">;</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">channel</span><span class="o">&lt;</span><span class="n">em</span><span class="o">&gt;-&gt;</span><span class="n">isWriting</span><span class="p">())</span>
        <span class="p">{</span>
            <span class="kt">ssize_t</span> <span class="n">n</span> <span class="o">=</span> <span class="n">sockets</span><span class="o">::</span><span class="n">write</span><span class="p">(</span><span class="n">channel</span><span class="o">&lt;/</span><span class="n">em</span><span class="o">&gt;-&gt;</span><span class="n">fd</span><span class="p">(),</span>
                               <span class="n">outputBuffer</span><span class="o">&lt;</span><span class="n">em</span><span class="o">&gt;</span><span class="p">.</span><span class="n">peek</span><span class="p">(),</span>
                               <span class="n">outputBuffer</span><span class="o">&lt;/</span><span class="n">em</span><span class="o">&gt;</span><span class="p">.</span><span class="n">readableBytes</span><span class="p">());</span></code></pre></div></p>

<p>服务器端的数据收发同客户端机制一致，不同的是连接(TcpConnection)的建立方式不同。</p>

<h2>服务器接收连接</h2>

<p>服务器接收连接的实现在一个网络库中比较重要。muduo中通过Acceptor类来接收连接。在TcpClient中，其Connector通过一个关心Channel可写的事件来通过连接已建立；在Acceptor中则是通过一个Channel可读的事件来表示有新的连接到来：</p>

<p><div class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="n">Acceptor</span><span class="o">::</span><span class="n">Acceptor</span><span class="p">(</span><span class="o">&amp;</span><span class="n">hellip</span><span class="p">;.)</span> <span class="p">{</span>
        <span class="o">&amp;</span><span class="n">hellip</span><span class="p">;</span>
        <span class="n">acceptChannel_</span><span class="p">.</span><span class="n">setReadCallback</span><span class="p">(</span>
            <span class="n">boost</span><span class="o">::</span><span class="n">bind</span><span class="p">(</span><span class="o">&amp;</span><span class="n">amp</span><span class="p">;</span><span class="n">Acceptor</span><span class="o">::</span><span class="n">handleRead</span><span class="p">,</span> <span class="k">this</span><span class="p">));</span>
        <span class="o">&amp;</span><span class="n">hellip</span><span class="p">;</span>
    <span class="p">}</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>

<span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="kt">void</span> <span class="n">Acceptor</span><span class="o">::</span><span class="n">handleRead</span><span class="p">()</span>
<span class="p">{</span>
    <span class="p">...</span>
    <span class="kt">int</span> <span class="n">connfd</span> <span class="o">=</span> <span class="n">acceptSocket_</span><span class="p">.</span><span class="n">accept</span><span class="p">(</span><span class="o">&amp;</span><span class="n">amp</span><span class="p">;</span><span class="n">peerAddr</span><span class="p">);</span> <span class="c1">// 接收连接获得一个新的socket</span>
    <span class="k">if</span> <span class="p">(</span><span class="n">connfd</span> <span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span><span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="p">...</span>
        <span class="n">newConnectionCallback_</span><span class="p">(</span><span class="n">connfd</span><span class="p">,</span> <span class="n">peerAddr</span><span class="p">);</span> <span class="c1">// 回调到TcpServer::newConnection</span>
<span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>

<span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span></code></pre></div></p>

<p><code>TcpServer::newConnection</code>中建立一个TcpConnection，并将其附加到一个EventLoopThread中，简单来说就是给其配置一个线程：</p>

<p><div class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">void</span> <span class="n">TcpServer</span><span class="o">::</span><span class="n">newConnection</span><span class="p">(</span><span class="kt">int</span> <span class="n">sockfd</span><span class="p">,</span> <span class="k">const</span> <span class="n">InetAddress</span><span class="o">&amp;</span><span class="n">amp</span><span class="p">;</span> <span class="n">peerAddr</span><span class="p">)</span>
    <span class="p">{</span>
        <span class="o">&amp;</span><span class="n">hellip</span><span class="p">;</span>
        <span class="n">EventLoop</span><span class="o">*</span> <span class="n">ioLoop</span> <span class="o">=</span> <span class="n">threadPool</span><span class="o">&lt;</span><span class="n">em</span><span class="o">&gt;-&gt;</span><span class="n">getNextLoop</span><span class="p">();</span>
        <span class="n">TcpConnectionPtr</span> <span class="nf">conn</span><span class="p">(</span><span class="k">new</span> <span class="n">TcpConnection</span><span class="p">(</span><span class="n">ioLoop</span><span class="p">,</span>
                                                <span class="n">connName</span><span class="p">,</span>
                                                <span class="n">sockfd</span><span class="p">,</span>
                                                <span class="n">localAddr</span><span class="p">,</span>
                                                <span class="n">peerAddr</span><span class="p">));</span>
        <span class="n">connections</span><span class="o">&lt;/</span><span class="n">em</span><span class="o">&gt;</span><span class="p">[</span><span class="n">connName</span><span class="p">]</span> <span class="o">=</span> <span class="n">conn</span><span class="p">;</span>
        <span class="o">&amp;</span><span class="n">hellip</span><span class="p">;</span>
        <span class="n">ioLoop</span><span class="o">-&gt;</span><span class="n">runInLoop</span><span class="p">(</span><span class="n">boost</span><span class="o">::</span><span class="n">bind</span><span class="p">(</span><span class="o">&amp;</span><span class="n">amp</span><span class="p">;</span><span class="n">TcpConnection</span><span class="o">::</span><span class="n">connectEstablished</span><span class="p">,</span> <span class="n">conn</span><span class="p">));</span></code></pre></div></p>

<h2>IO的驱动</h2>

<p>之前提到，一旦要关心某IO事件了，就调用<code>Channel::enableXXX</code>，这个如何实现的呢？</p>

<p><div class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="k">class</span> <span class="nc">Channel</span> <span class="p">{</span>
        <span class="o">&amp;</span><span class="n">hellip</span><span class="p">;</span>
        <span class="kt">void</span> <span class="nf">enableReading</span><span class="p">()</span> <span class="p">{</span> <span class="n">events</span><span class="o">&lt;</span><span class="n">em</span><span class="o">&gt;</span> <span class="o">|=</span> <span class="n">kReadEvent</span><span class="p">;</span> <span class="n">update</span><span class="p">();</span> <span class="p">}</span>
        <span class="kt">void</span> <span class="nf">enableWriting</span><span class="p">()</span> <span class="p">{</span> <span class="n">events</span><span class="o">&lt;/</span><span class="n">em</span><span class="o">&gt;</span> <span class="o">|=</span> <span class="n">kWriteEvent</span><span class="p">;</span> <span class="n">update</span><span class="p">();</span> <span class="p">}</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>

<span class="o">&lt;</span><span class="n">pre</span><span class="o">&gt;&lt;</span><span class="n">code</span><span class="o">&gt;</span><span class="kt">void</span> <span class="n">Channel</span><span class="o">::</span><span class="n">update</span><span class="p">()</span>
<span class="p">{</span>
    <span class="n">loop_</span><span class="o">-&amp;</span><span class="n">gt</span><span class="p">;</span><span class="n">updateChannel</span><span class="p">(</span><span class="k">this</span><span class="p">);</span>
<span class="p">}</span>

<span class="kt">void</span> <span class="n">EventLoop</span><span class="o">::</span><span class="n">updateChannel</span><span class="p">(</span><span class="n">Channel</span><span class="o">*</span> <span class="n">channel</span><span class="p">)</span>
<span class="p">{</span>
    <span class="p">...</span>
    <span class="n">poller_</span><span class="o">-&amp;</span><span class="n">gt</span><span class="p">;</span><span class="n">updateChannel</span><span class="p">(</span><span class="n">channel</span><span class="p">);</span>
<span class="p">}</span>
<span class="o">&lt;/</span><span class="n">code</span><span class="o">&gt;&lt;/</span><span class="n">pre</span><span class="o">&gt;</span>

<span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span></code></pre></div></p>

<p>最终调用到<code>Poller::upateChannel</code>。muduo中有两个Poller的实现，分别是Poll和EPoll，可以选择简单的Poll来看：</p>

<p><div class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">void</span> <span class="n">PollPoller</span><span class="o">::</span><span class="n">updateChannel</span><span class="p">(</span><span class="n">Channel</span><span class="o">*</span> <span class="n">channel</span><span class="p">)</span>
    <span class="p">{</span>
      <span class="o">&amp;</span><span class="n">hellip</span><span class="p">;</span>
      <span class="k">if</span> <span class="p">(</span><span class="n">channel</span><span class="o">-&gt;</span><span class="n">index</span><span class="p">()</span> <span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="mi">0</span><span class="p">)</span>
      <span class="p">{</span>
        <span class="c1">// a new one, add to pollfds&lt;em&gt;</span>
        <span class="n">assert</span><span class="p">(</span><span class="n">channels</span><span class="o">&lt;/</span><span class="n">em</span><span class="o">&gt;</span><span class="p">.</span><span class="n">find</span><span class="p">(</span><span class="n">channel</span><span class="o">-&gt;</span><span class="n">fd</span><span class="p">())</span> <span class="o">==</span> <span class="n">channels</span><span class="o">&lt;</span><span class="n">em</span><span class="o">&gt;</span><span class="p">.</span><span class="n">end</span><span class="p">());</span>
        <span class="k">struct</span> <span class="n">pollfd</span> <span class="n">pfd</span><span class="p">;</span>
        <span class="n">pfd</span><span class="p">.</span><span class="n">fd</span> <span class="o">=</span> <span class="n">channel</span><span class="o">-&gt;</span><span class="n">fd</span><span class="p">();</span>
        <span class="n">pfd</span><span class="p">.</span><span class="n">events</span> <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">short</span><span class="o">&gt;</span><span class="p">(</span><span class="n">channel</span><span class="o">-&gt;</span><span class="n">events</span><span class="p">());</span> <span class="c1">// 也就是Channel::enableXXX操作的那个events&lt;/em&gt;</span>
        <span class="n">pfd</span><span class="p">.</span><span class="n">revents</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span>
        <span class="n">pollfds</span><span class="o">&lt;</span><span class="n">em</span><span class="o">&gt;</span><span class="p">.</span><span class="n">push_back</span><span class="p">(</span><span class="n">pfd</span><span class="p">);</span> <span class="c1">// 加入一个新的pollfd</span>
        <span class="kt">int</span> <span class="n">idx</span> <span class="o">=</span> <span class="k">static_cast</span><span class="o">&lt;</span><span class="kt">int</span><span class="o">&gt;</span><span class="p">(</span><span class="n">pollfds</span><span class="o">&lt;/</span><span class="n">em</span><span class="o">&gt;</span><span class="p">.</span><span class="n">size</span><span class="p">())</span><span class="o">-</span><span class="mi">1</span><span class="p">;</span>
        <span class="n">channel</span><span class="o">-&gt;</span><span class="n">set_index</span><span class="p">(</span><span class="n">idx</span><span class="p">);</span>
        <span class="n">channels_</span><span class="p">[</span><span class="n">pfd</span><span class="p">.</span><span class="n">fd</span><span class="p">]</span> <span class="o">=</span> <span class="n">channel</span><span class="p">;</span></code></pre></div></p>

<p>可见Poller就是把Channel关心的IO事件转换为OS提供的IO模型数据结构上。通过查看关键的<code>pollfds_</code>的使用，可以发现其主要是在Poller::poll接口里。这个接口会在EventLoop的主循环中不断调用：</p>

<p><div class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">void</span> <span class="n">EventLoop</span><span class="o">::</span><span class="n">loop</span><span class="p">()</span>
    <span class="p">{</span>
      <span class="o">&amp;</span><span class="n">hellip</span><span class="p">;</span>
      <span class="k">while</span> <span class="p">(</span><span class="o">!</span><span class="n">quit</span><span class="o">&lt;</span><span class="n">em</span><span class="o">&gt;</span><span class="p">)</span>
      <span class="p">{</span>
        <span class="n">activeChannels</span><span class="o">&lt;/</span><span class="n">em</span><span class="o">&gt;</span><span class="p">.</span><span class="n">clear</span><span class="p">();</span>
        <span class="n">pollReturnTime</span><span class="o">&lt;</span><span class="n">em</span><span class="o">&gt;</span> <span class="o">=</span> <span class="n">poller</span><span class="o">&lt;/</span><span class="n">em</span><span class="o">&gt;-&gt;</span><span class="n">poll</span><span class="p">(</span><span class="n">kPollTimeMs</span><span class="p">,</span> <span class="o">&amp;</span><span class="n">amp</span><span class="p">;</span><span class="n">activeChannels</span><span class="o">&lt;</span><span class="n">em</span><span class="o">&gt;</span><span class="p">);</span>
        <span class="o">&amp;</span><span class="n">hellip</span><span class="p">;</span>
        <span class="k">for</span> <span class="p">(</span><span class="n">ChannelList</span><span class="o">::</span><span class="n">iterator</span> <span class="n">it</span> <span class="o">=</span> <span class="n">activeChannels</span><span class="o">&lt;/</span><span class="n">em</span><span class="o">&gt;</span><span class="p">.</span><span class="n">begin</span><span class="p">();</span>
            <span class="n">it</span> <span class="o">!=</span> <span class="n">activeChannels</span><span class="o">&lt;</span><span class="n">em</span><span class="o">&gt;</span><span class="p">.</span><span class="n">end</span><span class="p">();</span> <span class="o">++</span><span class="n">it</span><span class="p">)</span>
        <span class="p">{</span>
          <span class="n">currentActiveChannel</span><span class="o">&lt;/</span><span class="n">em</span><span class="o">&gt;</span> <span class="o">=</span> <span class="o">*</span><span class="n">it</span><span class="p">;</span>
          <span class="n">currentActiveChannel</span><span class="o">&lt;</span><span class="n">em</span><span class="o">&gt;-&gt;</span><span class="n">handleEvent</span><span class="p">(</span><span class="n">pollReturnTime</span><span class="o">&lt;/</span><span class="n">em</span><span class="o">&gt;</span><span class="p">);</span> <span class="c1">// 获得IO事件，通知各注册回调</span>
        <span class="p">}</span></code></pre></div></p>

<p>整个流程可总结为：各Channel内部会把自己关心的事件告诉给Poller，Poller由EventLoop驱动检测IO，然后返回哪些Channel发生了事件，EventLoop再驱动这些Channel调用各注册回调。</p>

<p>从这个过程中可以看出，EventLoop就是一个事件产生器。</p>

<h2>线程模型</h2>

<p>在muduo的服务器中，muduo的线程模型是怎样的呢？它如何通过线程来支撑高并发呢？其实很简单，它为每一个线程配置了一个EventLoop，这个线程同时被附加了若干个网络连接，这个EventLoop服务于这些网络连接，为这些连接收集并派发IO事件。</p>

<p>回到<code>TcpServer::newConnection</code>中：</p>

<p><div class="highlight"><pre><code class="language-c--" data-lang="c++"><span class="kt">void</span> <span class="n">TcpServer</span><span class="o">::</span><span class="n">newConnection</span><span class="p">(</span><span class="kt">int</span> <span class="n">sockfd</span><span class="p">,</span> <span class="k">const</span> <span class="n">InetAddress</span><span class="o">&amp;</span><span class="n">amp</span><span class="p">;</span> <span class="n">peerAddr</span><span class="p">)</span>
    <span class="p">{</span>
      <span class="o">&amp;</span><span class="n">hellip</span><span class="p">;</span>
      <span class="n">EventLoop</span><span class="o">*</span> <span class="n">ioLoop</span> <span class="o">=</span> <span class="n">threadPool_</span><span class="o">-&gt;</span><span class="n">getNextLoop</span><span class="p">();</span>
      <span class="o">&amp;</span><span class="n">hellip</span><span class="p">;</span>
      <span class="n">TcpConnectionPtr</span> <span class="nf">conn</span><span class="p">(</span><span class="k">new</span> <span class="n">TcpConnection</span><span class="p">(</span><span class="n">ioLoop</span><span class="p">,</span> <span class="c1">// 使用这个选择到的线程中的EventLoop</span>
                                              <span class="n">connName</span><span class="p">,</span>
                                              <span class="n">sockfd</span><span class="p">,</span>
                                              <span class="n">localAddr</span><span class="p">,</span>
                                              <span class="n">peerAddr</span><span class="p">));</span>
      <span class="o">&amp;</span><span class="n">hellip</span><span class="p">;</span>
      <span class="n">ioLoop</span><span class="o">-&gt;</span><span class="n">runInLoop</span><span class="p">(</span><span class="n">boost</span><span class="o">::</span><span class="n">bind</span><span class="p">(</span><span class="o">&amp;</span><span class="n">amp</span><span class="p">;</span><span class="n">TcpConnection</span><span class="o">::</span><span class="n">connectEstablished</span><span class="p">,</span> <span class="n">conn</span><span class="p">));</span></code></pre></div></p>

<p>注意<code>TcpConnection::connectEstablished</code>是如何通过Channel注册关心的IO事件到<code>ioLoop</code>的。</p>

<p>极端来说，muduo的每一个连接线程可以只为一个网络连接服务，这就有点类似于thread per connection模型了。</p>

<h2>网络模型</h2>

<p>传说中的Reactor模式，以及one loop per thread，基于EventLoop的作用，以及线程池与TcpConnection的关系，可以醍醐灌顶般理解以下这张muduo的网络模型图了：</p>

<p><img src="/assets/res/muduo-model.png" alt="muduo-model" /></p>

<h2>总结</h2>

<p>本文主要对muduo的主要结构及主要机制的实现做了描述，其他如Buffer的实现、定时器的实现大家都可以自行研究。muduo的源码很清晰，通过源码及配合<a href="http://blog.csdn.net/solstice">陈硕博客</a>上的内容可以学到一些网络编程方面的经验。</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[dhtcrawler2换用sphinx搜索]]></title>
    <link href="http://codemacro.com/2013/08/08/sphinx-dhtcrawler/"/>
    <updated>2013-08-08T00:00:00+08:00</updated>
    <id>http://codemacro.com/2013/08/08/sphinx-dhtcrawler</id>
    <content type="html"><![CDATA[<p>dhtcrawler2最开始使用mongodb自带的全文搜索引擎搜索资源。搜索一些短关键字时很容易导致erlang进程call timeout，也就是查询时间太长。对于像<code>avi</code>这种关键字，搜索时间长达十几秒。搜索的资源数量200万左右。这其中大部分资源只是对root文件名进行了索引，即对于多文件资源而言没有索引单个文件名。索引方式有部分资源是按照字符串子串的形式，没有拆词，非常占用存储空间；有部分是使用了rmmseg（我编译了rmmseg-cpp作为erlang nif库调用 <a href="https://github.com/kevinlynx/erl-rmmseg">erl-rmmseg</a>）进行了拆词，占用空间小了很多，但由于词库问题很多片里的词汇没拆出来。</p>

<p>很早以前我以为搜索耗时的原因是因为数据库太忙，想部署个mongodb集群出来。后来发现数据库没有任何读写的状态下，查询依然慢。终于只好放弃mongodb自带的文本搜索。于是我改用sphinx。简单起见，我直接下载了<a href="http://www.coreseek.cn/">coreseek4.1</a>（sphinx的一个支持中文拆词的包装）。</p>

<p>现在，已经导入了200多万的资源进sphinx，并且索引了所有文件名，索引文件达800M。对于<code>avi</code>关键字的搜索大概消耗0.2秒的时间。<a href="http://bt.cm/e/http_handler:search?q=avi">搜索试试</a>。</p>

<p>以下记录下sphinx在dhtcrawler的应用</p>

<h3>sphinx简介</h3>

<p>sphinx包含两个主要的程序：indexer和searchd。indexer用于建立文本内容的索引，然后searchd基于这些索引提供文本搜索功能，而要使用该功能，可以遵循searchd的网络协议连接searchd这个服务来使用。</p>

<p>indexer可以通过多种方式来获取这些文本内容，文本内容的来源称为数据源。sphinx内置mysql这种数据源，意思是可以直接从mysql数据库中取得数据。sphinx还支持xmlpipe2这种数据源，其数据以xml格式提供给indexer。要导入mongodb数据库里的内容，可以选择使用xmlpipe2这种方式。</p>

<!-- more -->


<h3>sphinx document</h3>

<p>xmlpipe2数据源需要按照以下格式提交：</p>

<pre><code>&lt;sphinx:docset&gt;
    &lt;sphinx:schema&gt;
        &lt;sphinx:field name="subject"/&gt;
        &lt;sphinx:field name="files"/&gt;
        &lt;sphinx:attr name="hash1" type="int" bits="32"/&gt;
        &lt;sphinx:attr name="hash2" type="int" bits="32"/&gt;
    &lt;/sphinx:schema&gt;
    &lt;sphinx:document id="1"&gt;
        &lt;subject&gt;this is the subject&lt;/subject&gt;
        &lt;files&gt;file content&lt;/files&gt;
        &lt;hash1&gt;111&lt;/hash1&gt;
    &lt;/sphinx:document&gt;
&lt;/sphinx:docset&gt;
</code></pre>

<p>该文件包含两大部分：<code>schema</code>和<code>documents</code>，其中<code>schema</code>又包含两部分：<code>field</code>和<code>attr</code>，其中由<code>field</code>标识的字段就会被indexer读取并全部作为输入文本建立索引，而<code>attr</code>则标识查询结果需要附带的信息；<code>documents</code>则是由一个个<code>sphinx:document</code>组成，即indexer真正要处理的数据。注意其中被<code>schema</code>引用的属性名。</p>

<p>document一个很重要的属性就是它的id。这个id对应于sphinx需要唯一，查询结果也会包含此id。一般情况下，此id可以直接是数据库主键，可用于查询到详细信息。searchd搜索关键字，其实可以看作为搜索这些document，搜索出来的结果也是这些document，搜索结果中主要包含schema中指定的attr。</p>

<h3>增量索引</h3>

<p>数据源的数据一般是变化的，新增的数据要加入到sphinx索引文件中，才能使得searchd搜索到新录入的数据。要不断地加入新数据，可以使用增量索引机制。增量索引机制中，需要一个主索引和一个次索引(delta index)。每次新增的数据都建立为次索引，然后一段时间后再合并进主索引。这个过程主要还是使用indexer和searchd程序。实际上，searchd是一个需要一直运行的服务，而indexer则是一个建立完索引就退出的工具程序。所以，这里的增量索引机制，其中涉及到的“每隔一定时间就合并”这种工作，需要自己写程序来协调（或通过其他工具）</p>

<h3>sphinx与mongodb</h3>

<p>上面提到，一般sphinx document的id都是使用的数据库主键，以方便查询。但mongodb中默认情况不使用数字作为主键。dhtcrawler的资源数据库使用的是资源info-hash作为主键，这无法作为sphinx document的id。一种解决办法是，将该hash按位拆分，拆分成若干个sphinx document attr支持位数的整数。例如，info-hash是一个160位的id，如果使用32位的attr（高版本的sphinx支持64位的整数），那么可以把该info-hash按位拆分成5个attr。而sphinx document id则可以使用任意数字，只要保证不冲突就行。当获得查询结果时，取得对应的attr，组合为info-hash即可。</p>

<p>mongodb默认的Object id也可以按这种方式拆分。</p>

<h3>dhtcrawler2与sphinx</h3>

<p>dhtcrawler2中我自己写了一个导入程序。该程序从mongodb中读出数据，数据到一定量时，就输出为xmlpipe2格式的xml文件，然后建立为次索引，最后合并进主索引。过程很简单，包含两次启动外部进程的工作，这个可以通过erlang中os:cmd完成。</p>

<p>值得注意的是，在从mongodb中读数据时，使用skip基本是不靠谱的，skip 100万个数据需要好几分钟，为了不增加额外的索引字段，我只好在<code>created_at</code>字段上加索引，然后按时间段来读取资源，这一切都是为了支持程序关闭重启后，可以继续上次工作，而不是重头再来。200万的数据，已经处理了好几天了。</p>

<p>后头数据建立好了，需要在前台展示出来。erlang中似乎只有一个sphinx客户端库：<a href="https://github.com/kevsmith/giza">giza</a>。这个库有点老，写成的时候貌似还在使用sphinx0.9版本。其中查询代码包含了版本判定，已经无法在我使用的sphinx2.x版本中使用。无奈之下我只好修改了这个库的源码，幸运的是查询功能居然是正常的，意味着sphinx若干个版本了也没改动通信协议？后来，我为了取得查询的统计信息，例如消耗时间以及总结果，我再一次修改了giza的源码。新的版本可以在我的github上找到：<a href="https://github.com/kevinlynx/giza">my giza</a>，看起来我没侵犯版本协议吧？</p>

<p>目前dhtcrawler的搜索，先是基于sphinx搜索出hash列表，然后再去mongodb中搜索hash对应的资源。事实上，可以为sphinx的document直接附加这些资源的描述信息，就可以避免去数据库查询。但我想，这样会增加sphinx索引文件的大小，担心会影响搜索速度。实际测试时，发现数据库查询有时候还真的很消耗时间，尽管我做了分页，以使得单页仅对数据库进行少量查询。</p>

<h3>xml unicode</h3>

<p>在导入xml到sphinx的索引过程中，本身我输出的内容都是unicode的，但有很多资源会导致indexer解析xml出错。出错后indexer直接停止对当前xml的处理。后来查阅资料发现是因为这些无法被indexer处理的xml内容包含unicode里的控制字符，例如 ä (U+00E4)。我的解决办法是直接过滤掉这些控制字符。unicode的控制字符参看<a href="http://www.utf8-chartable.de/">UTF-8 encoding table and Unicode characters</a>。在erlang中干这个事居然不复杂：</p>

<p><div class="highlight"><pre><code class="language-erlang" data-lang="erlang"><span class="nf">strip_invalid_unicode</span><span class="p">(</span><span class="err">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="err">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">&gt;&gt;</span><span class="p">)</span> <span class="o">-&gt;</span>
    <span class="err">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="err">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="o">&gt;&gt;</span><span class="p">;</span>
<span class="nf">strip_invalid_unicode</span><span class="p">(</span><span class="err">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="err">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="nv">C</span><span class="o">/</span><span class="n">utf8</span><span class="p">,</span> <span class="nv">R</span><span class="o">/</span><span class="n">binary</span><span class="o">&gt;&gt;</span><span class="p">)</span> <span class="o">-&gt;</span>
    <span class="k">case</span> <span class="n">is_valid_unicode</span><span class="err">&amp;</span><span class="n">copy</span><span class="p">;</span> <span class="k">of</span>
        <span class="n">true</span> <span class="o">-&gt;</span>
            <span class="nv">RR</span> <span class="o">=</span> <span class="n">strip_invalid_unicode</span><span class="err">&amp;</span><span class="n">reg</span><span class="p">;,</span>
            <span class="err">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="err">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="nv">C</span><span class="o">/</span><span class="n">utf8</span><span class="p">,</span> <span class="nv">RR</span><span class="o">/</span><span class="n">binary</span><span class="o">&gt;&gt;</span><span class="p">;</span>
        <span class="n">false</span> <span class="o">-&gt;</span>
            <span class="n">strip_invalid_unicode</span><span class="err">&amp;</span><span class="n">reg</span><span class="p">;</span>
    <span class="k">end</span><span class="p">;</span>
<span class="nf">strip_invalid_unicode</span><span class="p">(</span><span class="err">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="err">&amp;</span><span class="n">lt</span><span class="p">;_,</span> <span class="nv">R</span><span class="o">/</span><span class="n">binary</span><span class="o">&gt;&gt;</span><span class="p">)</span> <span class="o">-&gt;</span>
    <span class="n">strip_invalid_unicode</span><span class="err">&amp;</span><span class="n">reg</span><span class="p">;.</span><span class="o">&lt;/</span><span class="n">p</span><span class="o">&gt;</span>

<span class="o">&lt;</span><span class="n">p</span><span class="o">&gt;</span><span class="n">is_valid_unicode</span><span class="err">&amp;</span><span class="n">copy</span><span class="p">;</span> <span class="k">when</span> <span class="nv">C</span> <span class="err">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="mi">16#20</span> <span class="o">-&gt;</span>
    <span class="n">false</span><span class="p">;</span>
<span class="n">is_valid_unicode</span><span class="err">&amp;</span><span class="n">copy</span><span class="p">;</span> <span class="k">when</span> <span class="nv">C</span> <span class="o">&gt;=</span> <span class="mi">16#7f</span><span class="p">,</span> <span class="nv">C</span> <span class="o">=</span><span class="err">&amp;</span><span class="n">lt</span><span class="p">;</span> <span class="mi">16#ff</span> <span class="o">-&gt;</span>
    <span class="n">false</span><span class="p">;</span>
<span class="nf">is_valid_unicode</span><span class="p">(_)</span> <span class="o">-&gt;</span>
    <span class="n">true</span><span class="p">.</span></code></pre></div></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[磁力搜索第二版-dhtcrawler2]]></title>
    <link href="http://codemacro.com/2013/07/02/dhtcrawler2/"/>
    <updated>2013-07-02T00:00:00+08:00</updated>
    <id>http://codemacro.com/2013/07/02/dhtcrawler2</id>
    <content type="html"><![CDATA[<p>接<a href="http://codemacro.com/2013/06/21/magnet-search-impl/">上篇</a>。</p>

<h2>下载使用</h2>

<p>目前为止dhtcrawler2相对dhtcrawler而言，数据库部分调整很大，DHT部分基本沿用之前。但单纯作为一个爬资源的程序而言，DHT部分可以进行大幅削减，这个以后再说。这个版本更快、更稳定。为了方便，我将编译好的erlang二进制文件作为git的主分支，我还添加了一些Windows下的批处理脚本，总之基本上下载源码以后即可运行。</p>

<p>项目地址：<a href="https://github.com/kevinlynx/dhtcrawler2">https://github.com/kevinlynx/dhtcrawler2</a></p>

<h3>使用方法</h3>

<ul>
<li>下载erlang，我测试的是R16B版本，确保erl等程序被加入<code>Path</code>环境变量</li>
<li><p>下载mongodb，解压即用：</p>

<pre><code>  mongod --dbpath xxx --setParameter textSearchEnabled=true
</code></pre></li>
<li><p>下载dhtcrawler2</p>

<pre><code>  git clone https://github.com/kevinlynx/dhtcrawler2.git
</code></pre></li>
<li><p>运行<code>win_start_crawler.bat</code></p></li>
<li>运行<code>win_start_hash.bat</code></li>
<li>运行<code>win_start_http.bat</code></li>
<li>打开<code>localhost:8000</code>查看<code>stats</code></li>
</ul>


<p>爬虫每次运行都会保存DHT节点状态，早期运行的时候收集速度会不够。dhtcrawler2将程序分为3部分：</p>

<ul>
<li>crawler，即DHT爬虫部分，仅负责收集hash</li>
<li>hash，准确来讲叫<code>hash reader</code>，处理爬虫收集的hash，处理过程主要涉及到下载种子文件</li>
<li>http，使用hash处理出来的数据库，以作为Web端接口</li>
</ul>


<p>我没有服务器，但程序有被部署在别人的服务器上：<a href="http://bt.cm">bt.cm</a>，<a href="http://222.175.114.126:8000/">http://222.175.114.126:8000/</a>。</p>

<!-- more -->


<h3>其他工具</h3>

<p>为了提高资源索引速度，我陆续写了一些工具，包括：</p>

<ul>
<li>import_tors，用于导入本地种子文件到数据库</li>
<li>tor_cache，用于下载种子到本地，仅仅提供下载的功能，hash_reader在需要种子文件时，可以先从本地取</li>
<li>cache_indexer，目前hash_reader取种子都是从torrage.com之类的种子缓存站点取，这些站点提供了种子列表，cache_indexer将这些列表导入数据库，hash_reader在请求种子文件前可以通过该数据库检查torrage.com上有无此种子，从而减少多余的http请求</li>
</ul>


<p>这些工具的代码都被放在dhtcrawler2中，可以查看对应的启动脚本来查看具体如何启动。</p>

<h3>OS/Database</h3>

<p>根据实际的测试效果来看，当收集的资源量过百万时（目前bt.cm录入近160万资源），4G内存的Windows平台，mongodb很容易就会挂掉。挂掉的原因全是1455，页面文件太小。有人建议不要在Windows下使用mongodb，Linux下我自己没做过测试。</p>

<p>mongodb可以部署为集群形式(replica-set)，当初我想把http部分的查询放在一个只读的mongodb实例上，但因为建立集群时，要同步已有的10G数据库，而每次同步都以mongodb挂掉结束，遂放弃。在目前bt.cm的配置中，数据库torrent的锁比例（db lock）很容易上50%，这也让http在搜索时，经常出现搜索超时的情况。</p>

<h2>技术信息</h2>

<p>dhtcrawler最早的版本有很多问题，修复过的最大的一个问题是关于erlang定时器的，在DHT实现中，需要对每个节点每个peer做超时处理，在erlang中的做法直接是针对每个节点注册了一个定时器。这不是问题，问题在于定时器资源就像没有GC的内存资源一样，是会由于程序员的代码问题而出现资源泄漏。所以，dhtcrawler第一个版本在节点数配置在100以上的情况下，用不了多久就会内存耗尽，最终导致erlang虚拟机core dump。</p>

<p>除了这个问题以外，dhtcrawler的资源收录速度也不是很快。这当然跟数据库和获取种子的速度有直接关系。尤其是获取种子，使用的是一些提供info-hash到种子映射的网站，通过HTTP请求来下载种子文件。我以为通过BT协议直接下载种子会快些，并且实时性也要高很多，因为这个种子可能未被这些缓存网站收录，但却可以直接向对方请求得到。为此，我还特地翻阅了相关<a href="http://www.bittorrent.org/beps/bep_0009.html">协议</a>，并且用erlang实现了（以后的文章我会讲到具体实现这个协议）。</p>

<p>后来我怀疑get_peers的数量会不会比announce_peer多，但是理论上一般的客户端在get_peers之后都是announce_peer，但是如果get_peers查询的peers恰好不在线呢？这意味着很多资源虽然已经存在，只不过你恰好暂时请求不到。实际测试时，发现get_peers基本是announce_peer数量的10倍。</p>

<p>将hash的获取方式做了调整后，dhtcrawler在几分钟以内以几乎每秒上百个新增种子的速度工作。然后，程序挂掉。</p>

<p>从dhtcrawler到今天为止的dhtcrawler2，中间间隔了刚好1个月。我的所有业余时间全部扑在这个项目上，面临的问题一直都是程序的内存泄漏、资源收录的速度不够快，到后来又变为数据库压力过大。每一天我都以为我将会完成一个稳定版本，然后终于可以去干点别的事情，但总是干不完，目前完没完都还在观察。我始终明白在做优化前需要进行详尽的数据收集和分析，从而真正地优化到正确的点上，但也总是凭直觉和少量数据分析就开始尝试。</p>

<p>这里谈谈遇到的一些问题。</p>

<h3>erlang call timeout</h3>

<p>最开始遇到erlang中<code>gen_server:call</code>出现<code>timeout</code>错误时，我还一直以为是进程死锁了。相关代码读来读去，实在觉得不可能发生死锁。后来发现，当erlang虚拟机压力上去后，例如内存太大，但没大到耗尽系统所有内存（耗进所有内存基本就core dump了），进程间的调用就会出现timeout。</p>

<p>当然，内存占用过大可能只是表象。其进程过多，进程消息队列太长，也许才是导致出现timeout的根本原因。消息队列过长，也可能是由于发生了<em>消息泄漏</em>的缘故。消息泄漏我指的是这样一种情况，进程自己给自己发消息（当然是cast或info），这个消息被处理时又会发送相同的消息，正常情况下，gen_server处理了一个该消息，就会从消息队列里移除它，然后再发送相同的消息，这不会出问题。但是当程序逻辑出问题，每次处理该消息时，都会发生多余一个的同类消息，那消息队列自然就会一直增长。</p>

<p>保持进程逻辑简单，以避免这种逻辑错误。</p>

<h3>erlang gb_trees</h3>

<p>我在不少的地方使用了gb_trees，dht_crawler里就可能出现<code>gb_trees:get(xxx, nil)</code>这种错误。乍一看，我以为我真的传入了一个<code>nil</code>值进去。然后我苦看代码，以为在某个地方我会把这个gb_trees对象改成了nil。但事情不是这样的，gb_tress使用一个tuple作为tree的节点，当某个节点没有子节点时，就会以nil表示。</p>

<p><code>gb_trees:get(xxx, nil)</code>类似的错误，实际指的是<code>xxx</code>没有在这个gb_trees中找到。</p>

<h3>erlang httpc</h3>

<p>dht_crawler通过http协议从torrage.com之类的缓存网站下载种子。最开始我为了尽量少依赖第三方库，使用的是erlang自带的httpc。后来发现程序有内存泄漏，google发现erlang自带的httpc早为人诟病，当然也有大神说在某个版本之后这个httpc已经很不错。为了省事，我直接换了ibrowse，替换之后正常很多。但是由于没有具体分析测试过，加之时间有点远了，我也记不太清细节。因为早期的http请求部分，没有做数量限制，也可能是由于我的使用导致的问题。</p>

<p>某个版本后，我才将http部分严格地与hash处理部分区分开来。相较数据库操作而言，http请求部分慢了若干数量级。在hash_reader中将这两块分开，严格限制了提交给httpc的请求数，以获得稳定性。</p>

<p>对于一个复杂的网络系统而言，分清哪些是耗时的哪些是不大耗时的，才可能获得性能的提升。对于hash_reader而言，处理一个hash的速度，虽然很大程度取决于数据库，但相较http请求，已经快很多。它在处理这些hash时，会将数据库已收录的资源和待下载的资源分离开，以尽快的速度处理已存在的，而将待下载的处理速度交给httpc的响应速度。</p>

<h3>erlang httpc ssl</h3>

<p>ibrowse处理https请求时，默认和erlang自带的httpc使用相同的SSL实现。这经常导致出现<code>tls_connection</code>进程挂掉的错误，具体原因不明。</p>

<h3>erlang调试</h3>

<p>首先合理的日志是任何系统调试的必备。</p>

<p>我面临的大部分问题都是内存泄漏相关，所以依赖的erlang工具也是和内存相关的：</p>

<ul>
<li><p>使用<code>etop</code>，可以检查内存占用多的进程、消息队列大的进程、CPU消耗多的进程等等：</p>

<pre><code>  spawn(fun() -&gt; etop:start([{output, text}, {interval, 10}, {lines, 20}, {sort, msg_q }]) end).
</code></pre></li>
<li><p>使用<code>erlang:system_info(allocated_areas).</code>检查内存使用情况，其中会输出系统<code>timer</code>数量</p></li>
<li>使用<code>erlang:process_info</code>查看某个具体的进程，这个甚至会输出消息队列里的消息</li>
</ul>


<h3>hash_writer/crawler</h3>

<p>crawler本身仅收集hash，然后写入数据库，所以可以称crawler为hash_writer。这些hash里存在大量的重复。hash_reader从数据库里取出这些hash然后做处理。处理过程会首先判定该hash对应的资源是否被收录，没有收录就先通过http获取种子。</p>

<p>在某个版本之后，crawler会简单地预先处理这些hash。它缓存一定数量的hash，接收到新hash时，就合并到hash缓存里，以保证缓存里没有重复的hash。这个重复率经过实际数据分析，大概是50%左右，即收到的100个请求里，有50个是重复的。这样的优化，不仅会降低hash数据库的压力，hash_reader处理的hash数量少了，也会对torrent数据库有很大提升。</p>

<p>当然进一步的方案可以将crawler和hash_reader之间交互的这些hash直接放在内存中处理，省去中间数据库。但是由于mongodb大量使用虚拟内存的缘故（内存映射文件），经常导致服务器内存不够（4G），内存也就成了珍稀资源。当然这个方案还有个弊端是难以权衡hash缓存的管理。crawler收到hash是一个不稳定的过程，在某些时间点这些hash可能爆多，而hash_reader处理hash的速度也会不太稳定，受限于收到的hash类别（是新增资源还是已存在资源）、种子请求速度、是否有效等。</p>

<p>当然，也可以限制缓存大小，以及对hash_reader/crawler处理速度建立关系来解决这些问题。但另一方面，这里的优化是否对目前的系统有提升，是否是目前系统面临的最大问题，却是需要考究的事情。</p>

<h3>cache indexer</h3>

<p>dht_crawler是从torrage.com等网站获取种子文件，这些网站看起来都是使用了相同的接口，其都有一个sync目录，里面存放了每天每个月索引的种子hash，例如 <a href="http://torrage.com/sync/%E3%80%82%E8%BF%99%E4%B8%AA%E7%BD%91%E7%AB%99%E4%B8%8A%E6%98%AF%E5%90%A6%E6%9C%89%E6%9F%90%E4%B8%AAhash%E5%AF%B9%E5%BA%94%E7%9A%84%E7%A7%8D%E5%AD%90%EF%BC%8C%E5%B0%B1%E5%8F%AF%E4%BB%A5%E4%BB%8E%E8%BF%99%E4%BA%9B%E7%B4%A2%E5%BC%95%E4%B8%AD%E6%A3%80%E6%9F%A5%E3%80%82">http://torrage.com/sync/%E3%80%82%E8%BF%99%E4%B8%AA%E7%BD%91%E7%AB%99%E4%B8%8A%E6%98%AF%E5%90%A6%E6%9C%89%E6%9F%90%E4%B8%AAhash%E5%AF%B9%E5%BA%94%E7%9A%84%E7%A7%8D%E5%AD%90%EF%BC%8C%E5%B0%B1%E5%8F%AF%E4%BB%A5%E4%BB%8E%E8%BF%99%E4%BA%9B%E7%B4%A2%E5%BC%95%E4%B8%AD%E6%A3%80%E6%9F%A5%E3%80%82</a></p>

<p>hash_reader在处理新资源时，请求种子的过程中发现大部分在这些服务器上都没有找到，也就是发起的很多http请求都是404回应，这不但降低了系统的处理能力、带宽，也降低了索引速度。所以我写了一个工具，先手工将sync目录下的所有文件下载到本地，然后通过这个工具 (cache indexer) 将这些索引文件里的hash全部导入数据库。在以后的运行过程中，该工具仅下载当天的索引文件，以更新数据库。 hash_reader 根据配置，会首先检查某个hash是否存在该数据库中，存在的hash才可能在torrage.com上下载得到。</p>

<h3>种子缓存</h3>

<p>hash_reader可以通过配置，将下载得到的种子保存在本地文件系统或数据库中。这可以建立自己的种子缓存，但保存在数据库中会对数据库造成压力，尤其在当前测试服务器硬件环境下；而保存为本地文件，又特别占用硬盘空间。</p>

<h3>基于BT协议的种子下载</h3>

<p>通过http从种子缓存里取种子文件，可能会没有直接从P2P网络里取更实时。目前还没来得及查看这些种子缓存网站的实现原理。但是通过BT协议获取种子会有点麻烦，因为dht_crawler是根据<code>get_peer</code>请求索引资源的，所以如果要通过BT协议取种子，那么这里还得去DHT网络里查询该种子，这个查询过程可能会较长，相比之下会没有http下载快。而如果通过<code>announce_peer</code>来索引新资源的话，其索引速度会大大降低，因为<code>announce_peer</code>请求比<code>get_peer</code>请求少很多，几乎10倍。</p>

<p>所以，这里的方案可能会结合两者，新开一个服务，建立自己的种子缓存。</p>

<h3>中文分词</h3>

<p>mongodb的全文索引是不支持中文的。我在之前提到，为了支持搜索中文，我将字符串拆成了若干子串。这样的后果就是字符串索引会稍稍偏大，而且目前这一块的代码还特别简单，会将很多非文字字符也算在内。后来我加了个中文分词库，使用的是rmmseg-cpp。我将其C++部分抽离出来编译成erlang nif，这可以在我的github上找到。</p>

<p>但是这个库拆分中文句子依赖于词库，而这个词库不太新，dhtcrawler爬到的大部分资源类型你们也懂，那些词汇拆出来的比率不太高，这会导致搜索出来的结果没你想的那么直白。当然更新词库应该是可以解决这个问题的，目前还没有时间顾这一块。</p>

<h2>总结</h2>

<p>一个老外对我说过，&#8221;i have 2 children to feed, so i will not do this only for fun&#8221;。</p>

<p>你的大部分编程知识来源于网络，所以稍稍回馈一下不会让你丢了饭碗。</p>

<p>我很穷，如果你能让我收获金钱和编程成就，还不会嫌我穿得太邋遢，that&rsquo;s really kind of you。</p>
]]></content>
  </entry>
  
</feed>
