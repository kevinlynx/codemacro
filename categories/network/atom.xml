<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: network | loop in codes]]></title>
  <link href="http://codemacro.com/categories/network/atom.xml" rel="self"/>
  <link href="http://codemacro.com/"/>
  <updated>2014-10-15T22:43:59+08:00</updated>
  <id>http://codemacro.com/</id>
  <author>
    <name><![CDATA[Kevin Lynx]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[图解分布式一致性协议Paxos]]></title>
    <link href="http://codemacro.com/2014/10/15/explain-poxos/"/>
    <updated>2014-10-15T00:00:00+08:00</updated>
    <id>http://codemacro.com/2014/10/15/explain-poxos</id>
    <content type="html"><![CDATA[<p>Paxos协议/算法是分布式系统中比较重要的协议，它有多重要呢？</p>

<p><a href="http://coolshell.cn/articles/10910.html">&lt;分布式系统的事务处理></a>：</p>

<blockquote><p>Google Chubby的作者Mike Burrows说过这个世界上只有一种一致性算法，那就是Paxos，其它的算法都是残次品。</p></blockquote>

<p><a href="http://book.douban.com/subject/25723658/">&lt;大规模分布式存储系统></a>：</p>

<blockquote><p>理解了这两个分布式协议之后(Paxos/2PC)，学习其他分布式协议会变得相当容易。</p></blockquote>

<p>学习Paxos算法有两部分：a) 算法的原理/证明；b) 算法的理解/运作。</p>

<p>理解这个算法的运作过程其实基本就可以用于工程实践。而且理解这个过程相对来说也容易得多。</p>

<p>网上我觉得讲Paxos讲的好的属于这篇：<a href="http://coderxy.com/archives/121">paxos图解</a>及<a href="http://coderxy.com/archives/136">Paxos算法详解</a>，我这里就结合<a href="http://zh.wikipedia.org/zh-cn/Paxos%E7%AE%97%E6%B3%95#.E5.AE.9E.E4.BE.8B">wiki上的实例</a>进一步阐述。一些paxos基础通过这里提到的两篇文章，以及wiki上的内容基本可以理解。</p>

<!-- more -->


<h2>算法内容</h2>

<p>Paxos在原作者的《Paxos Made Simple》中内容是比较精简的：</p>

<blockquote><p>Phase  1</p>

<p>  (a) A proposer selects a proposal number n  and sends a prepare request with number  n to a majority of acceptors.</p>

<p>  (b)  If  an  acceptor  receives  a prepare  request  with  number  n  greater than  that  of  any  prepare  request  to  which  it  has  already  responded, then it responds to the request with a promise not to accept any more proposals numbered less than  n  and with the highest-numbered pro-posal (if any) that it has accepted.</p>

<p>  Phase  2</p>

<p>  (a)  If  the  proposer  receives  a  response  to  its  prepare requests (numbered  n)  from  a  majority  of  acceptors,  then  it  sends  an  accept request to each of those acceptors for a proposal numbered  n  with a value v , where v is the value of the highest-numbered proposal among the responses, or is any value if the responses reported no proposals.</p>

<p>  (b) If an acceptor receives an accept request for a proposal numbered n, it accepts the proposal unless it has already responded to a prepare request having a number greater than  n.</p></blockquote>

<p>借用<a href="http://coderxy.com/archives/121">paxos图解</a>文中的流程图可概括为：</p>

<p><img src="/assets/res/paxos/paxos-flow.png" alt="" /></p>

<h2>实例及详解</h2>

<p>Paxos中有三类角色<code>Proposer</code>、<code>Acceptor</code>及<code>Learner</code>，主要交互过程在<code>Proposer</code>和<code>Acceptor</code>之间。</p>

<p><code>Proposer</code>与<code>Acceptor</code>之间的交互主要有4类消息通信，如下图：</p>

<p><img src="/assets/res/paxos/paxos-messages.png" alt="" /></p>

<p>这4类消息对应于paxos算法的两个阶段4个过程：</p>

<ul>
<li>phase 1

<ul>
<li>a) proposer向网络内超过半数的acceptor发送prepare消息</li>
<li>b) acceptor正常情况下回复promise消息</li>
</ul>
</li>
<li>phase 2

<ul>
<li>a) 在有足够多acceptor回复promise消息时，proposer发送accept消息</li>
<li>b) 正常情况下acceptor回复accepted消息</li>
</ul>
</li>
</ul>


<p>因为在整个过程中可能有其他proposer针对同一件事情发出以上请求，所以在每个过程中都会有些特殊情况处理，这也是为了达成一致性所做的事情。如果在整个过程中没有其他proposer来竞争，那么这个操作的结果就是确定无异议的。但是如果有其他proposer的话，情况就不一样了。</p>

<p>以<a href="http://zh.wikipedia.org/zh-cn/Paxos%E7%AE%97%E6%B3%95#.E5.AE.9E.E4.BE.8B">paxos中文wiki上的例子</a>为例。简单来说该例子以若干个议员提议税收，确定最终通过的法案税收比例。</p>

<p>以下图中基本只画出proposer与一个acceptor的交互。时间标志T2总是在T1后面。propose number简称N。</p>

<p>情况之一如下图：</p>

<p><img src="/assets/res/paxos/paxos-e1.png" alt="" /></p>

<p>A3在T1发出accepted给A1，然后在T2收到A5的prepare，在T3的时候A1才通知A5最终结果(税率10%)。这里会有两种情况：</p>

<ul>
<li>A5发来的N5小于A1发出去的N1，那么A3直接拒绝(reject)A5</li>
<li>A5发来的N5大于A1发出去的N1，那么A3回复promise，但带上A1的(N1, 10%)</li>
</ul>


<p>这里可以与paxos流程图对应起来，更好理解。<strong>acceptor会记录(MaxN, AcceptN, AcceptV)</strong>。</p>

<p>A5在收到promise后，后续的流程可以顺利进行。但是发出accept时，因为收到了(AcceptN, AcceptV)，所以会取最大的AcceptN对应的AcceptV，例子中也就是A1的10%作为AcceptV。如果在收到promise时没有发现有其他已记录的AcceptV，则其值可以由自己决定。</p>

<p>针对以上A1和A5冲突的情况，最终A1和A5都会广播接受的值为10%。</p>

<p>其实4个过程中对于acceptor而言，在回复promise和accepted时由于都可能因为其他proposer的介入而导致特殊处理。所以基本上看在这两个时间点收到其他proposer的请求时就可以了解整个算法了。例如在回复promise时则可能因为proposer发来的N不够大而reject：</p>

<p><img src="/assets/res/paxos/paxos-e2.png" alt="" /></p>

<p>如果在发accepted消息时，对其他更大N的proposer发出过promise，那么也会reject该proposer发出的accept，如图：</p>

<p><img src="/assets/res/paxos/paxos-e3.png" alt="" /></p>

<p>这个对应于Phase 2 b)：</p>

<blockquote><p>it accepts the proposal unless it has already responded to a prepare request having a number greater than  n.</p></blockquote>

<h2>总结</h2>

<p>Leslie Lamport没有用数学描述Paxos，但是他用英文阐述得很清晰。将Paxos的两个Phase的内容理解清楚，整个算法过程还是不复杂的。</p>

<p>至于Paxos中一直提到的一个全局唯一且递增的proposer number，其如何实现，引用如下：</p>

<blockquote><p>如何产生唯一的编号呢？在《Paxos made simple》中提到的是让所有的Proposer都从不相交的数据集合中进行选择，例如系统有5个Proposer，则可为每一个Proposer分配一个标识j(0~4)，则每一个proposer每次提出决议的编号可以为5*i + j(i可以用来表示提出议案的次数)</p></blockquote>

<h2>参考文档</h2>

<ul>
<li>paxos图解, <a href="http://coderxy.com/archives/121">http://coderxy.com/archives/121</a></li>
<li>Paxos算法详解, <a href="http://coderxy.com/archives/136">http://coderxy.com/archives/136</a></li>
<li>Paxos算法 wiki, <a href="http://zh.wikipedia.org/zh-cn/Paxos%E7%AE%97%E6%B3%95#.E5.AE.9E.E4.BE.8B">http://zh.wikipedia.org/zh-cn/Paxos%E7%AE%97%E6%B3%95#.E5.AE.9E.E4.BE.8B</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[淘宝分布式配置管理服务Diamond]]></title>
    <link href="http://codemacro.com/2014/10/12/diamond/"/>
    <updated>2014-10-12T00:00:00+08:00</updated>
    <id>http://codemacro.com/2014/10/12/diamond</id>
    <content type="html"><![CDATA[<p>在一个分布式环境中，同类型的服务往往会部署很多实例。这些实例使用了一些配置，为了更好地维护这些配置就产生了配置管理服务。通过这个服务可以轻松地管理这些应用服务的配置问题。应用场景可概括为：</p>

<p><img src="/assets/res/diamond/disconf.PNG" alt="" /></p>

<p>zookeeper的一种应用就是分布式配置管理(<a href="http://wenku.baidu.com/view/ee86ca90daef5ef7ba0d3c7d.html">基于ZooKeeper的配置信息存储方案的设计与实现</a>)。百度也有类似的实现：<a href="https://github.com/knightliao/disconf">disconf</a>。</p>

<p><a href="http://code.taobao.org/p/diamond/src/">Diamond</a>则是淘宝开源的一种分布式配置管理服务的实现。Diamond本质上是一个Java写的Web应用，其对外提供接口都是基于HTTP协议的，在阅读代码时可以从实现各个接口的controller入手。</p>

<h2>分布式配置管理</h2>

<p>分布式配置管理的本质基本上就是一种<strong><a href="http://en.wikipedia.org/wiki/Publish%E2%80%93subscribe_pattern">推送-订阅</a></strong>模式的运用。配置的应用方是订阅者，配置管理服务则是推送方。概括为下图：</p>

<p><img src="/assets/res/diamond/pubsub.PNG" alt="" /></p>

<p>其中，客户端包括管理人员publish数据到配置管理服务，可以理解为添加/更新数据；配置管理服务notify数据到订阅者，可以理解为推送。</p>

<!-- more -->


<p>配置管理服务往往会封装一个客户端库，应用方则是基于该库与配置管理服务进行交互。在实际实现时，客户端库可能是主动拉取(pull)数据，但对于应用方而言，一般是一种事件通知方式。</p>

<p>Diamond中的数据是简单的key-value结构。应用方订阅数据则是基于key来订阅，未订阅的数据当然不会被推送。数据从类型上又划分为聚合和非聚合。因为数据推送者可能很多，在整个分布式环境中，可能有多个推送者在推送相同key的数据，这些数据如果是聚合的，那么所有这些推送者推送的数据会被合并在一起；反之如果是非聚合的，则会出现覆盖。</p>

<p>数据的来源可能是人工通过管理端录入，也可能是其他服务通过配置管理服务的推送接口自动录入。</p>

<h2>架构及实现</h2>

<p>Diamond服务是一个集群，是一个去除了单点的协作集群。如图：</p>

<p><img src="/assets/res/diamond/arch.PNG" alt="" /></p>

<p>图中可分为以下部分讲解：</p>

<h3>服务之间同步</h3>

<p>Diamond服务集群每一个实例都可以对外完整地提供服务，那么意味着每个实例上都有整个集群维护的数据。Diamond有两种方式保证这一点：</p>

<ul>
<li>任何一个实例都有其他实例的地址；任何一个实例上的数据变更时，都会将改变的数据同步到mysql上，然后通知其他所有实例从mysql上进行一次数据拉取(<code>DumpService::dump</code>)，这个过程只拉取改变了的数据</li>
<li>任何一个实例启动后都会以较长的时间间隔（几小时），从mysql进行一次全量的数据拉取(<code>DumpAllProcessor</code>)</li>
</ul>


<p>实现上为了一致性，通知其他实例实际上也包含自己。以服务器收到添加聚合数据为例，处理过程大致为：</p>

<pre><code>DatumController::addDatum // /datum.do?method=addDatum
    PersistService::addAggrConfigInfo 
    MergeDatumService::addMergeTask // 添加一个MergeDataTask，异步处理

MergeTaskProcessor::process
    PersistService::insertOrUpdate
        EventDispatcher.fireEvent(new ConfigDataChangeEvent // 派发一个ConfigDataChangeEvent事件

NotifyService::onEvent // 接收事件并处理
    TaskManager::addTask(..., new NotifyTask // 由此，当数据发生变动，则最终创建了一个NoticyTask

// NotifyTask同样异步处理
NotifyTaskProcessor::process
    foreach server in serverList // 包含自己
        notifyToDump // 调用 /notify.do?method=notifyConfigInfo 从mysql更新变动的数据
</code></pre>

<p>虽然Diamond去除了单点问题，不过问题都下降到了mysql上。但由于其作为配置管理的定位，其数据量就mysql的应用而言算小的了，所以可以一定程度上保证整个服务的可用性。</p>

<h3>数据一致性</h3>

<p>由于Diamond服务器没有master，任何一个实例都可以读写数据，那么针对同一个key的数据则可能面临冲突。这里应该是通过mysql来保证数据的一致性。每一次客户端请求写数据时，Diamond都将写请求投递给mysql，然后通知集群内所有Diamond实例（包括自己）从mysql拉取数据。当然，拉取数据则可能不是每一次写入都能拉出来，也就是最终一致性。</p>

<p>Diamond中没有把数据放入内存，但会放到本地文件。对于客户端的读操作而言，则是直接返回本地文件里的数据。</p>

<h3>服务实例列表</h3>

<p>Diamond服务实例列表是一份静态数据，直接将每个实例的地址存放在一个web server上。无论是Diamond服务还是客户端都从该web server上取出实例列表。</p>

<p>对于客户端而言，当其取出了该列表后，则是随机选择一个节点(<code>ServerListManager.java</code>)，以后的请求都会发往该节点。</p>

<h3>数据同步</h3>

<p>客户端库中以固定时间间隔从服务器拉取数据(<code>ClientWorker::ClientWorker</code>，<code>ClientWorker::checkServerConfigInfo</code>)。只有应用方关心的数据才可能被拉取。另外，为了数据推送的及时，Diamond还使用了一种long polling的技术，其实也是为了突破HTTP协议的局限性。<em>如果整个服务是基于TCP的自定义协议，客户端与服务器保持长连接则没有这些问题</em>。</p>

<h3>数据的变更</h3>

<p>Diamond中很多操作都会检查数据是否发生了变化。标识数据变化则是基于数据对应的MD5值来实现的。</p>

<h2>容灾</h2>

<p>在整个Diamond系统中，几个角色为了提高容灾性，都有自己的缓存，概括为下图：</p>

<p><img src="/assets/res/diamond/failover.PNG" alt="" /></p>

<p>每一个角色出问题时，都可以尽量保证客户端对应用层提供服务。</p>

<h2>参考文档</h2>

<ul>
<li><a href="http://code.taobao.org/p/diamond/src">diamond project</a></li>
<li><a href="http://jm-blog.aliapp.com/?p=1588">diamond专题</a></li>
<li><a href="http://jm-blog.aliapp.com/?p=3450">中间件技术及双十一实践·软负载篇</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[zookeeper节点数与watch的性能测试]]></title>
    <link href="http://codemacro.com/2014/09/21/zk-watch-benchmark/"/>
    <updated>2014-09-21T00:00:00+08:00</updated>
    <id>http://codemacro.com/2014/09/21/zk-watch-benchmark</id>
    <content type="html"><![CDATA[<p>zookeeper中节点数量理论上仅受限于内存，但一个节点下的子节点数量<a href="http://zookeeper-user.578899.n2.nabble.com/ZooKeeper-Limitation-td6675643.html">受限于request/response 1M数据</a> (<a href="http://web.archiveorange.com/archive/v/AQXskdBodZB7kWpjpjHw">size of data / number of znodes</a>)</p>

<p>zookeeper的watch机制用于数据变更时zookeeper的主动通知。watch可以被附加到每一个节点上，那么如果一个应用有10W个节点，那zookeeper中就可能有10W个watch（甚至更多）。每一次在zookeeper完成改写节点的操作时就会检测是否有对应的watch，有的话则会通知到watch。<a href="http://shift-alt-ctrl.iteye.com/blog/1847320">Zookeeper-Watcher机制与异步调用原理</a></p>

<p>本文将关注以下内容：</p>

<ul>
<li>zookeeper的性能是否会受节点数量的影响</li>
<li>zookeeper的性能是否会受watch数量的影响</li>
</ul>


<h2>测试方法</h2>

<p>在3台机器上分别部署一个zookeeper，版本为<code>3.4.3</code>，机器配置：</p>

<pre><code>Intel(R) Xeon(R) CPU E5-2430 0 @ 2.20GHz

16G

java version "1.6.0_32"
Java(TM) SE Runtime Environment (build 1.6.0_32-b05)
OpenJDK (Taobao) 64-Bit Server VM (build 20.0-b12-internal, mixed mode)
</code></pre>

<p>大部分实验JVM堆大小使用默认，也就是<code>1/4 RAM</code>：</p>

<pre><code>java -XX:+PrintFlagsFinal -version | grep HeapSize
</code></pre>

<p>测试客户端使用<a href="https://github.com/phunt/zk-smoketest">zk-smoketest</a>，针对watch的测试则是我自己写的。基于zk-smoketest我写了些脚本可以自动跑数据并提取结果，相关脚本可以在这里找到：<a href="https://github.com/kevinlynx/zk-benchmark">https://github.com/kevinlynx/zk-benchmark</a></p>

<!-- more -->


<h2>测试结果</h2>

<h3>节点数对读写性能的影响</h3>

<p>测试最大10W个节点，度量1秒内操作数(ops)：</p>

<p><img src="/assets/res/zk_benchmark/node_count.png" alt="" /></p>

<p>可见节点数的增加并不会对zookeeper读写性能造成影响。</p>

<h3>节点数据大小对读写性能的影响</h3>

<p>这个网上其实已经有公认的结论。本身单个节点数据越大，对网络方面的吞吐就会造成影响，所以其数据越大读写性能越低也在预料之中。</p>

<p><img src="/assets/res/zk_benchmark/node_size.png" alt="" /></p>

<p>写数据会在zookeeper集群内进行同步，所以其速度整体会比读数据更慢。该实验需要把超时时间进行一定上调，同时我也把JVM最大堆大小调整到8G。</p>

<p>测试结果很明显，节点数据大小会严重影响zookeeper效率。</p>

<h2>watch对读写性能的影响</h2>

<p>zk-smoketest自带的latency测试有个参数<code>--watch_multiple</code>用来指定watch的数量，但其实仅是指定客户端的数量，在server端通过<code>echo whcp | nc 127.0.0.1 4181</code>会发现实际每个节点还是只有一个watch。</p>

<p>在我写的测试中，则是通过创建多个客户端来模拟单个节点上的多个watch。这也更符合实际应用。同时对节点的写也是在另一个独立的客户端中，这样可以避免zookeeper client的实现对测试带来的干扰。</p>

<p>每一次完整的测试，首先是对每个节点添加节点数据的watch，然后在另一个客户端中对这些节点进行数据改写，收集这些改写操作的耗时，以确定添加的watch对这些写操作带来了多大的影响。</p>

<p><img src="/assets/res/zk_benchmark/watch.png" alt="" /></p>

<p>图中，<code>0 watch</code>表示没有对节点添加watch；<code>1 watch</code>表示有一个客户端对每个节点进行了watch；<code>3 watch</code>表示有其他3个客户端对每个节点进行了watch；依次类推。</p>

<p>可见，watch对写操作还是有较大影响的，毕竟需要进行网络传输。同样，这里也显示出整个zookeeper的watch数量同节点数量一样对整体性能没有影响。</p>

<h2>总体结论</h2>

<ul>
<li>对单个节点的操作并不会因为zookeeper中节点的总数而受到影响</li>
<li>数据大小对zookeeper的性能有较大影响，性能和内存都会</li>
<li>单个节点上独立session的watch数对性能有一定影响</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[分布式环境中的负载均衡策略]]></title>
    <link href="http://codemacro.com/2014/08/25/lb-policy/"/>
    <updated>2014-08-25T00:00:00+08:00</updated>
    <id>http://codemacro.com/2014/08/25/lb-policy</id>
    <content type="html"><![CDATA[<p>在分布式系统中相同的服务常常会部署很多台，每一台被称为一个服务节点（实例）。通过一些负载均衡策略将服务请求均匀地分布到各个节点，以实现整个系统支撑海量请求的需求。本文描述一些简单的负载均衡策略。</p>

<h2>Round-robin</h2>

<p>简单地轮询。记录一个选择位置，每次请求来时调整该位置到下一个节点：</p>

<pre><code>curId = ++curId % nodeCnt
</code></pre>

<h2>随机选择</h2>

<p>随机地在所有节点中选择：</p>

<pre><code>id = random(nodeCnt);
</code></pre>

<h2>本机优先</h2>

<p>访问后台服务的访问者可能本身是一个整合服务，或者是一个proxy，如果后台服务节点恰好有节点部署在本机的，则可以优先使用。在未找到本机节点时则可以继续走Round-robin策略：</p>

<pre><code>if (node-&gt;ip() == local_ip) {
    return node;
} else {
    return roundRobin();
}
</code></pre>

<!-- more -->


<p>一旦遍历到本机节点，则后面的请求会一直落到本机节点。所以这里可以加上一些权重机制，仅是保证本机节点会被优先选择，但不会被一直选择。例如：</p>

<pre><code>// initial
cur_weight = 100;
...
// select node
cur_weight -= 5;
if (cur_weight &lt;= 0)
    cur_weight = 100;
if (cur_weight &gt; 50 &amp;&amp; node-&gt;ip() == local_ip) {
    return node;
} else {
    return roundRobin();
}
</code></pre>

<h2>本机房优先</h2>

<p>服务节点可能会被部署到多个机房，有时候确实是需要考虑跨机房服务。同<code>本机优先</code>策略类似，本机房优先则是优先考虑位于相同机房内的服务节点。该请求是从哪个机房中的前端服务发送过来的，则需要前端在请求参数中携带上机房ID。</p>

<p>在服务节点对应的数据结构中，也最好按照机房来组织。</p>

<p>本机房优先策略实际上会作为节点选择的第一道工序，它可以把非本机房的节点先过滤掉，然后再传入后面的各种节点选择策略。这里还可以考虑节点数参数，如果本机房的节点过少，则可以不使用该策略，避免流量严重不均。</p>

<h2>Weighted Round-Robin</h2>

<p>加权轮询。相对于普通轮询而言，该策略中每一个节点都有自己的权重，优先选择权重更大的节点。权重可以根据机器性能预先配置。摘抄一下网上的算法：</p>

<pre><code>假设有一组服务器S = {S0, S1, …, Sn-1}，W(Si)表示服务器Si的权值，一个
指示变量i表示上一次选择的服务器，指示变量cw表示当前调度的权值，max(S)
表示集合S中所有服务器的最大权值，gcd(S)表示集合S中所有服务器权值的最大
公约数。变量i初始化为-1，cw初始化为零。

while (true) {
  i = (i + 1) mod n;
  if (i == 0) {
     cw = cw - gcd(S); 
     if (cw &lt;= 0) {
       cw = max(S);
       if (cw == 0)
         return NULL;
     }
  } 
  if (W(Si) &gt;= cw) 
    return Si;
}
</code></pre>

<p>遍历完所有节点后权重衰减，衰减到0后重新开始。这样可以让权重更大的节点被选择得更多。</p>

<h2>Consistent Hash</h2>

<p>一致性哈希。一致性哈希用于在分布式环境中，分布在各个节点上的请求，不会因为新增节点（扩容）或减少节点（节点宕机）而变化。如果每个服务节点上都有自己的缓存，其保存了该节点响应请求时的回应。正常情况下，这些缓存都可以很好地被运用，也即cache命中率较高。</p>

<p>如果某个节点不可用了，我们的选择策略又是基于所有节点的公平选择，那么原来一直分配在节点A上请求就很可能被分配到节点B上，从而导致节点A上的缓存较难被命中。这个时候就可以运用一致性哈希来解决。</p>

<p>其基本思想是，在节点选择区间内，在找节点时以顺时针方向找到不小于该请求对应的哈希值的节点。在这个区间里增加很多虚拟节点，每一个虚拟节点相当于一个物理节点的引用，这样相当于把物理节点变成了一个哈希值区间。这个哈希值区间不会因为增加节点和减少节点而变化，那么对某个请求而言，它就会始终落到这个区间里，也就会始终被分配到原来的节点。</p>

<p>至于这个不可用的节点，其上的请求也会被均匀地分配到其他节点中。</p>

<p>摘抄网上的一段代码：</p>

<pre><code>// 添加一个物理节点时，会随之增加很多虚拟节点
template &lt;class Node, class Data, class Hash&gt;
size_t HashRing&lt;Node, Data, Hash&gt;::AddNode(const Node&amp; node)
{
    size_t hash;
    std::string nodestr = Stringify(node);
    for (unsigned int r = 0; r &lt; replicas_; r++) {
        hash = hash_((nodestr + Stringify(r)).c_str());
        ring_[hash] = node;  // 物理节点和虚拟节点都保存在一个std::map中
    }
    return hash;
}

// 选择data对应的节点，data可以是请求
template &lt;class Node, class Data, class Hash&gt;
const Node&amp; HashRing&lt;Node, Data, Hash&gt;::GetNode(const Data&amp; data) const
{
    if (ring_.empty()) {
        throw EmptyRingException();
    }
    size_t hash = hash_(Stringify(data).c_str()); // 对请求进行哈希
    typename NodeMap::const_iterator it;
    // Look for the first node &gt;= hash
    it = ring_.lower_bound(hash); // 找到第一个不小于请求哈希的节点
    if (it == ring_.end()) {
        // Wrapped around; get the first node
        it = ring_.begin();
    }
    return it-&gt;second;
}
</code></pre>

<p>参考<a href="http://blog.csdn.net/sparkliang/article/details/5279393">一致性 hash 算法(consistent hashing)</a>，<a href="http://www.martinbroadhurst.com/Consistent-Hash-Ring.html">Consistent Hash Ring</a></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[select真的有限制吗]]></title>
    <link href="http://codemacro.com/2014/06/01/select-limit/"/>
    <updated>2014-06-01T00:00:00+08:00</updated>
    <id>http://codemacro.com/2014/06/01/select-limit</id>
    <content type="html"><![CDATA[<p>在刚开始学习网络编程时，似乎莫名其妙地就会被某人/某资料告诉<code>select</code>函数是有fd(file descriptor)数量限制的。在最近的一次记忆里还有个人笑说<code>select</code>只支持64个fd。我甚至还写过一篇不负责任甚至错误的博客(<a href="http://www.cppblog.com/kevinlynx/archive/2008/05/20/50500.html">突破select的FD_SETSIZE限制</a>)。有人说，直接重新定义<code>FD_SETSIZE</code>就可以突破这个<code>select</code>的限制，也有人说除了重定义这个宏之外还的重新编译内核。</p>

<p>事实具体是怎样的？实际上，造成这些混乱的原因恰好是不同平台对<code>select</code>的实现不一样。</p>

<h2>Windows的实现</h2>

<p><a href="http://msdn.microsoft.com/en-us/library/windows/desktop/ms740141(v=vs.85">MSDN</a>.aspx)上对<code>select</code>的说明：</p>

<pre><code>int select(
  _In_     int nfds,
  _Inout_  fd_set *readfds,
  _Inout_  fd_set *writefds,
  _Inout_  fd_set *exceptfds,
  _In_     const struct timeval *timeout
);

nfds [in] Ignored. The nfds parameter is included only for compatibility with Berkeley sockets.
</code></pre>

<p>第一个参数MSDN只说没有使用，其存在仅仅是为了保持与Berkeley Socket的兼容。</p>

<blockquote><p>The variable FD_SETSIZE determines the maximum number of descriptors in a set. (The default value of FD_SETSIZE is 64, which can be modified by defining FD_SETSIZE to another value before including Winsock2.h.) Internally, socket handles in an fd_set structure are not represented as bit flags as in Berkeley Unix.</p></blockquote>

<p>Windows上<code>select</code>的实现不同于Berkeley Unix，<strong>后者使用位标志来表示socket</strong>。</p>

<!-- more -->


<p>在MSDN的评论中有人提到：</p>

<blockquote><p>Unlike the Linux versions of these macros which use a single calculation to set/check the fd, the Winsock versions use a loop which goes through the entire set of fds each time you call FD_SET or FD_ISSET (check out winsock2.h and you&rsquo;ll see). So you might want to consider an alternative if you have thousands of sockets!</p></blockquote>

<p>不同于Linux下处理<code>fd_set</code>的那些宏(FD_CLR/FD_SET之类)，Windows上这些宏的实现都使用了一个循环，看看这些宏的大致实现(Winsock2.h)：</p>

<pre><code>#define FD_SET(fd, set) do { \
    u_int __i; \
    for (__i = 0; __i &lt; ((fd_set FAR *)(set))-&gt;fd_count; __i++) { \
        if (((fd_set FAR *)(set))-&gt;fd_array[__i] == (fd)) { \
            break; \
        } \
    } \
    if (__i == ((fd_set FAR *)(set))-&gt;fd_count) { \
        if (((fd_set FAR *)(set))-&gt;fd_count &lt; FD_SETSIZE) { \
            ((fd_set FAR *)(set))-&gt;fd_array[__i] = (fd); \
            ((fd_set FAR *)(set))-&gt;fd_count++; \
        } \
    } \
} while(0)
</code></pre>

<p>看下Winsock2.h中关于<code>fd_set</code>的定义：</p>

<pre><code>typedef struct fd_set {
    u_int fd_count;
    SOCKET fd_array[FD_SETSIZE];
} fd_set;
</code></pre>

<p>再看一篇更重要的MSDN <a href="http://msdn.microsoft.com/en-us/library/windows/desktop/ms739169(v=vs.85">Maximum Number of Sockets Supported</a>.aspx)：</p>

<blockquote><p>The Microsoft Winsock provider limits the maximum number of sockets supported only by available memory on the local computer.
The maximum number of sockets that a Windows Sockets application can use is not affected by the manifest constant FD_SETSIZE.
If an application is designed to be capable of working with more than 64 sockets using the select and WSAPoll functions, the implementor should define the manifest FD_SETSIZE in every source file before including the Winsock2.h header file.</p></blockquote>

<p>Windows上<code>select</code>支持的socket数量并不受宏<code>FD_SETSIZE</code>的影响，而仅仅受内存的影响。如果应用程序想使用超过<code>FD_SETSIZE</code>的socket，仅需要重新定义<code>FD_SETSIZE</code>即可。</p>

<p>实际上稍微想想就可以明白，既然<code>fd_set</code>里面已经有一个socket的数量计数，那么<code>select</code>的实现完全可以使用这个计数，而不是<code>FD_SETSIZE</code>这个宏。那么结论是，<strong><code>select</code>至少在Windows上并没有socket支持数量的限制。</strong>当然效率问题这里不谈。</p>

<p>这看起来推翻了我们一直以来没有深究的一个事实。</p>

<h2>Linux的实现</h2>

<p>在上面提到的MSDN中，其实已经提到了Windows与Berkeley Unix实现的不同。在<code>select</code>的API文档中也看到了第一个参数并没有说明其作用。看下Linux的<a href="http://linux.die.net/man/2/select">man</a>：</p>

<blockquote><p>nfds is the highest-numbered file descriptor in any of the three sets, plus 1.</p></blockquote>

<p>第一个参数简单来说就是最大描述符+1。</p>

<blockquote><p>An fd_set is a fixed size buffer. Executing FD_CLR() or FD_SET() with a value of fd that is negative or is equal to or larger than FD_SETSIZE will result in undefined behavior.</p></blockquote>

<p>明确说了，如果调用<code>FD_SET</code>之类的宏fd超过了<code>FD_SETSIZE</code>将导致<code>undefined behavior</code>。也有人专门做了测试：<a href="http://www.moythreads.com/wordpress/2009/12/22/select-system-call-limitation/">select system call limitation in Linux</a>。也有现实遇到的问题：<a href="http://serverfault.com/questions/497086/socket-file-descriptor-1063-is-larger-than-fd-setsize-1024-you-probably-nee">socket file descriptor (1063) is larger than FD_SETSIZE (1024), you probably need to rebuild Apache with a larger FD_SETSIZE</a></p>

<p>看起来在Linux上使用<code>select</code>确实有<code>FD_SETSIZE</code>的限制。有必要看下相关的实现 <a href="http://fxr.watson.org/fxr/source/sys/fd_set.h?v=NETBSD">fd_set.h</a>：</p>

<pre><code>typedef __uint32_t      __fd_mask;

/* 32 = 2 ^ 5 */
#define __NFDBITS       (32)
#define __NFDSHIFT      (5)
#define __NFDMASK       (__NFDBITS - 1)

/*
 * Select uses bit fields of file descriptors.  These macros manipulate
 * such bit fields.  Note: FD_SETSIZE may be defined by the user.
 */

#ifndef FD_SETSIZE
#define FD_SETSIZE      256
#endif

#define __NFD_SIZE      (((FD_SETSIZE) + (__NFDBITS - 1)) / __NFDBITS)

typedef struct fd_set {
    __fd_mask       fds_bits[__NFD_SIZE];
} fd_set;
</code></pre>

<p>在这份实现中不同于Windows实现，它使用了位来表示fd。看下<code>FD_SET</code>系列宏的大致实现：</p>

<pre><code>#define FD_SET(n, p)    \
   ((p)-&gt;fds_bits[(unsigned)(n) &gt;&gt; __NFDSHIFT] |= (1 &lt;&lt; ((n) &amp; __NFDMASK)))
</code></pre>

<p>添加一个fd到<code>fd_set</code>中也不是Windows的遍历，而是直接位运算。这里也有人对另一份类似实现做了剖析：<a href="http://my.oschina.net/u/870054/blog/212063">linux的I/O多路转接select的fd_set数据结构和相应FD_宏的实现分析</a>。在APUE中也提到<code>fd_set</code>：</p>

<blockquote><p>这种数据类型(fd_set)为每一可能的描述符保持了一位。</p></blockquote>

<p>既然<code>fd_set</code>中不包含其保存了多少个fd的计数，那么<code>select</code>的实现里要知道自己要处理多少个fd，那只能使用FD_SETSIZE宏去做判定，但Linux的实现选用了更好的方式，即通过第一个参数让应用层告诉<code>select</code>需要处理的最大fd（这里不是数量）。那么其实现大概为：</p>

<pre><code>for (int i = 0; i &lt; nfds; ++i) {
    if (FD_ISSET...
       ...
}
</code></pre>

<p>如此看来，<strong>Linux的<code>select</code>实现则是受限于<code>FD_SETSIZE</code>的大小</strong>。这里也看到，<code>fd_set</code>使用位数组来保存fd，那么fd本身作为一个int数，其值就不能超过<code>FD_SETSIZE</code>。<strong>这不仅仅是数量的限制，还是其取值的限制</strong>。实际上，Linux上fd的取值是保证了小于<code>FD_SETSIZE</code>的（但不是不变的）<a href="http://stackoverflow.com/questions/12583927/is-the-value-of-a-linux-file-descriptor-always-smaller-than-the-open-file-limits">Is the value of a Linux file descriptor always smaller than the open file limits?</a>：</p>

<blockquote><p>Each process is further limited via the setrlimit(2) RLIMIT_NOFILE per-process limit on the number of open files. 1024 is a common RLIMIT_NOFILE limit. (It&rsquo;s very easy to change this limit via /etc/security/limits.conf.)</p></blockquote>

<p>fd的取值会小于<code>RLIMIT_NOFILE</code>，有很多方法可以改变这个值。这个值默认情况下和<code>FD_SETSIZE</code>应该是一样的。这个信息告诉我们，<strong>Linux下fd的取值应该是从0开始递增的</strong>（理论上，实际上还有stdin/stdout/stderr之类的fd）。这才能保证<code>select</code>的那些宏可以工作。</p>

<h2>应用层使用</h2>

<p>标准的<code>select</code>用法应该大致如下：</p>

<pre><code>while (true) {
    ...
    select(...)
    for-each socket {
        if (FD_ISSET(fd, set))
            ...
    }

    ...
}
</code></pre>

<p>即遍历目前管理的fd，通过<code>FD_ISSET</code>去判定当前fd是否有IO事件。因为Windows的实现<code>FD_ISSET</code>都是一个循环，所以有了另一种不跨平台的用法：</p>

<pre><code>while (true) {
    ...
    select(. &amp;read_sockets, &amp;write_sockets..)
    for-each read_socket {
        use fd.fd_array[i)
    }
    ...
}
</code></pre>

<h2>总结</h2>

<ul>
<li>Windows上<code>select</code>没有fd数量的限制，但因为使用了循环来检查，所以效率相对较低</li>
<li>Linux上<code>select</code>有<code>FD_SETSIZE</code>的限制，但其相对效率较高</li>
</ul>

]]></content>
  </entry>
  
</feed>
